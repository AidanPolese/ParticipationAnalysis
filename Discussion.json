{
    "week-1": {
        "post-Moustafa-Mahmoud": {
            "content": "Although both of educational data mining and learning analytics aim to improve the quality of e-learning by analyzing a huge amount of data on learners and teachers, there are differences between them. According to Siemens & Baker (2012), One of the differences is the goal of the knowledge discovery process. In Learning analytics the measuring of the human behaviour is the goal. On the other hand, in education data mining it is a tool for automation discovery.  Secondly, Learning analytics analysis the whole learning system , rather than education data mining which divide the system to components then study these components. In addition, both of then use different techniques. The education mining uses the data mining algorithms of rule associations, clustering and classification. However, learning analytics focuses more on influence analysis. Examples of learning analytics techniques includes but not limited to social network analysis, influence analysis, discourse analysis, sentiment analysis, data visualization, and dashboards.(COMP863, unit 3)",
            "month-posted": "May",
            "date-posted": "24",
            "day-posted": "Tuesday",
            "time-posted": "12:45 PM",
            "replies": {}
        },
        "post-atulch": {
            "content": "Data mining: We live in a data-driven and information-rich world. While it's reassuring to know there's a plethora of information at your fingertips, the sheer volume of information can be overwhelming. The more data you have, the longer it will take to find the useful information you require. As a result, data mining is introduced, which is defined as: Data mining, also known as knowledge discovery in databases, is the act of uncovering interesting and valuable patterns and relationships in vast volumes of data in computer science. To examine huge digital collections known as data sets, the area integrates technologies from statistics and artificial intelligence (such as neural networks and machine learning) with database management. In business (insurance, banking, retail), science research (astronomy, medicine), and government security, data mining is widely applied (detection of criminals and terrorists). [1] Marketing and sales are the most common applications of data mining. To reduce client attrition, banks utilize data mining to examine customer transactions before customers decide to switch banks. In addition, some transaction outliers are evaluated for fraud detection. [2] Educational data mining (EDM): Educational data mining (EDM) is a multidisciplinary research area that looks at how artificial intelligence, statistical modeling, and data mining can be used with data from educational institutions. EDM uses computational methods to deal with complex educational data to investigate educational questions. To set a country apart from others, the education system must undergo a substantial transformation by rethinking its framework. Data mining techniques can be used to extract hidden patterns and data from diverse information repositories. [3] Learning analytics (LA) Learning analytics (LA) is a systematic and interdisciplinary topic that employs methods and analysis tools to assess the efficacy of various educational approaches such as problem-based learning (PBL), project-based learning (PBL), etc. Data analysis aids in the enhancement of higher education curriculum and material delivery. [4] Despite an increase in investigation and use, educational data mining and learning analytics continue to evade clear definition; the two concepts are sometimes used interchangeably. This could be since both fields include similar theme components. Identifying similarities and differences in themes between the two evolving disciplines is one way to bring clarity, uniformity, and consistency to the two fields. [5] For both educational data mining and learning analytics, five-topic models are inferred. It is discovered that, while there appear to be disciplinary differences in terms of study concentration, there is no evidence to indicate a clear separation between the two disciplines, other than their separate lineages. The trend indicates that advanced statistical learning approaches are being used to derive relevant insights from massive data streams to improve teaching and learning. Over the last five years, both fields have focused more on student behavior.",
            "month-posted": "May",
            "date-posted": "11",
            "day-posted": "Wednesday",
            "time-posted": "4:52 PM",
            "replies": {}
        },
        "post-leminhducng": {
            "content": "Data Mining: a powerful artificial intelligence tool, which can discover and extract useful information from large amounts of data sets by analyzing data from many angles or dimensional, categorize that information and summarize the relatioships identified in the database. Subsequently, it helps make or improve decisions. [1] Educational data mining (EDM): There has been increasing interest in the use of data mining to investigate scientific questions within educational research. EDM is defined as methods of making discoveries within the unique types of data that come from educational settings and using those methods to better understand students and the setting whith they learn in [2]. EDM is different from data mining methods in explicitly exploiting the multiple levels of meaningful hierarchy in educational data [3] Learning Analytics (LA): The measurement, collection, analysis and reporting of data about learners and their contexts for purposes of understanding and optimising learning and the environments in which it occurs. Both EDM and LA reflect the emergence of data-intensive approaches to education, share the goals of improving education by improving assessment, how problems in education are understood, and how interventions are planned and selected [4] According to Siemens & Baker (2012) [4], it is possible to identify five key distinctions between EDM and LA. These are: - Discovery: in EDM, researchers are interested in automated discovery, and leveraging human judgment is a tool for that; in LA it is quite the opposite, leveraging human judgement is the aim.- - Reduction and holism: EDM reduces systems to components and explores them and their relationships, while LA wants to understand whole systems. - Origins: EDM is rooted in educational software and student modelling; in contrast, LA origins are related to the semantic web, intelligent curriculum, outcome prediction and systemic interventions. - Adaption and personalization: EDM performs automated adaptation, whereas LA informs and empowers instructors and students. - Techniques and methods: EDM employs more techniques and methods of classification, clustering, Bayesian modelling, relationship mining, discovery with models, and visualization; while LA focuses on social network analysis, sentiment analysis, influence analysis, discourse analysis, learner success prediction, concept analysis and sense-making models.",
            "month-posted": "May",
            "date-posted": "1",
            "day-posted": "Sunday",
            "time-posted": "8:33 PM",
            "replies": {
                "reply-Robert-Kemp": {
                    "content": "Le Minh, thank you for your well constructed and thorough discussion post. I found the submission succinct and you were able to crystalize the important distinctions between Data Mining (DM), Educational Data Mining (EDM) and Learning Analytics (LA) to make it meaningful to the group. Well done. I am looking forward to working with you in this course. Thanks",
                    "month-posted": "May",
                    "date-posted": "2",
                    "day-posted": "Monday",
                    "time-posted": "9:02 PM",
                    "replies": {}
                }
            }
        },
        "post-Robert-Kemp": {
            "content": "Let's review the definitions of each in order to respond to the question; Data Mining - is an interdisciplinary subject and can be defined in many different ways and some consider data mining as a synonym for another popularly used term, knowledge discovery from data, or KDD, while others view data mining as merely an essential step in the process of knowledge discovery [1]. The knowledge discovery process is an iterative sequence of 1) Data Cleaning, 2) Data Integration 3) Data Selection 4) Data Transformation, 5) Data Mining, 6) Pattern evaluation and 7) Knowledge presentation[1]. Essentially data mining is the process of discovering interesting patterns and knowledge from large amounts of data. One could consider educational data mining as a subset of Data Mining that pertains specifically to educational data as opposed to other domains such as Marketing, Financial Services or Health care. As per the SOLAR site (Society for Learning Analytics Research),Learning Analytics is defined as the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs[2]. Learning analytics is both an academic field and commercial marketplace [2]. Based on the above three definitions, Learning Analytics is a more complete, more inclusive term that incorporates activities and a broader area of study than the Data Mining domain or the narrower domain of educational data mining which pertains to just educational subject matter.  The SOLAR site further defines Learning Analytics as a research and teaching field, where Learning Analytics sits at the convergence of educational research, learning and assessment sciences, educational technology combining it with Analytics, such as statistics, visualization, computer/data sciences, artificial intelligence, and Human-Centered Design (e.g. usability, participatory design, sociotechnical systems thinking). Therefore, Learning Analytics is a an inclusive cross section of domains with unique distinctions from that of Data Mining and Educational Data mining and is consistent with the mini literature review from [3], [4], [5] and [6]. What are your thoughts? Thank you",
            "month-posted": "April",
            "date-posted": "28",
            "day-posted": "Thursday",
            "time-posted": "6:57 PM",
            "replies": {
                "reply-Sheryl-Griffith": {
                    "content": "Good work Rob and congrats on being first out the gate! :-) I particularly appreciate the flow of your submission, it was easy to follow. I agree that EDM is a subset of data mining, however I also think that LA is also a subset.  I see LA as the creation or collection of data driven by students activities, and this data is analysed to predict the performance of the students. EDM develops techniques or methods to then analyse this data to create the settings for learning.",
                    "month-posted": "May",
                    "date-posted": "1",
                    "day-posted": "Sunday",
                    "time-posted": "1:43 PM",
                    "replies": {
                        "reply-Robert-Kemp": {
                            "content": "Sheryl, thank you for the comments and nice to emeet you earlier this week! I am discovering that Learning Analytics incorporate much more than my initial thoughts. I have to admit that I had never heard of the term before this course, but Unit 1 and Unit 2 readings has provided some good insights. Thanks for response. I look forward to working with you going forward.",
                            "month-posted": "May",
                            "date-posted": "2",
                            "day-posted": "Monday",
                            "time-posted": "8:56 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-kennethmc39": {
            "content": "What are the differences between Data Mining, Educational Data Mining, and Learning Analytics? The difference is one of granularity. Data Mining is a very general term, used to connote the collection of data from any imaginable source, for any number of purposes.   It can be viewed as an umbrella term, under which such concepts as Educational Data Mining live. If were able to define what Data Mining is, defining Educational Data Mining should be as simple as acknowledging the context in which it occurs.   As online learning becomes more and more popular, so too does the amount of data available relating to those efforts by students.  Educational Data Mining is, simply, the mining of available data from an educational context.  The means is clear; toward what end this mining occurs is undefined. Learning Analytics is the ends toward which Educational Data Mining is a means.  In other words, we mine Educational Data so that we may then perform Learning Analytics on it. Aldowah et al, made little distinction between Educational Data Mining and Learning Analytics in their paper Educational Data Mining and Learning Analytics for 21st century higher education: A review and synthesis.   That is not to say that they conflated the two, but the paper seemed to be written with the assumption that the difference between the two concepts would already be known to the user.[1] Viberg et al, noted that there is some confusion around the true nature of Learning Analytics [2]. Some people view Learning Analytics as the study of online student meta data, while others view Learning Analytics as the employment of that data in pursuit of supporting and improving the online student experience.   This may be because, as Dawson et al, noted, Learning Analytics is a young field.  Additionally, Learning Analytics is the convergence of multiple established fields, and this can lead to some conclusion.[3] Studying this data without considering its application would not be a fruitful exercise.  Indeed, education Data Mining already accounts for that possibility.   Instead, Learning Analytics are the natural extension of education Data Mining, as Educational Data Mining was an extension of Data Mining.  The distinction is one not only of granularity, but of focus and intent. It is worth noting that, as Viberg, et al, pointed out, often the main stakeholders in Learning Analytics are researchers [4]. The learners themselves generate the data, but are not directly involved in the Learning Analytics process.  This is not surprising. In many ways its like leading a blind study.  If the participants dont have knowledge of the experiment, the data is likely to be more honest.   By including the learners in the data generation process, youd effectively be changing the outcome by measuring it. In short, Data Mining begets Educational Data Mining, which in turn facilitates Learning Analytics. The concepts are all related, but are most certainly distinct.",
            "month-posted": "April",
            "date-posted": "27",
            "day-posted": "Wednesday",
            "time-posted": "8:02 PM",
            "replies": {
                "reply-janineis1": {
                    "content": "This is a great perspective on how these concepts can be related but have very differing characteristics. I like how you mentioned that we mine Educational Data... [to] perform Learning Analytics on it. One might infer that the two concepts are symbiotic in a sense as you would need both methods to truly understand business needs and inform decisions. Very interesting!",
                    "month-posted": "April",
                    "date-posted": "29",
                    "day-posted": "Friday",
                    "time-posted": "9:11 PM",
                    "replies": {}
                },
                "reply-George-Adyrhaiev": {
                    "content": "Great post - I got the same idea about LA vs EDM: Learning Analytics is the ends toward which Educational Data Mining is a means.  In other words, we mine Educational Data so that we may then perform Learning Analytics on it. At the same time, I am curious about what necessitated distinguishing EDM from DM altogether? The tools and techniques are the same, but the focus is different. Statistics for compsci vs statistics for psych is still statistics. ",
                    "month-posted": "April",
                    "date-posted": "30",
                    "day-posted": "Saturday",
                    "time-posted": "5:55 PM",
                    "replies": {}
                },
                "reply-Robert-Kemp": {
                    "content": "Ken, great post and it will be interesting to work with you again in this course. Yes, indeed you captured and identified the differences in the concepts.  Given the benefit being able to read Unit 2, and then responding, I think Learning Analytics is a very broad topic incorporating many domains and even after 10 years the area is still maturing.  I thought your quote was meaningful. The distinction is one not only of granularity, but of focus and intent. Well done.",
                    "month-posted": "May",
                    "date-posted": "2",
                    "day-posted": "Monday",
                    "time-posted": "8:46 PM",
                    "replies": {}
                }
            }
        },
        "post-janineis1": {
            "content": "In the traditional learning model, instructors have a principal role to sharing knowledge among students who are eager to learn expertise in subjects of their liking. However, as institutions acclimate to everchanging technology and growing data, learning curriculums that teach these new concepts must also adapt. With the introduction of the Internet, the appeal for online learning has grown significantly as it does not restrict students to a geographical location and since these courses are relatively affordable compared to on-site instruction, students are not constrained to overwhelmingly high student debt. As the above points are quite superficial in terms of how to attract more students to taking online courses, methods such as educational data mining and learning analytics are crucial to continuing the momentum for this attraction. In this case, since online learning can leave audit logs, or rather, an online fingerprint of how students use and interact with the platform, a plethora of data can be generated and analysed to inform business decisions or investments. In order to comprehend the vast amount of information on learning systems, methods such as data mining can be employed to process and extract patterns and ultimately, transform patterns into digestible chunks of knowledge. Educational data mining (EDM) is such that the focus is primarily on educational data with a goal to inform strategical business models on how to attract more students to a particular e-learning platform or more importantly, how it can cater to different student learning styles with automation at its core (Calvet Liñán & Juan Pérez, 2015). While EDM focuses more on technologies and statistical methodologies, learning analytics centers on the social aspect of EDM -- student behavior and the identification of learning styles through their interaction with learning platforms (University of Wisconsin-Madison, 2022). Contrary to the appeal of online courses being widely accessible and flexible to any type of learning environment and landscape, individuals may feel a sense of disconnect with the instructor, other students, and the institution itself. Therefore learning analytics is needed to enhance learning efficacy and boost best practice methods. Through my research, I found that EDM and learning analytics have striking differences in terms of how information is gathered but both concepts have the same goal to inform business decisions and increase efficacy of the learning process. Consequently, both methods should be employed at some level of degree to provide a comprehensive understanding to measure how effective instruction and teaching materials are, the learning styles of students in different cohorts, and how to enhance user interface design and usability.",
            "month-posted": "April",
            "date-posted": "29",
            "day-posted": "Friday",
            "time-posted": "9:02 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "I have certainly experienced that feeling of disconnection while being enrolled in this program.  For myself, I think it is exacerbated in courses with minimal focus on interactions between students.   I appreciated those courses that provided an avenue or a forum for real-time discussion.  For example, Comp 602 provided periodic Teams meetings, where the students and instructors could gather to discuss the work. Not many students attended, but even that face-time with the instructor helped a lot. The challenge in Learning Analytics doesn't seem to be the quantifying of student behaviour for the purposes of measuring it. The challenge as I see it is how to take those results and apply them so that all students benefit.  That is, there is a need to ensure that the solutions implied by Learning Analytics don't accidentally disadvantage other students.",
                    "month-posted": "April",
                    "date-posted": "30",
                    "day-posted": "Saturday",
                    "time-posted": "10:10 PM",
                    "replies": {
                        "reply-janineis1": {
                            "content": "Agreed - with online anything really, there is a strong sense of disconnect with human interaction and collaboration. However, there are some benefits in e-learning such that it certainly develops critical thinking skills and finetunes writing skills. Yes, that is definitely one part of the challenge - I think it would be interesting to investigate learning analytics and its use in understanding student learning styles further in order to increase benefits for students. Since there are varying styles, models should be adaptable enough to adjust to the complexities that come with understanding student behavior and the current learning model while keeping the goal of amalgamating different parts of EDM and learning analytics to minimise disadvantages for some students central to the process.",
                            "month-posted": "April",
                            "date-posted": "30",
                            "day-posted": "Saturday",
                            "time-posted": "1:44 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-Robert-Kemp": {
                    "content": "Janine, you were able to nicely summarize EDM and Learning Analytics and provide a caveat that learning analytics can enhance learning efficacy and boost best practice methods.  This is an important takeaway and I think we all can now view online courses with this perspective.  The follow up comments by Ken and yourself about how important the interaction between students for success - although it's quite limited in this program. I personally have found the real time interaction, ie group meetings via MS Teams  (along with a few oral tests) the most informative/memorable part of the courses that best facilitated learning. Looking forward to working with you in the program.",
                    "month-posted": "May",
                    "date-posted": "2",
                    "day-posted": "Monday",
                    "time-posted": "8:37 PM",
                    "replies": {}
                }
            }
        },
        "post-Che-Little-Leaf-Matusiak": {
            "content": "Learning Analytics is the study and analytics of data using various tools and techniques to determine insights about learners, how they learn, and the quality of learning materials delivered to learners. (Society for Learning Analytics Research, 2021)  According to the Society for Learning Analytics Research (2021), some of the tools used to perform this analysis is data mining, statistics, and algorithms amongst many other tools used for analysis.  A connection is that data mining is a tool used by learning analytics to facilitate this analysis. (Society for Learning Analytics Research, 2021). Educational data mining is the creation of techniques and processes for analyzing learners and the learning environment for which they are situated in using computerization to improve the educational model. (Siemens & Baker, 2012) Data mining would involve combining data sets and using various computerized statistical methods to learn about the data to determine some sort of revelation or course. (Society for Learning Analytics Research, 2021) Based on these definitions discussed in the readings, all three are connected by the use of data, data manipulation processes, and computerization techniques, however, they are meant to address different needs.",
            "month-posted": "May",
            "date-posted": "2",
            "day-posted": "Monday",
            "time-posted": "2:18 AM",
            "replies": {
                "reply-Sheryl-Griffith": {
                    "content": "I can see that you understand the concepts covered in this module. I appreciate your submission.",
                    "month-posted": "May",
                    "date-posted": "2",
                    "day-posted": "Monday",
                    "time-posted": "9:30 AM",
                    "replies": {}
                }
            }
        },
        "post-christianmu5": {
            "content": "Although these three terms are related to each other in some ways, they have significant differences that I would like to point out: Data mining aims at finding out valuable information in a large pool of data [1]. Data mining, therefore, has a broader meaning, it is not limited to any specific field or area. Data mining has become common in public as well as private institutions like Banking, insurance, medicine, and retail with the purpose of minimizing costs and maximizing benefits [2]. Educational Data Mining vs Learning Analytics Educational Data Mining (EDM) and Learning Analytics (LA) are both focused on collecting and analyzing data specifically in the education sector. Thats what they both have in common. However, what differentiates the two is that the EDMs data analytics process does not involve humans. It focuses on the learning tools and environment [3]. LA includes what EDM does but in addition, it takes into consideration human judgment. It helps to reveal patterns and information that cannot be detected with EDM [3]. Both EDM and LA have the goal of improving learning and its environment. In conclusion, all three terms are complementary like three good friends. What makes them unique is the kind of data they are able to collect.",
            "month-posted": "April",
            "date-posted": "28",
            "day-posted": "Thursday",
            "time-posted": "10:03 PM",
            "replies": {
                "reply-suzanneva5": {
                    "content": "When you say that EDM and LA have the goal of improving learning and its environment, by environment do you mean the learning platform? Or are you also including other parts of the business? I think LA and EDM are also differentiated by their data collection methods and the usage of the data. What are your thoughts?",
                    "month-posted": "April",
                    "date-posted": "30",
                    "day-posted": "Saturday",
                    "time-posted": "12:51 PM",
                    "replies": {
                        "reply-christianmu5": {
                            "content": "Thank you for your question. By learning environment I mean the students' learning settings and the overall environment in which the learning is taking place. As you know, All of us are affected by our environments every day. It can alter our mood, make us more/less productive, etc. Therefore the analysis of student-generated data through EDM tools and techniques can help enhance student engagement and learning.",
                            "month-posted": "May",
                            "date-posted": "1",
                            "day-posted": "Sunday",
                            "time-posted": "4:33 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-Matthew-Basaraba": {
                    "content": "You mention Learning Analytics (LA) encompasses Educational Data Mining (EDM). I'm curious to get your thoughts on if there's areas of EDM that aren't covered by LA? Applications of EDM include both feedback for instructors but also recommendations to students (Romero and Ventura, 2012). Whereas LA seems to have a much stronger focus on giving insights to instructors or educators to improve their learning environments. I could also imagine EDM research may lead to more development of non-instructor lead learning methodologies used alongside classroom or lecture based learnings.",
                    "month-posted": "May",
                    "date-posted": "1",
                    "day-posted": "Sunday",
                    "time-posted": "6:53",
                    "replies": {}
                }
            }
        },
        "post-Matthew-Basaraba": {
            "content": "Educational Data Mining (EDM) and Learning Analytics (LA) both derive from Data Mining a term used to encompass techniques which are used to turn a collection of data into useful information (Twin, 2021). Romero and Ventura (2012) define EDM as the application of data mining techniques on datasets from educational settings to provide insight into understanding students and how they learn. Currently the most widely accepted definition of LA comes from the 1st International Conference on Learning Analytics (Clow, 2013) Learning analytics is the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimizing learning and the environments in which it occurs. Siemens (2013) explains that LA extends EDM techniques and that one can assume that the tools used will overlap in both communities. As shown, EDM and LA are very similar in their methodologies of taking raw data and applying data mining or other techniques to create insightful information. The important distinction and reason for these being two separate communities stems from what they view as the purpose of this information. LA works to give insights to educators which can lead to more actionable outcomes, meanwhile EDM looks to find hidden relations or patterns for making predictions on learners outcomes (Aldowah et al., 2019). After investigating EDM and LA I have formed an understanding that while both areas have a lot of overlap in what techniques they employ to gain insights from data, EDM tends to focus on a more granular technology driven discovery of relations and predictions that can be made based of trends and behaviours of learners and LA has a stronger root in gathering insights that educators can use to make decisions to improve their learning environment at scale.",
            "month-posted": "May",
            "date-posted": "1",
            "day-posted": "Sunday",
            "time-posted": "6:21 PM",
            "replies": {
                "reply-Sheryl-Griffith": {
                    "content": "Great work Matthew! Well written and I agree with your assessment which is similar to my submission. I see that you understand the content.  My typical day is buried in data, customer, sales and finance data so when it comes to data mining, I am mostly interested in predictive and forecasting applications. What are your thoughts on any applications that are used in data mining?",
                    "month-posted": "May",
                    "date-posted": "1",
                    "day-posted": "Sunday",
                    "time-posted": "6:33 PM",
                    "replies": {}
                }
            }
        },
        "post-abebayi": {
            "content": "Similar to the views expressed by my colleagues, data mining, educational data mining, and learning analytics are highly linked. They all revolve around the collection, analysis, and utilization of data. However, there also exist some notable differences. While data mining is a complex process that relies on the use of software tools to locate and interpret data to establish patterns from any given data source, educational data mining differs in that its algorithms and models are refined to focus on educational data (North, 2012) (Romero and Ventura, 2020). Learning analytics then builds on educational data mining to analyze findings to incorporate other factors including societal, technical (such as predictive models), and environmental dimensions (Romero and Ventura, 2020). Therefore, the differences between data mining, educational data mining, and learning analytics can better be described as one that can be observed with building blocks, each unique in its own way, but one whose significance grows when leaning on the foundation of another. Furthermore, it is also important to note that this area of discipline continues to evolve as the era of e-learning spaces gains momentum. The rise of subset fields such as academic analytics, data-driven education, and educational data science is evidence that we are only scratching the surface of the significant contribution of data mining in education (Romero and Ventura, 2020). It is an important field that empowers decision-makers and end-users (learners and educators) to affect impactful change in the world of academia.",
            "month-posted": "April",
            "date-posted": "29",
            "day-posted": "Friday",
            "time-posted": "6:41 PM",
            "replies": {
                "reply-aidanpo1": {
                    "content": "I had a similar understanding of the information this week :)  I thought it was pretty interesting to see data mining techniques applied in an area where I would personally find it hard to apply them.  Did you think of some novel ways it could be applied because I found this information to be out of my personal expertise so I am asking around :)",
                    "month-posted": "April",
                    "date-posted": "30",
                    "day-posted": "Saturday",
                    "time-posted": "10:43",
                    "replies": {
                        "reply-abebayi": {
                            "content": "I find everywhere I turn there exist examples of data being mined and affecting policies. For example, ancestory tracking sites are being used to address what would have otherwise been cold cases[1].",
                            "month-posted": "May",
                            "date-posted": "1",
                            "day-posted": "Sunday",
                            "time-posted": "2:08 PM",
                            "replies": {
                                "reply-Sheryl-Griffith": {
                                    "content": "Thought I'd jump into the discussion. Abeba your example is very interesting, thanks for sharing. Another area where data mining is being used is in healthcare management, Sharma et al. data driven by healthcare transactions are being utilized to improve, plan and execute innovation in this sector.",
                                    "month-posted": "May",
                                    "date-posted": "1",
                                    "day-posted": "Sunday",
                                    "time-posted": "3:23 PM",
                                    "replies": {}
                                }
                            }
                        }
                    }
                }
            }
        },
        "post-eugenegr": {
            "content": "Agarwal, et al.(2012) and Ali (2013) do not establish a distinct process referred to as Educational Data Mining. Instead both papers reference Data Mining and the tools associated with Data Mining used for analysis of educational data. Romero and Ventura (2020) refers to Educational Data Mining as a method employing Data Mining tools to study unique educational data.  Arcinas (2021)  (Figure 1) provides a process overview for Educational Data Mining as a method involving the the collection of educational based data and the processing of the data by Data Mining tools. The distinction of EDM from DM is the addition of the collection of unique educational data. Otherwise both use DM tools. Why is educational data unique?",
            "month-posted": "May",
            "date-posted": "1",
            "day-posted": "Sunday",
            "time-posted": "2:20 PM",
            "replies": {}
        },
        "post-Sheryl-Griffith": {
            "content": "Week 1 discussion forum. How data mining, educational data mining, and learning analytics differ from each other (or, if you dont think there is a distinction, please detail why this is the case). As noted in previous submissions, and after my readings and research of relevant sources I too have concluded that these three concepts are related, and despite some differences, there are more similarities than differences. It all comes down to data and how it is utilized. Lets breakdown each concept: Data Mining According to Roiger et al, we can classify data mining as a method of finding or analyzing interesting structures in data. He goes on in the chapter to further expand and explain that data mining is about learning and here we are also introduced to knowledge discovery in databases (KDD) which can be interchanged with data mining. [1] Educational data mining (EDM) and Learning analytics (LA) Aldowah et al stated that both educational data mining and learning analytics use the application of data mining in educational settings to identify trends and patterns. [2] To further differentiate, educational data mining is data mining that looks at standard education data such as student enrolment records and exam results. Learning analytics is data collected and analyzed, using data mining about learning activities such as educational background and student background. [3] In conclusion, I see educational data mining and learning analytics as subsets of data mining, where data mining is utilized for use in the educational sector for decision making and to improve the education system. Educational data mining benefits the decision-makers and learning analytics should benefit the learner. [2] [3]",
            "month-posted": "April",
            "date-posted": "29",
            "day-posted": "Friday",
            "time-posted": "10:00 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "Learning Analytics seems to have some pretty clear goals. That is, the reading I've done indicates that the ultimate goal of Learning Analytics is to improve the student experience.  However, there may be some privacy concerns.  Do you feel that the benefits of allowing Educational Data Mining outweighs the potential risk to student privacy?",
                    "month-posted": "April",
                    "date-posted": "30",
                    "day-posted": "Saturday",
                    "time-posted": "10:05 AM",
                    "replies": {
                        "reply-Sheryl-Griffith": {
                            "content": "Good point in terms of the privacy concern. Although there are a number of advantages and are helpful for the student's progress, there is a concern about security and breach of privacy based on the sensitivity of data shared.  Robust systems and protocols must be put in place to safeguard the data. Also, transparency in policies must be established on how this data is utilized and shared.",
                            "month-posted": "May",
                            "date-posted": "1",
                            "day-posted": "Sunday",
                            "time-posted": "12:46 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-eugenegr-2": {
            "content": "Dawson et al. (2019)  outlines a laborious task of coding 522 papers to form the dimensions for analysis. Attempting to follow the same process to determine consumer trends based on discussions in a public forum would fail. Is siloing learning analytics and educational data mining detrimental to these processes? What is causing the specialization of LA and EDM? ",
            "month-posted": "May",
            "date-posted": "1",
            "day-posted": "Sunday",
            "time-posted": "10:32 AM",
            "replies": {}
        },
        "post-George-Adyrhaiev": {
            "content": "Data mining is the field of discovering novel and potentially useful information from large amounts if data. Education data mining is the area of scientific inquiry centred around the development of methods for making discoveries within the unique kinds of data that come from educational settings and using those methods to better understand students and the settings in which they learn [1]. Educational data mining often uses the following methods: prediction, clustering, relationship mining, discovery with models, and distillation of data for human judgement.   Learning analytics refers to the collection and analysis of data about learners and their environments for the purpose of understanding and improving learning outcomes.  Data mining, educational data mining, and learning analytics relate to each other in a way that learning analytics relies on educational data mining techniques to identify and collect educational data for analysis while using data mining approaches, tools, and techniques. From this perspective educational data mining can be looked at as a subset of broader data mining, focusing on a specific industry - education. While identifying the differences between educational data mining and learning analytics may be a bit trickier, since both are focusing on similar goals. As per Siemens and Baker [2], educational data mining emphasizes automated discovery, reducing to components and analyzing individual components and relationships between them, as well as a greater focus on automated adaptation (removal of human involvement). To achieve that, it relies on classification, clustering, Bayesian modelling, relationship mining, discovery with models, and visualization as the key techniques and methods. Learning analytics emphasizes leveraging human judgement, using automated discovery as a tool to accomplish this goal. It looks at understanding systems as a whole, in their full complexity, with a great focus on informing and empowering instructors and learners. The most prominent techniques and methods used in Learning Analytics are social network analysis, sentiment analysis, influence analytics, discourse analysis, learner success production, concept analysis, and sensemaking models. ",
            "month-posted": "April",
            "date-posted": "30",
            "day-posted": "Saturday",
            "time-posted": "5:35 PM",
            "replies": {
                "reply-aidanpo1": {
                    "content": "I had very similar ideas on this subject.  I thought of it kind of like a pipeline from EDM to LA under the umbrella of DM.  In terms of techniques being used it can be domain specific but it seems like unsupervised learning might fit for EDM and supervised learning for LA?  Do you think that this applies?  I'm not so sure myself but I thought it was interesting to think about :)",
                    "month-posted": "April 30",
                    "date-posted": "30",
                    "day-posted": "Saturday",
                    "time-posted": "10:30 PM",
                    "replies": {
                        "reply-George-Adyrhaiev": {
                            "content": "That is actually a good point Aidan - it does seem like LA would typically rely more on the supervised learning techniques that can help consider aspects of the system that is being analyzed. The clustered data points that EDM can provide to LA professionals can then be used as an input for supervised learning techniques like regression and classification. ",
                            "month-posted": "April",
                            "date-posted": "30",
                            "day-posted": "Saturday",
                            "time-posted": "11:43 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-aidanpo1": {
            "content": "To begin a comparison, it is important to define each term. Data Mining: Data Mining is the process of uncovering patterns and other valuable information from large data sets using machine learning algorithms [1].  The process for collecting the data can vary on a case-by-case basis ranging from automated screen scraping, to manual selection of data points.  The techniques that can be used to extract this information are things like neural networks, decision trees or k nearest neighbour [1].  The algorithm is selected based on data types and problem complexity and if the problem calls for supervised or unsupervised learning [1].   Educational Data Mining: EDM is concerned with developing methods to explore the unique types of data in educational settings and, using these methods, to better understand students and the settings in which they learn [2]. EDM seeks to use educational data to better understand learners and learning, and to develop computational approaches that combine data and theory to transform practice to benefit learners [2].  EDM also has goals in improving the learning process and guiding students learning, as well as pure research objectives, such as achieving a deeper understanding of educational phenomena [2]. Learning Analytics: Learning Analytics is the measurement, collection, analysis, and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs [3].  Learning Analytics is about closing a feedback loop enabled by data generated by learners and serves the purpose of enhancing learning and informing decisions in educational practices to optimize learning and the learning environment [3]. What does this all mean? Through my investigation in these research areas, I think that the three areas of study are subcategories of each other in a pipeline starting with Data Mining, then Educational Data Mining and finally Learning Analytics.  Each discipline building upon the previous by focusing down into a more specific area of study. Simply put, Data Mining is a process in which to extract information from large amounts of data through automated machine learning algorithms.  Data Mining is not restricted to a single domain as it is just a set of techniques that can be applied to all kinds of data provided that the data is able to be interpreted by an appropriate machine learning algorithm.  We can think of this as the starting point for Educational Data Mining and Learning Analytics.  Educational Data Mining is the next step in the pipeline.  Educational Data Mining utilizes standard Data Mining techniques and applies them in a learning context.  EDM uses these analysis tools to generate information and linkages between aspects of education and learning.  This information is usually assessed automatically using classical data mining techniques.  The information representation and linkages presented and generated by Educational Data Mining is then put into action through Learning Analytics.  Learning Analytics uses larger amounts of human involvement to also assess data in a more holistic fashion and then feeds the output into the hands of educators and learners to create a more optimal learning environment and learners.   Data Mining is the subject area. Educational Data Mining is the subfield. Learning Analytics is an actionable analysis of data for learners and educators.  They are all corelated areas of research.",
            "month-posted": "April",
            "date-posted": "30",
            "day-posted": "Saturday",
            "time-posted": "10:19 PM ",
            "replies": {}
        },
        "post-suzanneva5": {
            "content": "With regard to the difference between the terms data mining and educational data mining, data mining is a broad term that stems from information technology, machine learning, and statistics. Succinctly, data mining means information is being analyzed to gain knowledge. (Tretter, 2003). The term itself is not specific to any domain. It does not indicate which type of data, the data origins, the participants, how the data is being studied, or how the findings will be applied.  Educational data mining is a specific category of data mining focused on analyzing large datasets from educational settings (Romero, 2010). In some texts, authors have used educational data mining and learning analytics interchangeably. However, while there is overlap since both terms refer to educational datasets, educational data mining is a broader term that encompasses not only learning analytics but also administrative data, demographics, or other available information that may be pertinent to the educational setting (Scheuer & McLaren, 2012).  Learning analytics is the narrowest term. Its focus is on students and their interaction with both learning content and learning systems with the goal of improving the learning experience (Mah, 2016).",
            "month-posted": "April",
            "date-posted": "28",
            "day-posted": "Thursday",
            "time-posted": "9:45 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "After this week's readings, I've come to view Data Mining, Educational Data Mining, and Learning Analytics as sort of Russian Nesting Dolls.  One fits inside the other, which fits inside the other. Do you think that the narrow focus of Learning Analytics might cause researchers to get tunnel vision?  Is there the possibility that the solutions for issues with distance learning might exist outside of this discipline?",
                    "month-posted": "April",
                    "date-posted": "30",
                    "day-posted": "Saturday",
                    "time-posted": "10:13 AM",
                    "replies": {
                        "reply-suzanneva5": {
                            "content": "That's a great comparison and very true. I don't think the researchers in learning analytics will get tunnel vision. I prefer to think they can be focused on the task at hand. Their focus however isn't just students in my opinion. The findings should lend themselves to instructors/teachers so materials and the instructional platform can be improved. I would also mix in a bit of educational data mining with analytics if I felt that the EDM would complement findings. For example, categorization of learners by age, primary language, or some other social factor that may influence their learning or experience with a system.",
                            "month-posted": "April",
                            "date-posted": "30",
                            "day-posted": "Saturday",
                            "time-posted": "12:42 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-George-Adyrhaiev": {
                    "content": "After having gone through this week's readings I have actually developed a slightly different perspective. Educational data mining is one of the tools to achieve the goals of learning analytics, making learning analytics a broader concept than educational data mining. EDM is a subset of DM, while LA employs various techniques and methods like EDM and other types of analytics (like human judgement). Curious if I got that right?",
                    "month-posted": "April",
                    "date-posted": "30",
                    "day-posted": "Saturday",
                    "time-posted": "5:48 PM",
                    "replies": {
                        "reply-suzanneva5": {
                            "content": "I had the opposite impression based on some texts. It seems that LA is more focused on the data from the learners while they're using the learning system. ",
                            "month-posted": "April",
                            "date-posted": "30",
                            "day-posted": "Saturday",
                            "time-posted": "10:12 PM",
                            "replies": {}
                        }
                    }
                }
            }
        }
    },
    "week-2": {
        "post-Moustafa-Mahmoud": {
            "content": "According to Simplelearn (2019), Big Data is an expression of the huge amount of data that is entered every day via the Internet, whether by organizations or by individuals, which includes social networks, LMS, health systems, government regulations, airlines, shipping companies, banks, financial institutions, stock exchanges, and others. In order to say that the data is Big Data, it must have several characteristics, including that the size of the data has to be huge, and it must has value for people or organizations, or wider, and it must also be accurate, in addition to the speed of entry and update operations has to be high .Big data includes multiple formats such as text , audio , video , picture  data formats , It may be  structured , semi-structured  and unstructured  (Rouse  M., 2019). This means that if there is a large amount of data, but it has no value to people or organizations, or it is no longer used, this does not make it big data and analyzing it not become interesting whatever the big size of it. In order to handle the big data, we use distributed systems which store the data in different file systems. Then different copies of the same data files become available in multi storage nodes. these systems implement mechanism for backup and restore. They use parallel processing technique, they divide the task into smaller sub tasks, run them in parallel in different nodes/machine they combine  the results to retrieve the final output of the original task. In the field of learning analytics, the students click on the LMS may be analyzed in order to view the user experience and learning efficiency which called clickstream (Fischer, C., Pardos, Z. A., Baker, R. S., Williams, J. J., Smyth, P., Yu, R., Slater, S., Baker, R., and Warschauer, M. ,2020). Big data analysis has a promising future and that the whole world cares about it. For example Big data analytics has been able to predict the occurrence of some natural disasters days before they occur (Simplelearn ,2019). Analyzing consumer behaviour  for LMS websites and social networking sites led us to understand the sequence, duration and positions of user interaction, then we can determine the shortcomings and difficulties in some parts of the program, and therefore it directs us to improve the systems them.",
            "month-posted": "May",
            "date-posted": "24",
            "day-posted": "Tuesday",
            "time-posted": "5:57",
            "replies": {}
        },
        "post-atulch": {
            "content": "Big data Big data is defined as data with greater diversity, coming in higher volumes and with greater velocity. The three Vs is another name for this. Big data is simply larger, more complex data sets, particularly from new data sources. Traditional data processing software is incapable of handling these massive data sets. However, these huge amounts of data can be leveraged to solve business challenges that were previously unsolvable. [1] Big Data is defined as large-scale data. Bigdata is a phrase that refers to a large collection of data that is rising rapidly over time.Examples of Big Data analytics include financial exchanges, social media sites, and jet engines, among others. Structured, unstructured, and semi-structured data could be big data. Big Data features include volume, variety, velocity, and variability. Bigdata's benefits include greater customer service, operational efficiency, and decision-making. [2] Data science  To extract value from data, data science incorporates numerous domains such as statistics, scientific methodologies, artificial intelligence (AI), and data analysis; data science combines multiple fields. Data scientists are individuals that use a variety of talents to analyze data acquired from the web, smartphones, customers, sensors, and other sources in order to draw actionable insights. Data science refers to the process of cleansing, aggregating, and modifying data in order to undertake advanced data analysis. The results can then be reviewed by analytic applications and data scientists to uncover patterns and enable company leaders to make informed decisions. [3] Data-driven decision-making (DDDM) Facts, measurements, and statistics are used to influence strategic business decisions that match with your goals, objectives, and activities. When companies see the full value of their data, everyone benefits; whether you're a business analyst, a sales manager, or a human resource specialist. However, identifying the next strategic opportunity does not require merely selecting the suitable analytical technology. Data-driven decision-making allows you to move faster and make fewer mistakes, resulting in increased profits. More people are learning to incorporate data into their thinking as a cultural phenomenon. People are becoming increasingly conscious that their intuition can be incorrect as sectors become more computerized. When you have statistics, you can figure out when your content is performing well and why. The data can then be used to correctly anticipate how comparable material will perform in the future. IMPACT: The use of data for decision-making in educational institutions is neither a new topic nor an unknown practice. Indeed, since a growing awareness dating back to the 1990s, school principals, teachers, parents, stakeholders and policy-makers started looking at quantitative data as an indispensable source for making decisions, formulating diagnoses about strengths and weaknesses of institutions, and assessing the effects of initiatives and policies, etc. The diffusion of organizations and initiatives such as Education for the Future in the USA, or the DELECA - Developing Leadership Capacity for data-informed school improvement project in Europe, give testimony to a growing attention to the clever, intensive and informed use of data for improving schools activities and results. The commitment to engage with a stronger use of data became quite widespread across schools, sustained by the evidence that () the use of data can make an enormous difference in school reform efforts, by helping schools see how to improve school processes and student learning. On policy grounds, the movement for adopting more evidence-based educational policies and practices stems from the use of high-quality, originally developed data about academic results obtained through specific initiatives. The current debate of the present chapter is focused on the potential of so-called big data to transform education, as it is rapidly doing in several aspects of social life. [4]",
            "month-posted": "May",
            "date-posted": "11",
            "day-posted": "Wednesday",
            "time-posted": "4:50 PM",
            "replies": {}
        },
        "post-leminhducng": {
            "content": "Business Intelligence is the ability of a company to produce meaningful use of data collecteds in its day-to-day business operation [1]. The role of Business Intelligence is very important in improving organizational performance by identifying new opportunities, detecting potential threats, revealing new business insights and enhancing decision making process The advance of internet technology and computing power have facilitated collections of a large volume of heterogeneous data from multiple sources, that poses new challenge and opportunities for business intelligence. Big data analytics can assist companies to exploit big data for improving customer satisfaction, managing supply chain risk, generating competitive intelligence, and providing business real-time insights to help make important decision. According to research, a retailer can use big data to achieve the potential ability of increasing 60% of operating margins by obtaining market share over its rivals and exploiting the detailed consumer data [2]. There are five prime advantages of big data analytics. - First, it increases visiblity by making related data more accessible. - Second, it facilitates performance improvement and variability exposure by collecting accurate performance data - Third, it helps in better meeting the actual needs of customers by segmenting the population. - Fourth, it complements the decision making with automated algorithm by revealing valuable insights - Fifth, it yields new business models, principals, products and servies One of the most important applications of big data analytics is knowledge creation, and new management prinicples cultivation Big data analytics can improve the management of supply chain and support the supply chain to innovate new product and service development ideas and also optimize the operation processs in a cost effective way Big data supports the decision making processs [3], based on an improved understanding of diverse decision contexts and the required information processing mechanisms",
            "month-posted": "May",
            "date-posted": "9",
            "day-posted": "Monday",
            "time-posted": "9:21 AM",
            "replies": {}
        },
        "post-Robert-Kemp": {
            "content": "Big Data is a term that is used frequently and is part of our daily life now, certainty in the technical and marketing domains. The common definition is that Big data is a combination of structured, semi structured and unstructured data collected by organizations that can be mined for information and used in machine learning projects, predictive modeling and other advanced analytics applications [1]. Big Data incorporates a lot of data, an incredibly large amount - peta bytes or exabytes- but to me it is almost incomprehensible. A similar comparison regarding incomprehension is how many stars there are in the universe? Apparently there more stars than there are grains of sands on the beaches on the planet earth or approximately 200 billion trillion stars[2], [3]. (Personally Quantum mechanics was always a challenge for me too and trying to understand and comprehend the fact of faster you approach the speed of light, time slows down, but I am digressing) For me its easier to grasp the power of big data with practical applications and commercial benefits, ie using data generated from almost any activity to improve the service or benefit for consumer to save money, better service or improved patient care. This is where clickstreams, system logs and stream processing systems will benefit consumers and patient care. This seem innocent enough, but there is a much darker side and there is so much data being collected by every company its beyond what we can imagine. For example Google apparently tracks and records every location you have every been at, every Youtube video you have ever watched , every application you have used and every image you have viewed [4]. However, that is another topic for another day. Given this courses topic, ie Learning Analytics, I found one of the reference links intriguing on how Data Science is playing a Big role in higher education with a list of benefits such as improving completion rates, finding prospective students, accelerates fundraising, offer industry-relevant education, improves quality of campus life and gauges student engagement [5]. What was more interesting was a paper concerning if over practice is necessary for improving learning efficiency and responses can be gathered and understood through educational data mining[5]. This has great potential, if we could identify the very best ways to improve learning rates of adults and children alike. There are over 50 million children in public schools just in North America[6]. If we could understand the best ways to learn and applying those techniques would have huge benefit to society and to the children themselves. This provides better context in understanding this course and the impact of learning analytics. There are of course the commercial aspects for learning organizations as noted above and there are of course ways to use Big Data to identify the best donors to accelerate philanthropy [7] or why some alumni donate and others dont [7 Another helpful interesting understanding I stubbled upon is the many practitioners and academics now use the word analytics in place of Business Intelligence (BI) [8]. Many define analytics as the process of developing actionable decisions or recommendation for actions based upon insights generated from historical data[8]. Overall the readings and videos from this week were very interesting and informative. (I am not sure if its just me and my interest in the topic or were these readings extra interesting?) Big Data is hear to stay whether we like it or not.",
            "month-posted": "May",
            "date-posted": "5",
            "day-posted": "Thursday",
            "time-posted": "8:30 AM",
            "replies": {
                "reply-George-Adyrhaiev": {
                    "content": "Ditto to this week's readings being especially interesting. Thank you for your post - I kind of went down a bit of a rabbit hole reading about the speed of light and time, haha. I think that the incomprehensible nature of Big Data should be irrelevant to what you can do with it. We should focus on how to distill information out of any size of data, versus treating it differently than a huge excel table while we're trying to create a pivot table with specific parameters. With proper tooling, the size of a Big data dataset is pretty much irrelevant. At the same time, more of the focus should be (and is) put onto pre-processing of the data to ensure instead of the enormous swamp of unstructured data, companies collect and store something more useful.",
                    "month-posted": "May",
                    "date-posted": "5",
                    "day-posted": "Thursday",
                    "time-posted": "6:46 PM",
                    "replies": {
                        "reply-Robert-Kemp": {
                            "content": "George, thank your for your comments I agree that the focus should be on what we can do with Big Data rather than how vast the data we have so far collected is. With increased storage and lower costs we can store as much as we want. (I have heard that the major cloud companies offer some companies 'no cost' for data storage as an incentive to secure their cloud business and the chance to sell their other services) Yes, I agree the focus is on pre-processing of the data to make it as useful as possible - I found the AthabascaU's Data Mining course (COMP682) very interesting and it discussed the importance of data cleaning - I wish I had studied this topic 10 years ago. As with everything, there are some interesting video's/MOOC's on data mining too if and the Ian Witten's textbook is available online too one link to the video's is here  https://www.cs.waikato.ac.nz/ml/weka/courses.html .  WEKA is a free tool to learn about data mining and the video's are great",
                            "month-posted": "May",
                            "date-posted": "7",
                            "day-posted": "Saturday",
                            "time-posted": "8:37 AM",
                            "replies": {}
                        }
                    }
                },
                "reply-kennethmc39": {
                    "content": "Thanks for sharing your thoughts on this week's material.  Your submission reminded me of something I was thinking about while writing my own response, that I did not explore. Do you think big data is so big simply because of the nature of our digital existence? Or does big data beget more big data?   Does the system encourage the creation and collection of as much data as possible, or is the amount of big data that we generate at an organic (instead of artifically increased) level?",
                    "month-posted": "May",
                    "date-posted": "5",
                    "day-posted": "Thursday",
                    "time-posted": "6:48 PM",
                    "replies": {
                        "reply-Robert-Kemp": {
                            "content": "Ken, thank you for reading my discussion post and providing your comments. Hummmm yes, I think you crystalized it nicely. 'Big data begets big data'.  The very 'smart' people, (if thats the right word) saw the huge benefits of using this data very early on for commercial gain and did everything to build these 'data stores'.  In their rush to build their data stores they had to ignore any concept of privacy or data rights. Once you have the data and you need to keep it current and keep building it and the concept of user's data rights seem to have been prioritized much lower by many companies.  This data will grow as new industries are created, such as Autonomous Vehicles and question arise of who owns this data and how it will be  who needs to know the path I drive or how fast one drives or where I travel to and how often?",
                            "month-posted": "May",
                            "date-posted": "7",
                            "day-posted": "Saturday",
                            "time-posted": "9:13 AM",
                            "replies": {}
                        }
                    }
                },
                "reply-suzanneva5": {
                    "content": "I also too see the benefits of capturing and using this data to improve lives and services; however, as you say, there is a much darker side. ",
                    "month-posted": "May",
                    "date-posted": "6",
                    "day-posted": "Friday",
                    "time-posted": "9:56 AM",
                    "replies": {}
                },
                "reply-aidanpo1": {
                    "content": "I do actually think that exploring the darker side of big data is important as well, like you mentioned briefly.  I think that it is possible for it to be used in inappropriate ways rather than positive ones like you mentioned.  I do wonder what larger companies do with our data that we are unaware of.",
                    "month-posted": "May",
                    "date-posted": "8",
                    "day-posted": "Sunday",
                    "time-posted": "8:07 PM",
                    "replies": {}
                }
            }
        },
        "post-kennethmc39": {
            "content": "There is some debate as to whether or not big data is like oil.  It is my understanding that the argument in favor of this view posits that much like oil, big data is a resource that needs only be collected and processed in order to be valuable.  That is, big data exists whether someone makes use of it or not. Of course the reality is that many organizations are making use of our collective Big Data, all day every day.  This is true at all levels of government, as well as for unknown and uncountable private organizations.    For example, the Weather Network smartphone app was discovered to be selling user location to advertisers without user knowledge.[1] I say user knowledge, and not user consent.  Im confident that somewhere in the terms of service for the Weather Network app was a line about allowing them to do just this. But few people take the time to read End User License Agreements, and many organizations are taking advantage of that.[2] Big Data was, perhaps, inevitable.  Capitalism celebrates monetization, and the monetization of our locations and digital behaviors is a big business.   Prior to the internet and the smartphone resolution, the granularity of data available on customers and consumers was far less fine.  It simply wasnt possible to track a person through their day, without physically watching them. This weeks materials include a video by SimpliLearn, which explicitly outlines the benefits of big data such as its usefulness in improving healthcare services[3]. However, the video doesnt touch at all upon the pitfalls and downsides.   Chief amongst the downsides of big data is the erosion of the expectation of privacy.   People now expect to be watched, and may have some idea that everything they do and search for is being used to target them.  Whats the alternative?  Life as a luddite?  We traded privacy for convenience. There are many applications and uses for big data that arent dystopian.   If behavior can be quantified, it can be measured. And if it can be measured, it can be studied and conclusions may be drawn from it.  Patterns may be identified, and so services such as healthcare and education may be improved. Big data itself is not good or bad. It simply is.  The manner in which it is utilized ultimately determines the morality of the end product.   Big Datas influence will continue to grow, so long as theres a buck to be made.",
            "month-posted": "May",
            "date-posted": "5",
            "day-posted": "Thursday",
            "time-posted": "6:33 PM",
            "replies": {
                "reply-George-Adyrhaiev": {
                    "content": "Big data is a means to raising productivity rates and enjoying the benefits of it, alternatively, the development of our society would have stalled. Currently, a lot of companies realize that the biggest issue of Big Data is that the expectations are way too high while the outcomes are often questionable. Of course, we have the Alphabets, Metas, Expedias and so on of this world that know how to capitalize on the data points they collect on their users, but for a lot of organizations their efforts to collect data and distill knowledge do not contribute to any meaningful changes [1].  Privacy is also pretty subjective. I'd argue that there was less privacy in the early 20th century in a small village where everyone knows everything about everyone just because there is not much else to do but gossip as compared to today. Whatever people believed their privacy was and whatever it actually is, does not align, and I doubt that's just the fault of capitalism and technology. Certain basic concepts evolve with the changing times and values of the society, which is just natural, and in my opinion, the concept of privacy is one of those basic concepts. Capitalism, on the hand, caters to the need of paying customers, which results in privacy-focused solutions (often at a premium) that exist for those who seek.  I am also curious to explore how the education sector actually uses (not studies) EDM and Big Data to improve their services or operations. For a lot of students that would be quite an interesting observation as one part of a university is meddling with Big Data and learning analytics, while another fails to update study materials for over a decade or uses course scheduling software that was developed over 15 years ago. ",
                    "month-posted": "May",
                    "date-posted": "5",
                    "day-posted": "Thursday",
                    "time-posted": "7:21 PM",
                    "replies": {}
                },
                "reply-Robert-Kemp": {
                    "content": "Thank you for your well thought out discussion post. I agree with your comments about Big Data - that its not good or bad, but it is simply with us and there are many applications for Big Data that are not dystopian. If money is to be made, then people's perspective and morals can will change. Just think of the asbestos industry, the tobacco industry, man-made ecological disasters including climate change to name a few. At the same time, being optimistic, I think Big Data can be used to help understand and interpret the data to see problems and ultimately uncover the issues. How do we reveal the topics and have conversations about the darker side to Big Data and not cover them up for the sake of political correctness or because its against a companies commercial interest? What if we use Big Data to stream everyone to certain roles based using predictive analysis for the jobs they hold or the education they can receive? ",
                    "month-posted": "May",
                    "date-posted": "7",
                    "day-posted": "Saturday",
                    "time-posted": "8:07 AM",
                    "replies": {
                        "reply-kennethmc39": {
                            "content": "Thank you for taking the time to reply.  To your point, there comes a tipping point when the practices of a certain industry can no longer be ignored. However, the examples you cited have a direct, appreciable effect on a person.  That is, I feel like a person who has lung cancer is far more likely to raise a fuss than a person who is under the invisible hand of Mass Surviellance. If the adverse effects of Big Data can be cleanly and concretely explained to the masses, we stand the best chance of having a fully informed public.  After that, what the public chooses to do about it as a collective is up to us. Re: predictive analysis, that is another interesting point.  If big data is used to determine the outcome of a person's entire life, who's to say that manipulating big data for personal gain isn't an emerging market?  Parents already bribe Ivy League schools to let their children in.  I can't think of a reason why they wouldn't pay to manipulate the Life Path Algorithms of tomorrow.",
                            "month-posted": "May",
                            "date-posted": "8",
                            "day-posted": "Sunday",
                            "time-posted": "9:54 AM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-janineis1": {
            "content": "Big data is a term that describes large amounts of various datasets that are too complex for traditional data analytical tools to analyse and derive insights on. In today's world, it seems inevitable for institutions to lean towards using big data as the volume of data produced in one day is almost incomprehensible. Therefore, institutions moving towards a data-driven business must adapt to any limitations that big data can produce. I found the readings on how big data is being used in higher education quite interesting as it relates to this course and Athabasca University in general. However, it did get me thinking about how some challenges can arise using a data-driven approach to tailor courses that entice students, adapt to their different learning styles, and how they can appeal to prospective students. Continuing off from my post last week, catering to a variety of learning styles can be problematic in a virtually online university setting. This is because individuals who prefer in-person teaching might have a strong sense of disconnect with the instructor and therefore, at your own pace learning can prove to be tasking. Moreover, independent learning requires established time management/organisational skills in order to achieve success in a course. While it can be useful to collect quantitative data on the above issue, a focus on qualitative analysis through surveys and virtual roundtable discussions can be used to better understand student behavior and learning styles and ultimately, humanise the big data being generated through online course platforms. In this case, Baig, Shuib, & Yadegaridehkordi (2020) suggest that hybrid models that include both web-based and traditional techniques in higher education can benefit most students and provide a better understanding of how students view the difficulty of the course and their overall behavior towards learning in the class. Understanding human behavior is significant to creating adaptable but tailored courses to students, however, tracking online behaviors can prove to be invasive and brings up concerns of privacy and surveillance. For example, some ways in which institutions collect information about student behavior are through visual and facial emotions and reactions (Bousbia & Belamri, 2014). To alleviate concerns, Wang (2016) suggests that transparency from institutions on how data is collected (whether it is for aggregate statistical or research study purposes) is a must in order to secure trust from current and prospective students. Further discussions should include adopting a blended approach on how to integrate big data into education by using quantitative data to both improve data warehousing and mining techniques and to provide concrete foundations for any insights found qualitatively. Using a cautious and blended approach to investigate any challenges can hold significant potential to creating online courses that benefit students and their learning styles while also being a reliable institution in regards to how insights are collected to do so. As the future continues to move towards integrating big data in curricula and other institutions, the volume and velocity of how data is being produced can be challenging as there could be untapped potential within datasets and therefore, a loss of possibly significant insights on how to better improve online education and learning platforms. Moreover, as data increases, so can the complexities of data. Loss of insights can be addressed through continuing education on how to use sophisticated data analysis tools - thus, employers should prioritise literacy development for individuals working in the data realm in order to evolve to changing platforms and methodologies for collecting/analysing data (Miller, 2014).",
            "month-posted": "May",
            "date-posted": "8",
            "day-posted": "Sunday",
            "time-posted": "8:10 PM",
            "replies": {}
        },
        "post-Che-Little-Leaf-Matusiak": {
            "content": "As more and more companies, proponents, stakeholders, governments, healthcare providers, educators, and other organizations incorporate big data processes to bring value to their operations, these processes can also have other applications such as to Indigenous education in Canada.  When combined with learning analytic methods, big data techniques can be used to help understand how Indigenous peoples learn, improve learning outcomes of Indigenous learners, and help create a learning environment, through well developed guidelines, that promotes a culturally sensitive collaborative approach rather than a purely Eurocentric one.  This can ensure better outcomes for students as many Indigenous learners learn differently, a well-known fact to Indigenous peoples.  Botelho and Bigelow (2022) state that big data collected from populations can provide valuable insights and informed decisions.  These benefits can be extended to data from Indigenous learners. To provide evidence, Fischer et al., (2020) discusses that big data characteristics of variety, volume, and velocity can provide insights into learner behaviours in terms of educational research.  Further, insights can also help developers understand the effects of current educational policies and resolve issues that can result from these policies which will facilitate personalized learning in the end.  (Fischer et al., 2020)  Fischer et al., (2020) describes three big data frameworks for education which are micro, meso, and macro level big data.  The use of micro level big data can be most beneficial in this educational context as it involves data from cognition and student behaviours. (Fischer et al., 2020)  Recently, data mining is used as a tool to facilitate inferences and pattern detection to provide further insights into learning. (Fischer et al., 2020).  These previously described methods and technologies that exist within big data can facilitate collaborative approaches to Indigenous education.  Hopefully, more research can be devoted in the future to this topic to help improve the learning outcomes of many diverse populations and improve the service delivery of educational institutions.",
            "month-posted": "May",
            "date-posted": "9",
            "day-posted": "Monday",
            "time-posted": "3:10 AM",
            "replies": {}
        },
        "post-christianmu5": {
            "content": "BIG DATA This expression can be defined in so many ways, but three ingredients are essential for a set of data to be considered Big Data: Volume, Velocity, and Variety [1]. Volume refers to the amount of data to be processed, whether it is structured, semi structured, or unstructured. This data is coming from various sources. Velocity refers to how fast this data is being generated and transmitted. Not daily or weekly but on a real-time basis. Variety refers to the different types of data being generated. The bottom line is that big data exceeds the normal processing power of a conventional database. The data is too large, moves too fast, and doesnt fit in the database structure. Hence why we need alternative ways to process this data. [1] DATA SCIENCE Data science is basically the process of using scientific tools and techniques to generate meaningful insight and information from a big set of data, with the goal of helping businesses and organizations make informed decisions [2] With data growing every second in volume and type, Data science helps convert this massive amount of data into meaningful information. Data science is considered today as the engineering of the future [3] DATA SCIENTIST A data scientist is someone who works with a large amount of data to extract meaningful information through Data science tools and techniques. To be a good data scientist, one must have a good understanding of mathematics and critical thinking [4] As mentioned earlier, data scientists are engineers of the future [3]. A good engineer needs to know his tools and know how to use them. Therefore, a data scientist needs to know the different tools and technologies and how to apply them to solve real-world problems [4]. Knowing this motivates me personally to keep on going into this new field Im learning because there are so many opportunities to make a positive impact in our communities.",
            "month-posted": "May",
            "date-posted": "5",
            "day-posted": "Thursday",
            "time-posted": "11:27 PM",
            "replies": {
                "reply-suzanneva5": {
                    "content": "This week's reading were very motivating and enlightening. I too want to continue learning about data science and its various applications to make a meaningful difference.",
                    "month-posted": "May",
                    "date-posted": "6",
                    "day-posted": "Friday",
                    "time-posted": "9:43 PM",
                    "replies": {}
                },
                "reply-Robert-Kemp": {
                    "content": "Thank you for your discussion posting. Big Data is an interesting topic to jump into - given this is your first AU course and will certainly give you momentum and direction for your future courses. I think you have nicely described how many people approach Data Science and Big Data along the lines of wanting to know more about the tools and how to use them - and this applies to me too. Today, everything is about data now and being part of the 'gold rush' is important as it is fundamentally changing the ways we all work and think. As you have identified there are many opportunities and I hope as you do we can tame Big Data to have 'positive impact in our communities'. Do you have any ethical or moral concerns how Big Data is used and applied in society and would this impact your involvement with that company or organization?",
                    "month-posted": "May",
                    "date-posted": "7",
                    "day-posted": "Saturday",
                    "time-posted": "7:41 AM",
                    "replies": {
                        "reply-leminhducng": {
                            "content": "Big Data and Data Science Ethics is a very interesting topic. I would like share five principles of data ethics for business. 1. Ownership over personal information 2. Transparency (How data is collected, stored and used) 3.Privacy (Personal Identifiable Information) 4. Intention (If intention to collect data violates the ethic rules or harms other people/busineses) 5. Outcomes (If the impact of data analysis can cause inadvertent harm to individuals or group of people)",
                            "month-posted": "May",
                            "date-posted": "9",
                            "day-posted": "Monday",
                            "time-posted": "10:38 AM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-Matthew-Basaraba": {
            "content": "After completing this weeks readings on the change in prevalence of Big Data I found myself calling back to last weeks discussions on data mining. It seemed to me that like Data Mining, Big Data appeared to be a buzz word that congregates many techniques and applications under one name. If we look at Figure 1 a review of google trends we can see both Data Mining (purple) and Big Data (blue) moving in a downwards trend. What I see from this is a shift from introducing new topics and methods for acquiring, storing and process ding data to more result driven fields that make use of this more readily available and refined data. As companies adopt Big Data strategies roles and methods like data science and data analytics which use this become more important questions rather than should companies adopt strategies around Big Data. When we look at Big Data from the perspective of education and learning it appears that there will be unique challenges in capturing that data (Brown et. al, 2011). Along with costs and challenges of acquiring data in educational institutions there are also ethical and security concerns for storing, sharing and using student data in a way that cannot be used against the students these systems are trying to help (Wang, 2016).",
            "month-posted": "May",
            "date-posted": "8",
            "day-posted": "Sunday",
            "time-posted": "8:05 PM",
            "replies": {
                "reply-aidanpo1": {
                    "content": "Like you mentioned, I find ethics to be of big concern when talking about big data and what to do with it.  I do fear for my own privacy but it seems in this day and age, that is impossible, especially give how ubiquitous technology and digitally stored information is.",
                    "month-posted": "May",
                    "date-posted": "8",
                    "day-posted": "Sunday",
                    "time-posted": "8:13 PM",
                    "replies": {}
                }
            }
        },
        "post-abebayi": {
            "content": "The readings this week emphasized that Big Data is a field that is undergoing significant transformation as its value is becoming more prominent in affecting change in any given industry. It is no longer limited to the volume, variety, and velocity of structured, semi-structured, and unstructured data; instead, this field has now expanded to include veracity, value, and variability as its impact on business has gained momentum [1]. When we look at Big Data and education, it will be interesting to examine how it will shape higher education in the next couple of decades. Companies such as Coursera have already been able to identify gaps between university education and employer demands to offer certification programs that offer learners ample opportunities and higher chances of employability [2]. According to Dennis (2021), such alternative opportunities and changes in employer requirements for degrees have contributed to a decline of 2.3% in four-year undergraduate program attendance at universities in the U.S [2]. Big Data has contributed to this shift as companies are able to track and leverage data collected regarding learners preferences and attitudes towards education. Big Data as I see it, is a tool that can foster both positive and negative socio-economic outcomes depending on how it is utilized. Companies have become enlightened to the fact that they can leverage data of their consumers behaviors and sentiments to optimize their operations and increase their profits. From the standpoint of consumers, Big Data also presents opportunities for better and quicker product development. However, one cannot discuss this growing field without touching on some of the challenges it presents. If stakeholders are not careful, inferential analytics can further deepen the socio-economic divide by offering products and solutions to certain demographics thereby reinforcing and perpetuating existing discrepancies [3]. Similarly, it can also be used to discriminate and incorrectly profile certain groups of people [3]. Big Data also raises the question of ethics. Is it ethical for companies to entice consumers with free products and/or services such as enhanced security features of using biometric data at the cost of giving up ones right to privacy?",
            "month-posted": "May",
            "date-posted": "7",
            "day-posted": "Saturday",
            "time-posted": "9:34 AM",
            "replies": {
                "reply-Sheryl-Griffith": {
                    "content": "Abeba, you raise some interesting points regarding ethics, privacy, and online profiling. As we know Big Data is here to stay, with the evolution of technology, especially in the last decade, but is it the price we have to pay? People are now more aware that their personal information is out there in cyberspace and are demanding policies and regulations be put in place to protect such. Thanks for sharing!",
                    "month-posted": "May",
                    "date-posted": "8",
                    "day-posted": "Sunday",
                    "time-posted": "11:49 AM",
                    "replies": {}
                },
                "reply-suzanneva5": {
                    "content": "I'm glad you brought up the potential to misuse data which can lead to discrimination against and profiling of people based on their gender, race, sexual orientation, or some other demographic characteristic. Although this course is focused on learning analytics, we can look to other industries to see potential problems and cautionary tales. In law enforcement, big data has a large influence and is used at different touchpoints and in different ways. Data collection can come in the form of structured data depending on the investigation as well as in pictures, videos, or audio files among others. They even use AI (predictive analytics) to assess sentencing as well as the potential to re-offend. These algorithms are all human created, which can lead to bias. We read this article in a library science class not long ago. You may find it interesting. Although not related to law enforcement or education, bias in AI has creeped into hiring practices. Even Amazon had to abandon its use of AI during recruiting because the developers had trained the system to reject female applicants. According to Dastin, the system penalized resumes that included the word womens as in womens chess club captain. And it downgraded graduates of two all-womens colleges, according to people familiar with the matter. When we're collecting data, we need to be aware of what we're capturing and the context of its application, especially in reference to other demographic data that may potentially inject bias into AI/machine learning",
                    "month-posted": "May",
                    "date-posted": "8",
                    "day-posted": "Sunday",
                    "time-posted": "12:11 PM",
                    "replies": {
                        "reply-Sheryl-Griffith": {
                            "content": "Suzanne, thanks for sharing! You have hit on some important elements regarding bias in AI, and machine learning. Expanding on your points, this brings out the veracity of Big Data, which relates to the origin, accuracy and quality of the data. Is it reliable and is the source trustworthy? This may be considered the most important V as veracity helps to filter through what is important and what is not. Once recognized would this then remove the biases?",
                            "month-posted": "May",
                            "date-posted": "8",
                            "day-posted": "Sunday",
                            "time-posted": "7:11 PM",
                            "replies": {
                                "reply-suzanneva5": {
                                    "content": "I would like to think that veracity removes the bias but if humans are involved in determining what's important and what's not, then there's room for bias. I think it's important to understand how data is collected, why, and how it's processed to eliminate as much bias as possible.  ",
                                    "month-posted": "May",
                                    "date-posted": "8",
                                    "day-posted": "Sunday",
                                    "time-posted": "10:30 PM",
                                    "replies": {
                                        "reply-Sheryl-Griffith": {
                                            "content": "Suzanne, you are absolutely correct. Then systems should be built without biases, tested and evaluated to ensure such. I know easier said than done.",
                                            "month-posted": "May",
                                            "date-posted": "9",
                                            "day-posted": "Monday",
                                            "time-posted": "9:09 AM",
                                            "replies": {}
                                        }
                                    }
                                }
                            }
                        },
                        "reply-Robert-Kemp": {
                            "content": "Thank you for your discussion post. You raise a very interesting point that Big Data will shape 'higher education' in the decades to come.  I agree that this could be one domain that could continue to experience significant change. The traditional university/college - with formal registration, physical buildings and along with significant tuition costs are now competing with the likes of Coursea, Udemy, Youtube and many others.  An example is Udemy offers 80 hours of Java programming for $14 taken by over 672000 people. Stanford University has long had their Machine Learning lectures posted on Youtube by AI 'rock star' Andrew Ng as early as 2008 (who is a co-founder of Coursea). Anyone with knowledge to share and can effectively communicate to students will have students gravitate towards them. These alternative opportunities will only continue to grow. Do you think Big Data will be impacting education in the years to come at all levels and not just 'higher education' Thanks",
                            "month-posted": "May",
                            "date-posted": "8",
                            "day-posted": "Sunday",
                            "time-posted": "5:16 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-eugenegr": {
            "content": "Pyscho-history dealt not with man, but with man-masses. It was the science of mobs; mobs in their billions. It could forecast reactions to stimuli with something of the accuracy that a lesser science could bring to the forecast of a rebound of a billiard ball. The reaction of one man could be forecast by no known mathematics; the reaction of a billion is something else again. centuries would yet pass before the mighty works of fifty generations of humans would decay past use. Asimov, I. (1952). Foundation and empire. Garden City, N.Y: Doubleday. The concept is to collect large volumes of information from human kind to establish the trends and behaviours that resulted in a historical event. The models created could then be used to determine the minimum event required to change the probability of a desired future event. The process repeats over time until the time of the future event. The results are studied and another adjustment occurs. The author also indicates information has a life span; even Earth has faded to a fictional status similar to the status Atlantis holds today.",
            "month-posted": "May",
            "date-posted": "7",
            "day-posted": "Saturday",
            "time-posted": "8:16 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "Rob raised an interesting point when replying to my discussion, regarding the use of big data to predict which jobs people would be suited for.  Like anything else, I feel such a system would be subject to manipulation and collusion, especially when the outcome of a person's entire life depends on it.  Also, the biases inherent in the designers  would inevitably be present in the resulting system.  As a simply example, any racist or sexist tendencies of the designers would ultimately be an emergent property.  In spite of all that, I would not be surprised if some form of this system became reality in the future.",
                    "month-posted": "May",
                    "date-posted": "8",
                    "day-posted": "Sunday",
                    "time-posted": "10:00 AM",
                    "replies": {}
                }
            }
        },
        "post-eugenegr-2": {
            "content": "Hasan, at al. (2020) summarizes the research found in 34 papers focused on Big Data in the finance industry. The study found machine learning has changed the industry in multiple areas including but not limited to fraud detection, risk analysis, process improvements, and revenue generation demonstrating the positive influence of Big Data. The study further summaries issues requiring further refinements and studies in the areas of quality, privacy, protection, management, analysis, and cost of Big Data. In addition, a requirement exists for further studies on the influence of Big Data on financial products and services. Conversely, the importance of the organizations culture on Big Data with respect to the implementation of Volume, Velocity, Variety, Veracity, and Value is the basis for the study by Nguyen (2018). The study concludes organizations tending toward adhocracy have a positive impact on all five Vs, whereas organizations with a hierarchical culture have negative impacts on all five Vs. A mixture of positive and negative impacts on the Vs can be seen in organizations based on a clan or marketing culture. Big Datas influence on expectations for advancement is exemplified by Yu et al. (2021). The potential of Big Data acquired from research, technology, competitions, education and industries related to sports may provide a source for guidance in the training of students in physical education programs. Big Datas success in some organizations has increased expectations of value-added possibilities. This influence can be positive if successfully implemented with proper consideration for the organizations goals and data requirements.",
            "month-posted": "May",
            "date-posted": "5",
            "day-posted": "Thursday",
            "time-posted": "6:10 PM",
            "replies": {
                "reply-George-Adyrhaiev": {
                    "content": "Thank you for your post. Big data and PII is a huge topic in the financial industry as regulations for data protection and compliance are quite strict. In my experience, data governance and data obfuscation for Protected B or C data at a bank are two extremely important and mandatory domains that often prevent certain data points to be used in a wisdom-generating process by data analysts and scientists.  I also liked the article you shared on the effects of organizational culture on Big Data - I haven't considered that before.",
                    "month-posted": "May",
                    "date-posted": "5",
                    "day-posted": "Thursday",
                    "time-posted": "7:34 PM",
                    "replies": {
                        "reply-eugenegr": {
                            "content": "Thank you for the response. The article was a pleasant surprise and relates to personal experience, especially with a strong hierarchical culture.",
                            "month-posted": "May",
                            "date-posted": "7",
                            "day-posted": "Saturday",
                            "time-posted": "1:04 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-abebayi": {
                    "content": "Thank you, Eugene for sharing the articles and summarizing them for discussion. You summarized the impact of Big Data in the financial industry very well. As this field continues to evolve, of the five V's discussed, I am curious to know how you would prioritize them in terms of significance? I personally find that as the influence of Big Data gains momentum, more focus needs to be placed on veracity. This perhaps may impact the velocity and volume of data collected, but it may generate higher quality data. ",
                    "month-posted": "May",
                    "date-posted": "7",
                    "day-posted": "Saturday",
                    "time-posted": "9:46 AM",
                    "replies": {
                        "reply-eugenegr": {
                            "content": "Thank you for the question. From my personal experience as an Analyst responsible for a manufacturing quality support system in the late 90s. Veracity was a function of Variety, Velocity, Volume, and analysis. The definition of Value was obtained by the objectives and defined the importance of the other four Vs. (https://www.bbva.com/en/five-vs-big-data/ mentions a similar pyramid ordering of the 5 Vs.) In addition, sensor derived information is not perfect and therefore will reduce the veracity and require cleaning of the data as part of the preprocessing before analysis. Unfortunately, this is a singular perspective from a definite hierarchical culture. To answer your question, yes veracity is important but attempting to achieve higher quality data maybe impossible or costly. The tools and methods of Data Mining (COMP 682) address analysis and Veracity of the data.",
                            "month-posted": "May",
                            "date-posted": "12:59 PM",
                            "day-posted": "Saturday",
                            "time-posted": "12:59 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-Sheryl-Griffith": {
            "content": "Use this forum to (a) discuss any questions or thoughts you have about the reading materials for this week. So much to learn, such interesting and important topics in todays world! I thoroughly enjoyed this weeks learnings. The first thing that stood out for me is that Big Data is not about size or volume and can be classified as structured, semi-structured, and unstructured. Breaking it down into the various Vs and how big data can benefit organizations. In my company, I see we are making advances in terms of exploiting big data and data analytics but not fast enough, we are still behind, with so many opportunities. From better customer insights, improved operations, marketing intellect, and a nimbler supply chain, the list is endless really. We can appreciate that Big data is for machines; small data is for people. [1] [2] I can relate to The Internet of things, or IoT where a thing can be classified as a person with an implant, as my husband has an implantable cardioverter-defibrillator. Its fascinating to me that this device allows RPM (remote patient monitoring). Reading this article brought to light privacy and security where these devices can actually be hacked, and there is a need for enforceable security policies and malware defences. [3] Data Science  call me a nerd but it is exciting to see the evolution of technology, as we saw in 2001, William S. Clevelands intention was to take data mining to an alternative degree by combining computer science and data mining. Linking the same concepts of data science and data analytics in higher education to a business, for instance, to ascertain a similar scenario in the HR area, get an innate insight into job prospects, and select the best candidate for the job. We actually see this in ATS which is software that recruiters use to streamline the hiring process. Data science and data analytics can assist the HR function to analyze employees engagement, improving work-life balance, and predicting employee turnover. [4] [5] For further discussion and I look forward to hearing input from the group.",
            "month-posted": "May",
            "date-posted": "7",
            "day-posted": "Saturday",
            "time-posted": "9:57 PM",
            "replies": {}
        },
        "post-George-Adyrhaiev": {
            "content": "First of all I wanted to say thank you for a great selection of resources for this weeks reading. I smiled when I saw JoMa techs YouTube channel as Ive been following some of his videos about data science and working at FAANG for a bit. Secondly, I watched the Simplilearn video (I usually enjoy their content) but this time it caught my attention that they claim 40 Exabytes of data is being generated by each smartphone user per month. In our previous week, we looked at great infographics that mention that in 2025 we will be producing 463 EB of data per day globally. Additionally, by 2025 well get to only 16 zettabytes of available storage globally, so Simplilearns statement seems very exaggerated and wrong in this context. If each smartphone user produces 1.3 EB of data per day, times 5 bn smartphone owners - were getting over 6.5 billion exabytes of data per day, which is something I am not sure the name for even exists today. Curious if someone has picked that up too or maybe Ive just grossly misunderstood the message.  Regarding Big Data and the growing influence of it, Id like to make a case that it is no longer a shiny new thing that were just learning about. Big Data has become ubiquitous in pretty much every industry, from healthcare to education, from the government to aviation, and so on. More and more companies are moving away from traditional data warehouses into Lakehouse (hybrid of a data lake and a data warehouse) and data lake solutions as they can no longer rely on just structured data to derive insights and support the strategic decision-making process. Skills like data visualization, data cleansing, R, Python, and Machine Learning development have become standard for any data analyst hoping to get hired these days [1]. In 2022 you can not be successful in a data analytics role without a thorough understanding of Big Data architecture, stages of data pre-processing along the pipeline, data modelling techniques, metadata management, data governance and obfuscation.  Big data is just another source of data points that on their journey to knowledge and wisdom [2] that needs to be properly dealt with, and in 2022 there are plenty of tools to do that. Given that over 97% of companies of Fortune 1000 companies invest actively in big data and AI [3], it needs to be embraced, accepted and treated as a major source of potentially useful insight, rather than some sort of mythical novelty that is ambitiously exciting for companies to start tapping into. Ive also found a great source of Big Data stats - some facts are quite interesting [4]! ",
            "month-posted": "May",
            "date-posted": "3",
            "day-posted": "Tuesday",
            "time-posted": "11:12 PM",
            "replies": {
                "reply-christianmu5": {
                    "content": "Thanks for sharing the article on some of the interesting facts about Big data. One fact that really got me thinking is the amount of time it would take to download all the data from the internet. The article mentions that it would take 181 million years to download all the data on the internet. That is just crazy! And for sure it would require a huge hard drive. I'm guessing in the years to come we will move from the term Big data to VERY big data or Huge Data.",
                    "month-posted": "May",
                    "date-posted": "4",
                    "day-posted": "Wednesday",
                    "time-posted": "6:53 AM",
                    "replies": {
                        "reply-George-Adyrhaiev": {
                            "content": "Thank you for your comment. 181 million years is indeed quite a bit, wondering what bandwidth was taken into consideration :) I think that over time the size of big data becomes irrelevant. Even today for an analyst working on a data set that is 100 GB or 2 TB the only thing that changes is a query processing time. The general approach to how to deal with data of that size won't really change. ",
                            "month-posted": "May",
                            "date-posted": "5",
                            "day-posted": "Thursday",
                            "time-posted": "6:18 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-eugenegr": {
                    "content": "I enjoyed the article on 21 surprising big data statistics, trends & facts for 2022 I especially liked 9. Poor data costs the US $3.1 trillion a year hints at the need to define objectives before investing in big data.",
                    "month-posted": "May",
                    "date-posted": "4",
                    "day-posted": "Wednesday",
                    "time-posted": "5:46 PM",
                    "replies": {
                        "reply-George-Adyrhaiev": {
                            "content": "I think that is one of the main reasons why data science and data analytics is such a hot industry these days. Reducing direct and indirect costs of poor/noisy data and distilling knowledge from massive amounts of unstructured data is a must-have competency of any data-driven enterprise. ",
                            "month-posted": "May",
                            "date-posted": "5",
                            "day-posted": "Thursday",
                            "time-posted": "6:22 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-Robert-Kemp": {
                    "content": "I agree that the focus should be on what we can do with Big Data rather than how the vast amounts of data, with increased storage and lower costs we can store as much as we want. (I have heard that the major cloud companies offer the Canadian banks no cost for data storage as an incentive to secure their cloud business and the chance to sell their other services) Yes, I agree the focus is on pre-processing of the data to make it as useful as possible - btw I found the AthabascaU's Data Mining course (COMP682) very interesting and it discussed the importance of data cleaning - I wish I had studied this topic 10 years ago. As with everything, there are some interesting video's/MOOC's on data mining too if interested and Ian Witten's textbook is available online too  https://www.cs.waikato.ac.nz/ml/weka/courses.html .  WEKA is a free tool to learn about data mining and the video's are great ",
                    "month-posted": "May",
                    "date-posted": "7",
                    "day-posted": "Saturday",
                    "time-posted": "9:20 AM",
                    "replies": {
                        "reply-Robert-Kemp": {
                            "content": "George - thanks for your discussion posting - I mistakenly posted my comments twice and it was actually meant to be in response to your comments on my posting  however  I saw the term Data Lakehouse for the first time last year, but recognized it immediately as new more encompassing term from Data Warehouse or Data Lake. Not sure how long its been in use  I aim to learn more about this too You are absolutely correct, that its essential to have a thorough understanding of Big Data architecture, data pre-processing, data modelling techniques, metadata management, data governance. The focus is on Big Data everywhere! How much emphasis and focus should be data ethics and privacy on this new Big Data 'gold rush' ?",
                            "month-posted": "May",
                            "date-posted": "8",
                            "day-posted": "Sunday",
                            "time-posted": "5:39 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-aidanpo1": {
            "content": "In terms of Big Data, I am both scared and impressed which may sound a little silly.  40 exabytes of data is produced each month by a single smartphone user [1].  This amount of data being produced indicates exactly who a single person is and even where they have been that day.  Lots of this information is easily trackable with a bit of Linux knowledge [2] and access to Wireshark, a network protocol analyzer [3].  Also, a lot of each users information is being monitored by large companies like Google as a person might be using their Google account for most tasks.  This type of user is exposing their information willingly.  Having this much data being created, and possibly followed, opens up a single persons data to large misuse.  Fortunately, larger companies like Google are open about what they do with a users data [4]. However, I do worry about other organizations. However, there are lots of benefits to the use of big data that I think are wonderful!  Specifically in the education sector as presented in this weeks material.  Education is very relevant to me as a current student, but also in my future as I plan to pursue my PhD under my current supervisor at Queens University and hopefully attain professorship at some point.  The use of big data in the education sector to improve student success is slowly coming into use at universities [5].  Student demographics, academic background, academic performance, course enrolment and more are being used as indicators to improve student performance by showing indications of what causes a student to receive low grades and when it is a good time to intervene to prevent a student from failing [5].  Also, other universities are using micro, meso and macro level big data to improve student success as well [6].  This data is used over a student's entire academic career to improve things like intelligent tutoring systems to help a student achieve their goals [6].  Big data applications like this make me believe in a brighter future for learners. There are newer systems being put into place that are more personalized for all types of learners.  Academia is a fairly rigid and largely standardized learning environment that doesnt work for all people.  There are people who flourish in this type of environment, and there are those who want to perform better but the one size fits all style of learning might not be conducive to the way they learn.  Personalized learning systems might be a great way to help all types of learners perform and that is very inspiring.",
            "month-posted": "May",
            "date-posted": "8",
            "day-posted": "Sunday",
            "time-posted": "8",
            "replies": {
                "reply-Matthew-Basaraba": {
                    "content": "I enjoy your insight on big data in education, it's interesting that you point out that one size fits all style of learning as I believe that the structure of this class focused around group discussions and engagement may foster better learning outcomes than other models focused on testing memorization we see in a lot of public education. In terms of big data I thought you might find this article interesting which touches on different fields and their perceived value and ease of data acquisition (Brown et al, 2011):",
                    "month-posted": "May",
                    "date-posted": "8",
                    "day-posted": "Sunday",
                    "time-posted": "9:55 PM",
                    "replies": {}
                }
            }
        },
        "post-suzanneva5": {
            "content": "According to new statistics from EarthWeb, there are 2.5 quintillion bytes of data created daily. This figure includes not only structured data like transactions but also other unstructured data types like emails, texts, video chats, and so forth. I dont think most people unless they work in a field that includes or studies data collection or methodology understand how much information they are sharing with an organization through normal interactions. We only have to think back to the Cambridge Analytica scandal to know that people dont necessarily think about their data privacy, especially when using an app. Although I work in experience design and am experienced at looking at data from surveys and analytics in order to create solutions, I was still taken aback at the creation of donor profiles to get more money. It just comes across as very manipulative, much like that CA usage of personal Facebook data. I also have my reservations about using data, especially human created data, to create any type of campaign for students or employees. In my mind, there can be room for bias, either intentional or unintentional. If most students are of a certain race, economic status, culture (or another demographic measure) and they are successful, the campaign will focus on them. All other groups will be excluded, even though these may have been equally successful but there are less of them. According to Murrell in Forbes, the usage of student demographics in predictive models can easily perpetuate the structural and historical inequities that persist in access to education. Also, the concept of tracking students physical movements to determine a schools student engagement rate is questionable. This information could be abused if someones identifiable information is indicated, and their physical location is also noted.",
            "month-posted": "May",
            "date-posted": "5",
            "day-posted": "Thursday",
            "time-posted": "10:13 PM",
            "replies": {
                "reply-Robert-Kemp": {
                    "content": "Thank you for your discussion post and I do agree with you that many people dont think about their privacy and have acquiesced in giving up a lot of their privacy in return for services from Big Tech.  I read once that Google stores over 100,000 data points on everyone. I can't find that reference for this response, but esquire's article about a tech consultant documenting what google has stored is unnerving, which includes how long it takes you to get to work and when you turn your phone on and off - every time. [1] Big data seems like its the proverbial 'wild wild west' with how we treat access to data and there are limited restrictions on how this data can be used by the industry.  We all desire to have the useful benefits, like increased learning rates among learners, improved Healthcare diagnostics and down to optimized routes to drive. The bias you refer to is disconcerting. Take for example, Life Insurance companies could use Big Data analysis to understand if one is 'too risky' for life insurance, Car insurance companies already employ data analysis based upon constant monitoring to offer lower rates, but despite denials, could easily deny insurance to others. You already mentioned Cambridge Analytica and how data is used to (my words) to learn how to ultimately manipulate voters. The question is who determines what is questionable or not acceptable to track and monitor?",
                    "month-posted": "May",
                    "date-posted": "7",
                    "day-posted": "Saturday",
                    "time-posted": "7:13 AM",
                    "replies": {}
                },
                "reply-abebayi": {
                    "content": "Thank you for your post, you touched on important points. You identified the limited knowledge of the general public with respect to the amount of personal data produced that gets into the hands of third parties as part of Big Data. What is alarming to me is, that it is becoming very difficult to be certain even when you think you are using proper precautions to control access to your personal data. For example, in 2020, Facebook was fined 5 Billion USD for repeatedly misrepresenting the extent to which its users could control access to their personal data (Kemp, 2020). This demonstrates the ugly side of Big Data which is that it can be manipulated by companies to rob people of their right to privacy and use it as a tool for generating more capital. Do you think, the associated advantages of Big Data outweigh its disadvantages? The unfortunate truth is that a fine of any magnitude does not bring back your personal data that has already been utilized. What measure, if any, can be set to deter companies from overextending their access to our data?",
                    "month-posted": "May",
                    "date-posted": "7",
                    "day-posted": "Saturday",
                    "time-posted": "8:27 AM",
                    "replies": {}
                }
            }
        }
    },
    "week-3": {
        "post-Moustafa-Mahmoud": {
            "content": "Google trend is an online tool which analysze the web search queries. Other tools such as Semrush and Similarweb. It always comes to mind that the first benefit of using Google Trend is free and that it does not need to download programs, but there is another important advantage, which is the available API that allows programmers to grab the results from Google trend to their apps of dashboard programmatically. These free online tools with their usefulness and ease of use, but they are permanently limited by the capabilities of the tool and cannot extends to use all the requirements and questions of the user.",
            "month-posted": "May",
            "date-posted": "31",
            "day-posted": "Tuesday",
            "time-posted": "9:18 AM",
            "replies": {}
        },
        "post-atulch": {
            "content": "The four main analytical models organizations can deploy are: descriptive diagnostic predictive prescriptive The usefulness of each model increases as you progress from descriptive to prescriptive analytics. However, their complexity grows at the same time. Descriptive analytics This is the most popular sort of business analytics. It usually relies on historical data from a single internal source to determine when an event happened. As an example: How many sales did we make in the previous week, day, or hour? Which customers needed the most assistance from our support team? How many people have visited our site? Which of the products have the most flaws? Dashboards and reports with descriptive analytics are popular ways to consume data and make decisions. The majority of the statistics we employ are accounted for by descriptive analytics, such as basic aggregation (e.g. count or sum of items filtered from a column or data), averages, and percentage changes. Diagnostic analytics The next question is: Why did it happen? Diagnostic analytics can assist us to answer this. Analysts achieve this by digging deeper into an organization's historical data and merging numerous sources to look for patterns, trends, and correlations. What are the advantages of using diagnostic analytics? Identify anomalies: Analysts utilize descriptive analysis results to identify areas that require more examination and to raise questions that cannot be answered just by looking at the data. For instance, why have sales increased in a place where marketing has not changed? To detect correlations, analysts must look for patterns outside of existing data sets. They may need to employ data mining techniques as well as data from outside sources. Determine causal relationships: After identifying anomalies and looking for patterns that could be linked, analysts utilize more advanced statistical approaches to see if they're linked. Data analysts used to do diagnostic analytics by hand, but as data volume, diversity, and velocity grow, purely manual analysis is no longer practical. Modern diagnostic analytics systems, on the other hand, use machine-learning approaches to supplement the analyst's abilities. Computers can process massive volumes of data and recognize patterns, find anomalies, and reveal 'abnormal' events, as well as apply analytical techniques from a portfolio of algorithms to identify change drivers and assess causality. Predictive analytics As an organization's analytical maturity grows and it begins to use predictive analytics, its focus moves from understanding historical events to gaining insights into the present or future conditions. Predictive analytics is a hybrid of traditional statistical analysis and cutting-edge artificial intelligence (AI) approaches. It attempts to answer the following question: What will happen next? Although it is impossible to forecast exactly what will happen in the future, predictive analytics may help businesses determine the likelihood of various outcomes and improve their chances of selecting the optimal course of action. Many industries employ predictive analytics. Predictive analytics are used in the aerospace industry to estimate the impact of various maintenance activities on aircraft reliability, fuel consumption, availability, and uptime. Predictive analytics is used in the financial services industry to construct credit-risk models and forecast financial market developments. Manufacturing - Predictive analytics is used to anticipate the location and rate of machine failures, as well as to optimize raw material ordering and delivery based on expected future demand. Online retail  Systems track customer behavior, and predictive algorithms evaluate whether giving more product information or incentives would boost the chances of a sale. Excel or Tableau can be used to develop simple predictive models. These analytics become the job of data scientists when these models incorporate more variables with more intricate interactions. Prescriptive analytics The most difficult sort of analytics is predictive analytics. To achieve the best results, it integrates internal data, external sources, and machine-learning approaches. A decision-making process is applied to descriptive and predictive models in prescriptive analytics to determine the combinations of existing conditions and possible options that are most likely to have the largest impact in the future. This process is both hard and resource-intensive, yet it may deliver enormous value to a company when done correctly. Examples of prescriptive analytics applications include: management of risk enhancing health care marketing, selling, and pricing with guidance Prescriptive analytics, as the most complicated type of analytics, not only faces technical obstacles but is also influenced by external factors including government legislation, market risk, and existing organizational behavior. If you're thinking about implementing prescriptive analytics, make sure you have a good business case for why machine-generated recommendations are acceptable and reliable for each choice. Data scientists working on prescriptive models must not only identify and program each option but also ensure that all possible consequences are considered. They must test their models repeatedly after deploying them to ensure that they are producing useful recommendations and that there are no costly errors.",
            "month-posted": "May",
            "date-posted": "12",
            "day-posted": "Thursday",
            "time-posted": "6:33 AM",
            "replies": {}
        },
        "post-atulch-2": {
            "content": "Analytical frameworks are intended to assist analysts to organize their ideas and think logically in a systematic way. In a nutshell, analytical frameworks are models that try to guide and facilitate the process of creating sense. A visual representation of an analytical framework is common. In the humanitarian sector, frameworks are frequently required to be both needs and risk-oriented, allowing for the construction of a model that considers present and future humanitarian developments. A good framework guarantees that the data is organized in a way that allows for concrete results from the analysis. For example, analytical results can be used to answer queries like what are the most critical needs? and what are the existing gaps in humanitarian response? To define a theoretical framework, analysts must be selective. This means they'll have to figure out which variables are the most significant and useful, limiting the amount of data collected and analyzed. The impact of selection and process biases is reduced when the analysis is conducted utilizing frameworks because it is focused on the research objectives, methodical, complete, and transparent. A framework assists various stakeholders in analyzing data by allowing them to research the same phenomenon using the same categorization, reducing information duplication. Frameworks for analysis are essential for: The data analysis plan must reflect the categories that are offered in an analytical framework, including indicators, sources, units of analysis, and data gathering methods. A database's structure/data storage method In the event of collaborative analysis, the type of analytical results that must be achieved and agreed upon by different stakeholders A final report's structure",
            "month-posted": "May",
            "date-posted": "12",
            "day-posted": "Thursday",
            "time-posted": "6:31 AM",
            "replies": {}
        },
        "post-atulch-3": {
            "content": "You'll work with two sorts of data when conducting research and data analysis: quantitative and qualitative. It's critical to grasp the differences between the two because they demand different ways of data collecting and analysis. What is quantitative data? Any data that can be quantified is referred to as quantitative data. Quantitative data is everything that can be counted, quantified, or assigned a numerical value. Quantitative data can tell you how many, how much, or how often something happened for example, how many people attended the webinar last week? What was the company's revenue for the year 2019? What percentage of your customers utilize internet banking? You will execute statistical studies to examine and make sense of quantitative data. What is qualitative data? Qualitative data, unlike quantitative data, cannot be tallied or quantified. It's descriptive, meaning it's expressed in words rather than numbers. To address Why? or How? inquiries, researchers frequently turn to qualitative data. For example, if your quantitative data shows that a certain website visitor abandoned their shopping basket three times in a week, you'll undoubtedly want to find out why—which may require gathering qualitative data from the user. Perhaps you'd like to know how a consumer feels about a specific product; qualitative data can help with that. You're not simply looking at numbers in this situation; you're asking the user to explain why they did something or how they feel using language. Qualitative data also includes the terms or labels used to describe specific attributes or traits, such as describing the sky as blue or designating a specific ice cream taste like vanilla. What are the main differences between quantitative and qualitative data? Quantitative and qualitative data differ primarily in what they tell us, how they are obtained, and how they are processed. Let's summarise the main distinctions before delving deeper into each one: Quantitative data is numerical data that can be counted or measured. Language-related qualitative data is descriptive. Quantitative data tells us how many, how much, and how frequently something happens (for example, 20 people signed up for our email newsletter last week). Qualitative data can help us understand the why or how behind specific behaviors, or it might simply describe a characteristic, such as The postbox is red or I joined up for the email newsletter because I'm really interested in hearing about local activities. Quantitative information is static and universal, whereas qualitative information is subjective and dynamic. For example, 20 kilograms is an objective truth. Two persons, on the other hand, may have very different qualitative assessments of how they experienced a specific event.Quantitative data is collected by counting and measuring. Interviewing and observing are used to gather qualitative data. Statistics are used to examine quantitative data, whereas qualitative data is studied by organizing it into meaningful groups or themes. Quantitative vs qualitative data: methods of analysis The way quantitative and qualitative data are examined is another significant difference. Quantitative data can be used for statistical analysis and quantitative calculations, but qualitative data is normally evaluated by classifying it into relevant categories or themes. Quantitative data analysis The type of data you've collected and the insights you wish to unearth will determine how you analyze your quantitative data. Statistical analysis can be used to find trends in data, determine if there is any form of relationship between a group of variables (for example, does social media spending correlate with sales), compute the probability to properly forecast future occurrences, and much more. Some of the most popular methods used by data analysts include: Regression analysis Monte Carlo simulation Factor analysis Cohort analysis Cluster analysis Time series analysis Qualitative data analysis: Making sense of unstructured data is the goal of qualitative data analysis (such as large bodies of text). Because qualitative data cannot be measured objectively, it is subject to subjective interpretation and hence necessitates a distinct analysis strategy. Thematic analysis is the most common type of analysis done with qualitative data. The data is essentially coded to find recurrent keywords or subjects and then sorted into meaningful themes based on these codes. Sentiment analysis is another sort of analysis that attempts to categorize and analyze the emotions expressed in textual data. This enables organizations to determine how customers feel about various parts of the brand, product, or service, as well as how widespread these feelings are within the overall customer base. Qualitative data analysis has a bad reputation for being time-consuming. However, currently, the process may be largely automated, and there are numerous tools and software programs available to aid in the interpretation of qualitative data. Check out this list of the most useful qualitative analysis tools on the market to learn more about qualitative analysis and what you can do with it.",
            "month-posted": "May",
            "date-posted": "12",
            "day-posted": "Thursday",
            "time-posted": "6:29 AM",
            "replies": {}
        },
        "post-leminhducng": {
            "content": "Easy-to-use tools such as Google Trend have a lot of advantanges in data analytics. Benefits of using Google trends include the convenence of use, easy availability, easy transportation of data, ability to access many data sources, and so on.. In particular, Google Trends has become a very popular data source and an analytic tool among researchers of a wide variety of fields over the last decade. However, the limited understanding of the methods may raise some issue of the quality data that is used. The quality of data is a wide multidimensional concept affecting diffrent perspecties of the data source [1]. According to Karr, the main quality dimensions of data are accuracy, completeness, consistency and validity. Take example of Google Trend as an Easy-to-use tool - Google Trend posts an issue in terms of accuracy, derived from the fact that the reports are generated from sample of searches made by users [2]. The sampling methods are not disclosed or fully understood, so it is impossible to quantify the sampling error - Google Trend presents an issue in term of completeness, as sometime the value zero is reported when search did not reach a minimum threshold. The issue related to the accuracy of the data could become a significant source of bias, if it is not corrected.",
            "month-posted": "May",
            "date-posted": "16",
            "day-posted": "Monday",
            "time-posted": "9:43 PM",
            "replies": {}
        },
        "post-Robert-Kemp": {
            "content": "There was extensive (and lengthy) set of readings this week - all very pertinent and interesting and this gives plenty of material to post into our CMAPS. The readings regarding sentiment analysis I found particularly interesting and informative. I have to admit I never thought about if it was even an area of research but it makes sense and there has been many papers published in this area The Monkey Learn site is an easy read and comprehensive [1] regarding sentiment analysis and recommended to view.  Furthermore, Mantyla et al (2017) provides a comprehensive summary of the growth of sentiment analysis along with a review of past papers identifying that there was a 50 fold grown in ten years between 2005 and 2016 [2]. Personally, I have used sentiment analysis very casually in group settings, especially during these COVID times on virtual calls to quickly receive feedback if the venue and topic was relevant and if we need to continue the conversation or change to another topic.  Using one of the collaboration tools, like MS Teams allows for a show of hand or in the chat window to place an emoji. I found this is an effective method and one enjoyed by the meeting participants too.  The available list of emoji's is extensive too -  one such link from the Monkey Learn site https://unicode.org/emoji/charts/full-emoji-list.html#1f970  [3] - as I found out from my kids some of them have different meanings than you first think ... lol lol I found the Gapminder quiz interesting - you might too at https://www.gapminder.org/ [4] Gapminder nicely displays inherent misconceptions that exist and this is a good example what can be done with data. Regarding tools, I know that Microsoft is aggressively marketing Power BI and for large enterprise accounts are essentially providing 'no cost' licenses for Power BI which saves a lot of money for each organization over other products such as Tableau (which was purchased by Salesforce in 2019) [5].  This MS Power BI Link provides a summary along with Gartner's Magic Quadrant for Analytics and BI platforms[6].  Power BI appears to have many if not most of the desirable features and can be 'test driven' at no cost and is available with a very limited free license[7] .  If there is one analytic tool to learn, my thinking is to invest time into MS Power BI - what are your thoughts?",
            "month-posted": "May",
            "date-posted": "12",
            "day-posted": "Thursday",
            "time-posted": "9:49 AM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "I've had to act as the PowerBI expert for at least one BC Government Ministry.   It works pretty well, but many of these BI tools are wildly different from each other.  An organization had better be certain that something like PowerBI is the tool they really want to go with, before investing time and money adopting it.  Expertise with the tool is only nominally transferable to other tools. On the other hand, it interfaces well with the MS Office suite of tools, which the vast majority of offices use.  So that is one advantage!",
                    "month-posted": "May",
                    "date-posted": "15",
                    "day-posted": "Sunday",
                    "time-posted": "9:40 AM",
                    "replies": {
                        "reply-Robert-Kemp": {
                            "content": "Ken, thanks for your comments and interesting insight. What is the most compelling BI feature of Power BI? Are you able to elaborate and explain to us laypeople what behaviour Power BI has versus other similar tools and why organizations should be 'certain that Power BI' is the tool they want to use? I know within organizations its politically charged when the decree is to only use Power BI instead of say Tableau but have no concept of the real issues?",
                            "month-posted": "May",
                            "date-posted": "16",
                            "day-posted": "Monday",
                            "time-posted": "1:41 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-kennethmc39": {
            "content": "The disconnect between ease of use and understanding of methods is an interesting topic.   In one sense the two are inversely related; ease of use is directly symptomatic of a tool or piece of software guiding a user through the process.    Were the tool to require knowledge of the methods, it would be much more difficult to use for laypeople. I feel that it is important to make that distinction; the question is about persons having limited understanding of the methods employed by the tools.  It could be said that even with knowledge of the methods, some actions performed by these tools would still be quite challenging to manually perform. However, those people having that knowledge do not factor in to this discussion. We are focused solely on persons having limited understanding of methods, but still wishing to interact with data. One advantage of these tools is that they provide analytical access to data that would otherwise be out of reach of laypeople.  The statistics provided by Google Trends not only employ statistical methods, but also utilize data from an otherwise closed system.  That is, if a user wished to perform the statistical analysis themselves, getting access to raw data from Google would be quite challenging. Another advantage to these tools is that they remove much of the possibility of human error.  Manually performing such calculations, especially by laypersons, would provide much more opportunity to miscalculate or make a mistake.   A tool like Google Trends, on the other hand, is nearly foolproof. Regarding disadvantages, one major drawback to these tools is that they require faith in the system.  The user must accept the answer given, and assume that the tool was coded to be accurate and that the tool was coded without inherent fiscal or moral bias.  For example, Wagner, et al, noted that Google Maps presents a number of biases, even outside of the paid advertising inherent in the tool. [1] Further to this, the tools are often not transparent in their methods.  Statistics are easily manipulated, and this is especially true when the methods employed by a tool are opaque.   Given that Google is an advertising company masquerading as a search engine, it is conceivable that these free tools would be designed to add to the company's bottom line in some way.  In conclusion, these tools provide easy access to analysis of data, but require that the user accept whatever answer they are given. ",
            "month-posted": "May",
            "date-posted": "15",
            "day-posted": "Sunday",
            "time-posted": "9:13 AM",
            "replies": {
                "reply-Sheryl-Griffith": {
                    "content": "Good work. For the most part, I agree with your statement require that the user accept whatever answer they are given. Most people are using the tools because of their ease of use and don't necessarily have the time or inclination to understand the methods. When it relates to data, how often do we hear garbage in garbage out? I think this is such a fundamental statement, according to Lagoze, 2014 Similar to so many aspects of our modern digital culture such as journalism (e.g., the New York Times versus the flood of grassroots news blogs) and reference information (e.g., Encyclopedia Britannica versus Wikipedia), it is futile and even undesirable to seek a return to traditional, rigid control zones. Nevertheless, we are left with the challenge with Big Data to reap its benefits while simultaneously holding science to the same standards that it has been held to for centuries. Lagoze, Carl. Big Data, Data Integrity, and the Fracturing of the Control Zone. Big Data & Society, vol. 1, no. 2, 2014, p. 205395171455828., https://doi.org/10.1177/2053951714558281 Lagoze C (2010) Lost Identity: The Assimilation of Digital Libraries into the Web (PhD dissertation). Cornell University, Ithaca. Available at: http://carllagoze.files.wordpress.com/2012/06/carllagoze.pdf. Thanks!",
                    "month-posted": "May",
                    "date-posted": "15",
                    "day-posted": "Sunday",
                    "time-posted": "3:43 PM",
                    "replies": {}
                }
            }
        },
        "post-janineis1": {
            "content": "As a data analyst with a local non-profit, I have a love-hate relationship with easy-to-use, no code tools for data. For a bit of context, the organisation that I am currently working for is still in the beginning stages of transitioning into a data-driven company, thus, low- to no code software has strong appeal since the company strives to transition quickly and organically, where new programs are easily usable with very little learning time is needed to understand the software. While I can see the appeal to using easy-to-use tools, issues such as rigid analytics, a dwindling lack of talent, and security should be considered. No-code tools for analytics can certainly empower the public and make the transition to becoming more data-driven smoother as they can provide a high level understanding of insights that analytics can generate with various topics, from pandemic case rates to how many times a celebrity has been searched for within a day. However, the rigidity of easy-to-use tools can limit the amount of understanding as it can restrict users to only learning about analytics at a superficial level. Moreover, as we trudge on through the current societal need for instant gratification, users want quick answers or solutions to issues that arise. As the demand for software exceeds the supply of users who understand and are immersed in the field of analytics, the push for no code software is much more evident since it provides rapid-fire analytics without a need for higher education on how to use the tool. This lack of talent can constrict advances within the field as developers many choose to create different versions of the same program in order to keep up with the demand for quick and dirty analytics. The grossly generalised analytics from default plug and play analyses built into these software programs constrict the user to finding out other insights and perhaps even cost the company significant funding as they may have to look into more sophisticated tools to either find a workaround that actually meets the requirements of fulfilling their business need or one that provides complex analytics with high customisability. Finally, the issue of security and vendor lock with easy-to-use tools is something that companies may overlook. Since the company I work for is a non-profit, budgets are of high importance. Vendor lock, where vendors make a customer dependent on services, is greatly considered when making choices on which platform to use to further our business needs as finding workarounds can be costly. Moreover, the non-profit uses highly sensitive data and the issue of security is one that is also considered. In this case, choosing a reputable vendor is sought after as vulnerabilities in the data can essentially make or break the company. Thus, easy-to-use tools are often a last resort as there is usually limited control over the security of data. While easy-to-use tools provide a push towards a more data-driven society and provide useful insights, issues in terms of customisability and security must be considered. In this case, companies should strive to create adaptable easy-to-use tools that simultaneously pique a user's interest in learning more about the program and how they can become more self-sufficient with customisable tools that provide comprehensive insights on their data.",
            "month-posted": "May",
            "date-posted": "14",
            "day-posted": "Saturday",
            "time-posted": "5:41 PM",
            "replies": {
                "reply-abebayi": {
                    "content": "What a great post Janine! You touched on many very important points. In my field, I often hear consultants who rave about easy and free project management tools to which I quickly respond with there is no such thing as free. Plugging sensitive and privileged information into software to generate results for analysis is a major security risk. It is important for end-users to be weary as one software can as you put it make or break a company.  The question I have for you is, do you think a company that offers free service that is found to be selling data to third-party vendors should be held liable or does the responsibility belong solely to the company or the individual who utilized free services for its operations?",
                    "month-posted": "May",
                    "date-posted": "15",
                    "day-posted": "Sunday",
                    "time-posted": "5:30 PM",
                    "replies": {
                        "reply-janineis1": {
                            "content": "That's a great question - I think the onus is on the company for sure. In our consumerist society, transparency is key and using services is inevitable so I tend to err on the side that it should start at the source - in this case, the company. It's up to them as well to write terms and conditions that are accessible to the public in terms of using easily understandable jargon without any hidden fine print that can potentially harm an individual using the service.",
                            "month-posted": "May",
                            "date-posted": "15",
                            "day-posted": "Sunday",
                            "time-posted": "9:52 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-Robert-Kemp": {
                    "content": "Janine, thank you for your posting. You definitely have some good experience and knowledge of these products What are a few names of each type of tool that are 'no code' and the other type that requires 'code'?",
                    "month-posted": "May",
                    "date-posted": "16",
                    "day-posted": "Monday",
                    "time-posted": "2:12 PM",
                    "replies": {
                        "reply-janineis1": {
                            "content": "A couple of tools come to mind - Power BI and Kintone CRM. Both of these platforms boast a no-code easy-to-use tool for analytics with an option to create custom charts through editor features. I have just a little bit of experience with Kintone - in the short time that I used it, I found that it had a relatively easy-to-understand user interface and templates that were available to use for analytics. However, though they claimed to have options for customising the user interface and analytics, the default analytics were basic and it was pretty difficult to find the custom editor.   In general, both tools allow the user to create analytics using default created charts or pivot tables and while they offer customisable editors on the backend of these platforms, it is still difficult to create custom analytics to answer a specific business need. Additionally, the editors that allow for custom analytics require expert knowledge on programming languages which may require a company to outsource talent and therefore require more funding to do so.   Hope that answers your question!",
                            "month-posted": "May",
                            "date-posted": "17",
                            "day-posted": "Tuesday",
                            "time-posted": "11:04 AM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-Che-Little-Leaf-Matusiak": {
            "content": "This weeks learning activity and question is an interesting one.  I will discuss this topic from the point of view from my past experiences working with data in health care. In my previous life working in health sciences and patient triage, I would often need to find statistics, trends, and information on patients as reports from an application to help the medical staff complete the triage process of patients.  I found this information system good in the sense that I was able to find the general information that was needed from the categories and selections it allowed.  However, this application had limitations.  There could be so much more done and seen with the data if more capabilities and processes were extended to the user.  The fact that this application is easy/user friendly (and a probably more financially cheaper method) for all users with diverse technical abilities is a positive because it ensures data access equality for health care workers.  More people can find data to fulfill their job duties accurately.  Many did not have in-depth technical skills.  However, this came at several costs.  As a health care worker needing data, I had less control over this computer application and could only do what the tool allowed me to do.  If a process was created that better facilitated user queries, analytics, and data manipulation tools, more better insights could have been provided between health care colleagues.  Sometimes the reports generated by this application provided information that was not sufficient enough, effective, or was confusing, or did not show one version of a truth.  Health care workers at times needed to rely on the information technology specialists to get more advanced data analytics.  With the application trying to be user friendly, ensuring equality, it actually did not turn out to be completely user friendly/easy for all. It is great that more easy/user friendly tools are being created to equip many with the power of analytics, however further knowledge can be gained in knowing methods and not being limited by the tool itself.  These tools very well might draw you off course, could be inaccurate, bias, or confusing adding to the cost and time in fulfilling a goal: ensuring proper patient care, peoples lives depend on it.",
            "month-posted": "May",
            "date-posted": "16",
            "day-posted": "Monday",
            "time-posted": "1:37 AM",
            "replies": {}
        },
        "post-christianmu5": {
            "content": "Qualitative and Quantitative analytics are two key methods of by which data is collected and interpreted. What I found interesting is that the two can be used concurrently and complement each other, leading to a more quality result Quantitative Analytics Quantitative Analytics is generally concerned with measuring quantity such as weight, length, temperature, speed, etc. In addition, Data is randomly selected from large samples and then analyzed. After the collected data is analyzed, it is displayed either in a tabular form or using graphs and charts. The advantage of Quantitative Data is that it is more conclusive and objective in nature, and it can be applied to a general population. However, Quantitative Analytics has some limitations. For example, it cannot explain the why of a certain situation, nor can it uncover new concepts. Qualitative Analytics In Qualitative Analytics, the focus is on analyzing data that cannot be quantified such as color, gender, nationality, taste, appearance, etc. This analysis method helps to provide a deeper understanding of a situation by answering why a particular phenomenon occurs. There are limitations to qualitative analysis. For instance, it cannot be used to generalize the population. Small samples are used in an unstructured approach, and they are non-representative of the general population hence the method cannot be used to generalize the entire population. In conclusion, Qualitative analysis seeks to get a deeper understanding of a phenomenon while quantitative analysis seeks to test hypotheses and even give future predictions. The good thing is that both methods complement each other and where one is limited, the other can step in.",
            "month-posted": "May",
            "date-posted": "13",
            "day-posted": "Friday",
            "time-posted": "8:50 AM",
            "replies": {}
        },
        "post-Matthew-Basaraba": {
            "content": "In a world where tools are widely available and answers are made available easily I see this as the perfect storm for deception. Whether intentional or not and whether that one is deceiving themselves or others tools that are able to show patterns, trends or even answers leave open the option for misinterpretation when used by individuals without a full understanding. Without proper knowledge and training, no matter how easy a tool is to use, any value it could provide are hard to see used effectively. For example an employee who ran a marketing campaign could note that sales were up by X% the month after a new campaign ad may have incorrectly compared one month to the previous when other factors like seasonal demand were in play. Having trained users like data scientists or data analysts involved in sourcing data to draw conclusions from or utilizing extra techniques like A-B testing are very important for reaching conclusions or driving action. ",
            "month-posted": "May",
            "date-posted": "15",
            "day-posted": "Sunday",
            "time-posted": "10:45 PM",
            "replies": {
                "reply-Robert-Kemp": {
                    "content": "Matthew, thank you for your comments and providing your quotation - its one my favorites Similarly, 'you don't know what you don't know' (Socrates, Yogi Berra and Donald Rumsford all said something similar too and 'unknown unknowns' is a Risk Management concept  ) What are the best features or functionality one should look for in these tools?  Or do the users need to be better educated and informed about the data they are crunching?",
                    "month-posted": "May",
                    "date-posted": "16",
                    "day-posted": "Monday",
                    "time-posted": "2:02 PM",
                    "replies": {
                        "reply-Matthew-Basaraba": {
                            "content": "Thanks for the response. From my perspective the most important part of using any tool is making sure it's reviewable. Including data sets and formulas used for analysis can help review processes and allow for others to understand how conclusions were made. For example someone who ranks players in soccer could use any number of metrics but if they focused on offensive metrics like goals scored it might not paint a fair picture for players in defensive or supporting positions. If someone were to work on analyzing metrics the most important question one could ask themselves is likely What data am I not including?",
                            "month-posted": "May",
                            "date-posted": "22",
                            "day-posted": "Sunday",
                            "time-posted": "3:10 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-abebayi": {
            "content": "No submission",
            "month-posted": "No submission",
            "date-posted": "No submission",
            "day-posted": "No submission",
            "time-posted": "No submission",
            "replies": {}
        },
        "post-eugenegr": {
            "content": "Conclusions derived solely on trend information without analysis of the data or minimal understanding of the statistical methods may result in misinterpretation. The Misinterpretation of statistics (Statistics Canada, 2021) provides an overview of the possible misuse of statistics resulting from misunderstanding, poor alignment, and deceptive use of data. Greenland et al. (2016) complicates the issue of limited understanding of methods . According to the paper various statistical methods have caused confusion and frustration amongst experienced scientist. The paper even hints at a journals ban of certain statistical test and mathematical methods. The paper outlines the common misinterpretations of the P value (probability). The rapid creation of statistically proof or certainty, generated without any background knowledge, can be demonstrated using WEKA (Frank et al. 2016). The following example provides proof that the Naive Bayes Classifier will successfully classify Iris data without any understanding of the method or statistics .",
            "month-posted": "May",
            "date-posted": "15",
            "day-posted": "Sunday",
            "time-posted": "1:13 PM",
            "replies": {
                "reply-Robert-Kemp": {
                    "content": "Thank you your posting and a good link to Stats Canada ..however I am not understanding what point you are advocating I know that Naive Bayes is a statistical classifier and proven to work but you also state that conclusions derived solely on trend information without analysis of the data or minimal understanding of the statistical methods may result in misinterpretation If you provide any insight it would be helpful - it could just be me not having a deep understanding of the material.",
                    "month-posted": "May",
                    "date-posted": "16",
                    "day-posted": "Monday",
                    "time-posted": "2:22 PM",
                    "replies": {
                        "reply-eugenegr": {
                            "content": "I was considering the Google Trends examples and how easy it would be to support a misleading claim using the trends without any information on the dataset. The WEKA example used a toy data set and WEKA allowed the generation of a classifier without any specialized knowledge. In addition, WEKA provided statistical proof of the success. If this wasn't a toy data set, misleading claims could be made about the classifier or the strength of the data (intentionally or not) and the claims would be supported by statistics. The lack of information with respect to the data used in the generation of statistical or trend data is a concern. The statistics used to validate a conclusion may hide weaknesses in data, subsets generated from the original dataset, assumptions, and understanding of the statistics used. The outcome from a bad choice based on proofs derived from weak analytics, especially in automation, could be disastrous.",
                            "month-posted": "May",
                            "date-posted": "16",
                            "day-posted": "Monday",
                            "time-posted": "3:38 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-Sheryl-Griffith": {
            "content": " It's interesting that Google is now seen as a verb, how many times a day do we say to someone Google it or I googled it, but do we even stop to think if the information is trustworthy or consider the how, the analytics method used to gather this data? For the most part, people may not scrutinize or filter to ascertain relevancy. With easy-to-use tools such as Wikipedia, Reddit, or YouTube where subscribers can add or edit content, there is the possibility of maliciousness influencing the veracity of the information (de Laat 2016). I am of the opinion that one may not necessarily need to know the methods but at least do the groundwork to evaluate the reliability of the information. These websites also have the responsibility to users to ensure authenticity and preserve the public's trust.",
            "month-posted": "May",
            "date-posted": "13",
            "day-posted": "Friday",
            "time-posted": "6:18 PM",
            "replies": {
                "reply-suzanneva5": {
                    "content": "I recently did a marketing project related to information organizations. I chose Google as the subject. Google search is just one aspect of the organization now. It's basically a big index for the web. It's agnostic as far as what should appear. It's not going to flag a site, post, picture or web element so it doesn't appear in the results. It's up to individuals to do their research about any given subject. Although this isn't about Google, The Disinformation Project at SFU is interesting research (https://www.sfu.ca/communication/research/labs/the-disinformation-project.html).",
                    "month-posted": "May",
                    "date-posted": "14",
                    "day-posted": "Saturday",
                    "time-posted": "9:33 PM",
                    "replies": {
                        "reply-Sheryl-Griffith": {
                            "content": "Thanks for the feedback Suzzane. Interesting..there are posts there on fake news and disinformation. I agree it's up to the individual to do thorough research in order to eliminate the false and get to the facts.",
                            "month-posted": "May",
                            "date-posted": "15",
                            "day-posted": "Sunday",
                            "time-posted": "11:41 AM",
                            "replies": {}
                        },
                        "reply-abebayi": {
                            "content": "Thank you Sheryl for this insightful post. I would also add that the practice of shadow banning which is the partial or complete banning of comments by users (sometimes the users are not even aware that others cannot view their posts) that is prevalent on online platforms is an example of how easy-to use methods can be subject to bias [1]. This becomes especially cumbersome when you consider that researchers and decision-makers use social media analytics to make assumptions and conclusions on any given topic. As such, more than ever we need to understand the significant limitations implicated in such reports as government restrictions or assumed  posts that are banned can generate inaccurate data skewing final findings.  Do you think the positives of easy-to-use/access information that the likes of Google provide outweigh its negative impact? I would argue while it provides us with many benefits the danger that comes from people accessing information without being mindful of the algorithms behind their searches makes it quite controversial. ",
                            "month-posted": "May",
                            "date-posted": "15",
                            "day-posted": "Sunday",
                            "time-posted": "5:17 PM",
                            "replies": {
                                "reply-Sheryl-Griffith": {
                                    "content": "I agree that the subject is controversial and I believe this will become more prevalent in the future as additional tools come available, with focus areas of big data analytics and data science.",
                                    "month-posted": "May",
                                    "date-posted": "15",
                                    "day-posted": "Sunday",
                                    "time-posted": "7:32 PM",
                                    "replies": {}
                                }
                            }
                        }
                    }
                }
            }
        },
        "post-George-Adyrhaiev": {
            "content": "This is an interesting topic to discuss in the context of taking this course as well as being enrolled in a MSc degree. In our day-to-day, we are very used to relying on tools that have become possible through very complex development and implementation methods. I would typically argue that this ease of use of technology and solutions it provides us with is indisputably a good thing, as utilizing tools that help us make better decisions without even thinking of how they work is just convenient. Do you really need to know how Google Maps calculated the best path for you to take, or how electricity is being generated and distributed for you to have light on at home, or how authentication works for AU's Moodle or Landing? As long as we can access what we need and accomplish the goals we have, aren't we just dandy? Same can be told about a CEO of a company that relies on the dashboards or other data visualization solutions that their Data department has created for easier and faster decision-making. As long as those who create the dashboards ensure they are accurate, that's all that matters.  However, in the context of this course and the topics of this week's reading, it is extremely important to remember that one should always remain critical when dealing with easy-to-use data analytics and data visualization solutions. Same data can be interpreted in many ways, so understanding how the data has been obtained as well as what kind of data processing has been applied to it can shed some light on any bias that may be present in the dataset you've accessed. Better understanding where the data is coming from as well as how it is being processed to provide a beautiful chart gives you more confidence in the information itself, as well as ensures you're not misinterpreting the results.  So to sum up, the key positives of the disconnect between easy-to-use tools and limited understanding of methods that I can think of are: quicker access to the information provided via these tools, which can result in decisions or action that helps you achieve your goals. One doesn't have to take a LKA course in order to build a simple learning dashboard for students taking a course with tools like Tableau, or train an ML model using clustering algorithms that have been proven reliable for decades. At the same time, the negatives of such a disconnect are higher potential for misrepresenting the data (including not reading the chart properly) as well as missing important results due to skewed or inaccurate datasets that can be used by an easy-to-use tools. If you can't verify the validity of the data you access, it is difficult to trust enough so that you can bank on it.  ",
            "month-posted": "May",
            "date-posted": "11",
            "day-posted": "Wednesday",
            "time-posted": "11:13 PM",
            "replies": {
                "reply-janineis1": {
                    "content": "Hi George, I definitely agree with how easy-to-use tools can lead to misinterpretations of data. This current age of needing instant gratification is perhaps feeding into that as more and more people are wanting quick results without thoroughly analysing the context and data. Being critical of processes and the path to how a visualisation was created is indeed very important.",
                    "month-posted": "May",
                    "date-posted": "14",
                    "day-posted": "Saturday",
                    "time-posted": "6:08 PM",
                    "replies": {}
                }
            }
        },
        "post-aidanpo1": {
            "content": "My perspective on this topic comes down a lot to underlying algorithms and internal workings and how transparent the tools are.  There are a lot of easy-to-use tools and tutorials on the internet that will allow for a person to download large amounts of data and show them how to create a neural network to do a pre-defined task or even something more customizable.  An example of this could be something like Teachable Machine [1].  This website allows for machine learning models to be trained and saved all by using a simple online UI [1].  These models can do things like image processing and identification and sound processing and identification [1].  However, the site is not fully transparent with the way it works, or at least it is not easily accessible.  Going to the FAQ page will only say some bare bones explanation giving a brief overview of transfer learning and that the project is built with TensorFlow.js [1].  There are other sites and programs like this like Runway which allows for machine learning powered video editing [2], and SuperAnnotate which is a program that allows for an image to be processed to label all elements within an image [3]. I do think that having access to tools like this is overall a good thing.  It allows for those who don't have technical skill to make use of cutting-edge powerful technology to achieve the outcomes they would like.  Not having access to things like classification models would make labeling large amounts of data incredibly difficult for someone.  Having access to these tools can make a large amount of work fly by for everyone who uses them.  However, I do think that having access to tools like this also comes with an unfortunate side effect.  Many sites and companies are not totally transparent with how their tools work and what is going on under the hood that powers them.  I understand why though, some of the tools are purchasable or licenced and revealing all the inner workings would invalidate the need to buy the product.  It is also hard to communicate everything about a certain learning algorithm and model in a documentation page for a product that people are purchasing because they don't know how to create a machine learning model themselves.  This leads to some people using technology they don't understand and possibly incorrectly.  Of course, this isn't going to have crazy world ending outcomes, but it does mean that some people might be turned off from the power of machine learning or perhaps get too excited about the technology and what it can do.  It just comes down to information understanding and what to expect from the current technology and what it can do. I think more manual programming libraries like TensorFlow [4] and PyTorch [5] are great places to start as they offer fairly extensive tutorials and cover many bases like how a specific model works and how to implement it.  This also teaches what certain models are capable of and what to expect.  I'm not saying that it is for everyone but I do think for people who would like to know how machine learning models work underneath, I think it is a good place to learn.",
            "month-posted": "May",
            "date-posted": "14",
            "day-posted": "Saturday",
            "time-posted": "11:53 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "It seems like you and I are in agreement, as far as our general stances on this issue.  The methods employed by the tools aren't transparent, but that may not be a bad thing. Users looking to start investigating machine learning may find the math involved overwhelming at first.  A user may have a better chance of becoming invested in a topic if they have easy access to some basic functionality, rather than being run over by an avalanche of theory. I'm not sure I agree that transparency would reduce the need for people to buy these tools.  For something like machine learning, the process and coding is so complicated that I have to think a majority of people would still rather just use the software rather than recreating it.  Many of these tools and methods have open-source versions, but people still pay large amounts of money for commercial ML products.",
                    "month-posted": "May",
                    "date-posted": "15",
                    "day-posted": "Sunday",
                    "time-posted": "9:25 AM",
                    "replies": {}
                }
            }
        },
        "post-suzanneva5": {
            "content": "This week's readings gave me a flashback to a python course I took a couple years back and an information visualization course I took in spring 2021. In Translating learning into numbers: A generic framework for learning analytics, the authors discussed interpretation and critical thinking as required competences for a framework. Analysts should question data, including data that is absent. Without all pertinent information, you can jump to the wrong conclusion and cause an error, which could have expensive, and maybe legal, consequences. We've all been victim of bad interpretations, including visualizations. If the data findings are correct, they need to be shown in the most appropriate way to get the message across.  Data is ugly on Reddit can be quite funny. :)",
            "month-posted": "May",
            "date-posted": "13",
            "day-posted": "Friday",
            "time-posted": "11:40 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "You make a good point about the ramifications of bad data or an inaccurate presentation. It's not simply inconvenient, but could have legal or fiscal consequences.   As well, it would be so hard to tell that you were presenting incorrect data until something went wrong.  For that reason, I do agree that data analysis and presentation should be an exact science as much as it is an art.  The underlying methods must be well-understood by anyone wishing to utilize the data for data-driven decision making.",
                    "month-posted": "May",
                    "date-posted": "15",
                    "day-posted": "Sunday",
                    "time-posted": "9:43 AM",
                    "replies": {}
                }
            }
        }
    },
    "week-4": {
        "post-Moustafa-Mahmoud": {
            "content": "Tableau Public.      Provides tools to load data, choose visual mapping, view data, develop insights and share them .      Intuitive interface .      Provides wide range of visualisations and supports many different data formats and data servers Tableau has many analytical tools including Network Graph and Word Cloud visualization Gephi is created to analysis networks and draw network graphs (The Open Graph Viz Platform, 2022). .      Open source network analysis tool .      Builtin functions to filter, cluster and customise graph networks .      Uncovers patterns, outliers and trends in data .      Handles large datasets and databases .      Supports dynamic data exploration Gapminder .      Promotes understanding of statistics and other information related to global development .      Topics range from health, population, work, environment, education, economy, society, infrastructure and energy.      Intuitive .     uses the visual interactive approach to help analyze datasets (Avella et al., 2016). VisualizeFree .      Online and intuitive .      Used to find patterns in multidimensional data Connect a dataset as excel or text file .Then, prepare data including filtering , sorting , grouping, aggregating ,and create a visualization. Many charts supported includes Word Cloud Chart (A word frequency of occurrence in the data set) ICTA .      Web-based system for automated text analysis and discovery of social networks .      Content analysis of Internet forums and email lists .      Preprocesses text and identifies concepts with its frequencies It offering a set of text mining techniques coupled with useful visualizations. It has a  content-based method called name 'networks' which can be used to transform even unstructured Internet data into social network data. This allow us to analyze, and make judgments about, social connections in a virtual community. .( Gruzd,2011) Orange .      Open source data visualization, machine learning, and data mining toolkit .      Support visual programming or python scripts Orange provides text analysis and network analysis AAT.      Easy to use .      Tool to analyze student online behavior on a learning management system .      Provide general types of learning objects, annotated by user-defined controlled vocabulary .      Predefined set of queries for student patterns, also allow definition of custom analytics queries .      SQL accessible database the Academic Analytics Tool imports and analyse LMS data without technical knowledge required (Ross et al., 2016). It can represent the analysis result in table structure. User creates a project then import data from a single LMS then create a dataset then create a pattern (similar to SQL view). Then, select attributes that is involved on the study. The analys dome by pressing Perform Analysis ",
            "month-posted": "May",
            "date-posted": "30",
            "day-posted": "Monday",
            "time-posted": "1:57 PM",
            "replies": {}
        },
        "post-atulch": {
            "content": "WEEK 4: Analytics tools/systems Modern organizations use data analysis as an important strategy. It is difficult to choose the right data calculation tool because no single solution can meet all your needs. Before exploring the various tools, there are a few things to consider. To get started, you must first understand the types of data your company wants to review, as well as your data integration requirements. Additionally, before you can begin data analysis, you will need to select the data sources, as well as the tables and columns within them, and return them to the data repository to create a single true mathematical source. Data security and data management are also important. When sensitive information is transferred between departments, access control and consent measures should be in place. Business Intelligence (BI) tools, for example, are a feature I would like to see in analytics tools. Modern Business Intelligence (BI) solutions help to visualize and comprehend data. The BI tools fall into three categories, according to Gartner: . Data acquisition, time reporting, simulation models, performance management, and other complex analytical skills are all done through online analytical processing, or OLAP. . Visualizations, reports, and dashboards are all examples of how information is delivered. . BI integration controls metadata and provides an environment for improvement to help you execute your strategy. The purpose of BI technology is to help organizations access data through processes such as data mining, predictive modeling, and natural language processing. Trends such as embedded statistics and data visualization are also emerging, allowing non-technical people to have access to information that previously required IT support. Citizen data scientists may use their understanding of data, statistics, and business to solve key business problems because of these easily accessible platforms. Key Skills: . Infographics . Future modeling . data mining . Predictability . Fully automatic reporting . Dashboards can be customized . Other data sources and forums can be integrated. . Data quality management . Native language analysis (NLP) . Outcome management . Random research . Simulation models . Budgeting Examples: 1. Power BI by Microsoft Microsoft Power BI is a powerful business intelligence tool that works with multiple data sources. Users can create and share reports, dashboards, and visuals. Users can create a Power BI application in a collection of dashboards and reports for easy use. Power BI also integrates with Azure Machine Learning and allows users to create automated machine learning models. 2. BusinessObjects from SAP SAP BusinessObjects is a business intelligence suite that combines data acquisition, analysis, and reporting tools. The tools are designed for business users who are not technical, but can also handle complex analysis. BusinessObjects works with Microsoft Office products, allowing business analysts to switch between applications such as Excel and BusinessObjects reports easily. Self-help statistics are also possible. 3. Sisense Sisense is a data analysis platform designed to assist both technology developers and business analysts in analyzing and visualizing all their company data. It has a large number of drag and drop features as well as interactive dashboards. Sisense platform is distinguished by its custom In-Chip technology, which improves computing using CPU caching instead of slow RAM. This can result in a quick calculation of 10-100x in some cases.",
            "month-posted": "May",
            "date-posted": "17",
            "day-posted": "Tuesday",
            "time-posted": "7:26 AM",
            "replies": {
                "reply-abebayi": {
                    "content": "Hello Atul thank you for a great post. You did a great job going beyond describing the tools to explain the important factors that factor into a great analysis tool such as data source and access control. You also made a good point to mention that the appropriate tool relies on the type of data your company views. Most of the tools covered in this week's reading were out of my scope of expertise and I found it difficult to evaluate the tools. Hence why I decided to lean on Youtube tutorial to give my feedback based on quick tutorials.  With respect to the three tools you mentioned above, do you have a preferred tool? If so, what makes it most agreeable to you?",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "12:22 PM",
                    "replies": {}
                }
            }
        },
        "post-leminhducng": {
            "content": "Which tools/systems did you like? I used QlikView (now named Qlik Sense), an analytic and business intelligence tool, in some of projects before. QlikView provides integration into any relational DB, converts raw data into knowledge base, analyzes data, offers data visualization and dashboards in a meaningful and innovative way. One of the features the I like the most is that Qlikview support multiple sources of data into a single application. QlikView works on an in-memory Associative Model, you may not need IT Professionals if you can write SQL query and understand the data. Qlik View is now provided as cloud-based analytics system under the new name Qlik Sense which combined the power of cloud platform with business intelligence and AI analytic tools. You may refer to https://www.qlik.com/us/products/qlik-sense What type of functionality would you like to see in analytics? I believe that natural language process is changing data analytics. NLP has the potential to make both business and consumer applications easier to use. Software developers are already incorporating it in more applications than ever, including machine translation, speech recognition, sentiment analysis, chatbots, market intelligence, text classification, and spell checking.This technology can be especially useful within data analytics, which analyzes data to help business leaders, researchers, and others gain insights that assist them in making effective decisions.",
            "month-posted": "May",
            "date-posted": "23",
            "day-posted": "Monday",
            "time-posted": "9:10 PM",
            "replies": {}
        },
        "post-Robert-Kemp": {
            "content": "Regarding analytical tools, I put forth that the safe bet is to understanding Microsoft, Google and AWS. I agree with Ken's comments that each tool is a separate lengthy process to understand before one can confidently speak about its capabilities. There could be better tools but if you know 2 of the 3 or 3 of the 3 then you have a solid understanding and you can quickly adapt This MS Power BI Link provides a summary along with Gartner's Magic Quadrant for Analytics and BI platforms[6].  Power BI appears to have many if not most of the desirable features and can be 'test driven' at no cost and is available with a very limited free license[7] .  If there is one analytic tool to learn, my thinking is to invest time into MS Power BI. The last 5-8 years Microsoft, according to Gartner Group has taken a commanding lead in the BI analytics space - anyone care to comment about Tableau and its capabilities as compared to Power BI? Also, Suzanne mentioned that Lynda.com (owned by Microsoft/Linked in) offer courses on Azure Data Analytics - these are free to AU students - the link is on AU homepage under GENERAL INFORMATION on the right hand column at   https://my.athabascau.ca/render.userLayoutRootNode.uP I am going to start listening to these to gain some understanding of the Azure product Google analytics also has a analytics program, Google refers to it as their analytics academy at https://analytics.google.com/analytics/academy/ Coursea also offers a Google Data Analytics Professional certificate at https://www.coursera.org/professional-certificates/google-data-analytics Given that I have absolutely no knowledge or background in analytics this seems a reasonable place to start - Has anyone tried the Azure or google course thanks ",
            "month-posted": "May",
            "date-posted": "22",
            "day-posted": "Sunday",
            "time-posted": "9:15 PM",
            "replies": {
                "reply-Adaobi-Emoka": {
                    "content": "I just happen to be online and seen your post.  You are right about if you know 2 or three, you pretty much are set in grasping easily other  analytic tools. I use LinkedInLearning (former Lynda.com), it's also free for those who reside in Edmonton and have an Edmonton public library card. I use this in tandem with Udemy.com. I actually learned a lot of about Power BI from Udemy, even though it's not free.  I had a very short stint with Tableau when I worked with the City of Edmonton. It's a great tool and handles huge data really well.  For Power BI, if you've worked with Excel, you will find the layout a bit similar to the power query studio in Power BI. One aspect, I really think it's important in leveraging the capabilities of either tool, is understanding Data Modeling.  I started using IBM's Cognos AI assistant Assistant - IBM Documentation which is great when you have the right data. This tool sometimes has a hard time producing anything meaningful if your data or your question isn't specific enough. I like that Tableau and Microsoft have an available community, the plethora of tutorials, classes and information unlike IBM Cognos. Thanks for the links. I'll check them out.",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "10:33 PM",
                    "replies": {}
                }
            }
        },
        "post-kennethmc39": {
            "content": "I initially set out to write about my experience with each of the example tools in some depth. However, I quickly discovered that learning to use each of these with any degree of competency is a project unto itself.   It's easy enough to slither around on the surface of some of them, but to dive deep would take weeks. I will say that I've used PowerBI extensively in the past, and Visualize Free is a very similar tool.  I appreciate that it's entirely web based.  In this, the age of Web 3.0, it's almost surprising to have to actually install a tool or piece of software. My experience with PowerBI was as the product owner for it in the system, so I learned both how to use the software, and how to teach others how to use it. It's not a bad tool, but unsurprisingly there's a cost associated with using it. Gephi was interesting but also overwhelming.  I used one of the datasets from Gephi's dataset repository on Github.  I did manage to get it to produce a graph, but the amount of data on the screen was overwhelming. You sort of end up with a chicken and egg scenario.  You can't really choose a tool until you know all about them, but you can't know all about them until you've chosen one at a time and sat down to learn it. Gapminder was… interesting.  It felt like there was some bias behind the explanation of some of the questions/answers.  Like oh actually only 6% of the world's plastic waste ends up in the ocean . Well that's still a hell of a lot of plastic.   Interestingly, apparently Google owns Gapminder, and has since 2007. [1] Academic Analytics didn't load at all, so I can't say much about that. I found a list of free Data Analytics tools that I'll be exploring in the coming weeks.  Most of these weren.t on the list from this week's coursework, so it'll be a useful jumping-off point for some of the assignments.",
            "month-posted": "May",
            "date-posted": "21",
            "day-posted": "Saturday",
            "time-posted": "9:53 AM",
            "replies": {
                "reply-janineis1": {
                    "content": "In terms of completely web-based tools, I think I prefer desktop apps more! There are some functionalities that are missing from web apps but I am probably just overlooking or only understanding how to use a certain software at a superficial level.  I agree with your option on Gephi. It was very overwhelming and I wasn't able to find and documentation on how to really understand and take insights in from those types of graphs.  I actually liked Gapminder! The quiz tool was interesting to use and play around with - I think a tool that can lay out uncomfortable conversations to critically analyse are ones that are a step in the right direction. That said, the actual statistical tool program that you could download was very limiting (if we're talking about analysis in itself). It seems to only allow CSV or excel spreadsheets which doesn't seem promising at all since the world of data is continually changing. ",
                    "month-posted": "May",
                    "date-posted": "21",
                    "day-posted": "Saturday",
                    "time-posted": "11:34 AM",
                    "replies": {}
                },
                "reply-Sheryl-Griffith": {
                    "content": "Ken, I totally agree with you. It is important to take the time to learn the tools as each has its own factors and options. For work, I guess we are obliged to use whatever tools the business has invested in and any free software options we can find. Thanks for sharing the links, I will check out Visualize Free. Interesting that you are also training others with Power BI, that's awesome.  Have you dabbled in any other tools other than Power BI? I feel like I have just touched the surface of Power BI. Top 24 Tools for Data Analysis and How to Decide between Them. Stitch,https://www.stitchdata.com/resources/data-analysis-tools",
                    "month-posted": "May",
                    "date-posted": "21",
                    "day-posted": "Saturday",
                    "time-posted": "4:19 PM",
                    "replies": {}
                },
                "reply-suzanneva5": {
                    "content": "I agree with the chicken and egg scenario. I removed Gapminder from my list entirely as I just don't understand how it can be used by us for analytics. I checked out Academic Analytics and that URL hasn't been in use for a couple years.",
                    "month-posted": "May",
                    "date-posted": "21",
                    "day-posted": "Saturday",
                    "time-posted": "11:58 PM",
                    "replies": {}
                },
                "reply-Robert-Kemp": {
                    "content": "Ken, thanks for your posting and knowledgeable insights based upon your solid practical experience. I will check out the links you have provided The fact about Gapminder and that Google is involved was interesting ...it led me down a rabbit hole and one of the founders wrote a book called Factfulness ... a top book in 2018 recommended by Bill Gates ...https://www.gapminder.org/factfulness-book/  - I am putting this on my reading list https://www.amazon.ca/Factfulness-Reasons-World-Things-Better/dp/1250107814 btw - its available to read at the open library  https://openlibrary.org/ btw Academic Analytics - there is a typo the URL domain - it is incorrect should be .com and not .ca",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "1:49 PM",
                    "replies": {}
                }
            }
        },
        "post-janineis1": {
            "content": "My current workplace has fully integrated into using the entire Microsoft Office suite so I am definitely partial to writing about Microsoft software such as Power BI or Azure. In my position at my workplace, I lean more towards the front-end side of dashboards as I have a great interest in the design portion of visualising data as well as the user experience factor. I find that Power BI has the capability to create interactive, modern, and slick looking dashboards with an intuitive interface but has its restrictions in terms of pricing for pro licenses, the fact that people need licenses to view dashboards, and cut-offs to the amount of customisability. That said, because Power BI is widely used professionally, I like how there is a plethora of tutorials online - it truly is a self-service type of platform that can allow users to make basic visualisations but also provides the capability to tack on more advanced visualisations through developer mode and the integration of DAX for more complex calculations. In terms of modern designs that do a great job of data storytelling, I highly recommend looking into Darkhorse Analytics - a local company in Edmonton that create, analyse, and consult clients on compelling ways to engage stakeholders with data. In most of the discussions posted on the forum, Gapminder seemed to be a tool that had a bit of controversy. For me, the overall concept of Gapminder is interesting - using data to drive critical thinking in the public eye is an excellent shift in the right direction. It was great to see that even if the data is challenging common misconceptions, it is also critical to question the type of data that is being digested. Can data ever be truly unbiased? In my opinion, no. Bias is innately part of being human but that being said, it was still a visually striking tool that was easy to use and can certainly whet a user's appetite into social research (and beyond). The downloadable tool that the organisation offered however... a whole different story. This tool was very rudimentary compared to Tableau Public or Power BI. It was superficially a great beginner statistical tool but had limitations in many areas such as modern design and file types that were able to be imported in the tool. Gephi was basically the opposite of Gapminder in that it was a highly technical tool that is geared towards specialists and researchers. Although it offered a developer's playground (if you will), the lack of documentation on how to derive insights apart from looking at the density of each node in the graphs themselves was not promising as an all-around helpful tool. I felt like I needed to take an entire course on network analysis itself just to be able to understand what was happening with Gephi! Overall, I think the functionalities that I look for as an individual working with and digesting data daily is a user-friendly interface to create basic visualisations but also provide extensions with practicing how to create with the tool and learning about the tool as it evolves with the ever-changing world of data. One example that I found helpful through my workplace was the Salesforce Trailhead Learning Hub. This particular hub was quite different from other e-learning platforms that I have used as it provides a great mixture of different media (readings, videos, charts etc.) but also provides an area to put theory into action and where you can learn hands-on. This was particularly beneficial because it allows the user to digest the information that was given but also put it into practice on the fly. An integration of theoretical and hands-on learning is definitely something I lean towards when looking for analytical tools to use in my professional and personal life, if needed.",
            "month-posted": "May",
            "date-posted": "23",
            "day-posted": "Monday",
            "time-posted": "12:20 AM",
            "replies": {}
        },
        "post-Che-Little-Leaf-Matusiak": {
            "content": "I will discuss this post from the business/managerial/board point of view.  Tableau is my favorite analytics tool.  This is based on my past and present professional experiences exploring different tools.  Tableau creates well done visualizations based on the data that is uploaded to the program. (Bonthu & Bindu, 2017)  It is able to handle big data datasets and you do not need to have extensive knowledge in a programming language to use this tool such as in python's pandas, scikit-learn, and matplotlib visualization libraries. (Bonthu & Bindu, 2017)  Being able to accurately communicate a story from data through visualization to businesses in a relevant sensible way to achieve operational goals and good decision making is key. (Bonthu & Bindu, 2017., Jena, 2019) Bonthu & Bindu (2017) describe an analytics development cycle which includes a period of data discovery, preparation, modeling, and communication.  During the discovery phase, the business decides what data scope is needed to accurately make forecasts which aid in knowledgeable business decisions. (Bonthu & Bindu, 2017)  Additionally, there is a period of tool-technology exploration that occurs in this phase to carry out this plan. (Bonthu & Bindu, 2017)  The preparation phase answers the question: Is the data passable enough for modeling? (Bonthu & Bindu, 2017)  The modelling and communication phases test the appropriateness of diverse analytic tools and communicates output results of these tools, respectively. (Bonthu & Bindu, 2017)  According to Bonthu & Bindu (2017), Tableau is the prevailing tool for business data visualization in the analytics process.",
            "month-posted": "May",
            "date-posted": "22",
            "day-posted": "Sunday",
            "time-posted": "11:53 PM",
            "replies": {}
        },
        "post-christianmu5": {
            "content": "Data Analysis tools I always like to compare Data mining to mining gold or any other mineral. To be successful, one needs good tools. It saves time, and cost, and most importantly it gets you exactly what you want. In Data analytics, there are also several tools that help to collect, organize, and analyze data. However, As this week's summary stated, data analysis tools in this context refer specifically to tools that help to analyze data. I looked at each of the tools provided and one that I fell in love with is Orange. First of all, it is free open-source software. I found it user-friendly, easy to navigate, and to learn. It has several widgets which are used for displaying data in several forms and helps to easily read, visualize and compare data. In addition, Orange gives a more interactive and fun atmosphere. It's simply cool! In conclusion, I think it's too early for me to say what functionalities I would like to see added. I feel that I need to be using these tools more so I can better see their limitations. But One thing that I am primarily concerned about is the security of the data, especially for open source tools. I just hope that Data scientists and developers will all work together to ensure that the Data is not only mined and analyzed in the best possible way, but also that it is safe.",
            "month-posted": "May",
            "date-posted": "24",
            "day-posted": "Tuesday",
            "time-posted": "1:21 AM",
            "replies": {}
        },
        "post-Matthew-Basaraba": {
            "content": "As a software engineer, I can bring a different perspective than others have provided so far in terms of analytics tools. Whereas this class may focus more on analytics made from existing datasets and tools that work well with providing insights, in my work I am much more exposed to transactional data and tools for monitoring and measuring service performance. Two tools, Prometheus and Graphite, are used to collect and store transactional data at scale, all of our APIs will collect metrics such as response times, success rates, at scale requests per second, and server resource usage (CPU, RAM, storage). Graphite and Prometheus provide easy to use time series data representations of these events so one can monitor and gain insights into server health, user retention, and more. When combined with proper testing methodologies, it becomes easy to prove value in architecture changes of servers or APIs by showing changes in server resource usage, server response times, or increases in usage through requests per second. While both Prometheus and Graphite have visualization options, it is also very common for these tools to be paired with a dedicated visualization tool which to my knowledge is most commonly Grafana, a tool with a large community which shares premade dashboards for both Graphite and Prometheus[1]. Of the tools mentioned in this week's Unit, I think a large trade-off of free or trial applications is the ability to link multiple data sources. Of the tools listed I saw that Tableau seems to be the most feature rich especially for connecting and working with a plethora of data sources, the trade-off though would be they have the largest jump in pricing structure where all users need licenses to work on and view dashboards[2] where other tools may choose to only charge users who wish to work with and publish dashboards. While not an issue for usage with the free versions of this software if someone were to learn Tableau and bring that knowledge set into their work, Tableau may benefit from having presented a more user friendly free tool rather than a more focused tool like Gephi or Gapminder that appear to focus on creating specific types of visualizations from data.",
            "month-posted": "May",
            "date-posted": "22",
            "day-posted": "Sunday",
            "time-posted": "3:02 PM",
            "replies": {
                "reply-aidanpo1": {
                    "content": "I worked with Graphite and Prometheus when I worked at IBM.  I thought that they worked very well in tandem with Grafana as well. I personally thought that the Elastic Search family of products was easier to use and provided better dashboards but I do think that is really up to personal preference.",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "5:14 PM",
                    "replies": {}
                }
            }
        },
        "post-abebayi": {
            "content": "Since my professional experience has been specific to the legal industry, the tools/systems that I am familiar with are quite limited. The software that is used for practice management in many firms which center around matter and billing management tools is Aderant.  The system facilitates easy-to-use billing systems but moreover, it provides invaluable analysis for stakeholders providing them with better visibility of their overall practice including profit analysis and matter management. The reason I like this tool is that it provides a comprehensive solution for all - by improving the billing process from timekeepers to billing coordinators and accounting it helps capture accurate data in real-time which I believe is the first critical step in effective data analysis. The functionality of Aderant also includes providing tools for the Business Intelligence team that generates reports to better make informed business decisions. I have since worked with other tools in the legal service industry which aim to achieve what Aderant offers. However, the user-friendly and logical setup of Aderant in my opinion is superior to others such as Progress and Acumin.   I have also briefly examined two select tools mentioned this week: Tableau: I have never used this before but I decided to watch Youtube tutorial to gain some familiarity with the tool. https://www.youtube.com/watch?v=jEgVto5QME8 While the tool seems like it offers flexibility of options in the presentation of analysis, I can see that it may be challenging for a beginner. Once you gain a certain level of familiarity with the tool, it may be a powerful tool. Orange: I explored Orange similarly. At the first glance, the tool has a clean look and its categories are logically organized. Although I have never used it before, I can easily follow the logic in the tutorials. It seems like a great tool for data scientists. https://www.youtube.com/c/OrangeDataMining",
            "month-posted": "May",
            "date-posted": "22",
            "day-posted": "Sunday",
            "time-posted": "11:13 AM",
            "replies": {
                "reply-Robert-Kemp": {
                    "content": "Abe, Thanks for your posting and your excellent insights - this level of detail I found very helpful. I agree that a user-friendly GUI and logical approach is best for these products- especially if users are only occasional users ... perhaps that is why I have only used excel to to sort data to find a quick answer ... Thanks for sharing the links and I will check them out.",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "1:16 PM",
                    "replies": {}
                }
            }
        },
        "post-eugenegr": {
            "content": "Miner et al. (2012) identified seven processes of text mining as follows: Information Retrieval Web Mining Concept Extraction  Natural Language Processing Information Extraction Document Classification  Document Clustering In addition to the text mining focus areas, ease of use, availability of extensions, and free access were significant considerations for the tools. Based on this, RapidMiner, KNIME, and Orange are the front runners. All use a flow-based programming style (Morrison, 1994). Gephi would not activate on the MacBook Air, possibly a backward compatibility issue. I may still consider investigating this software if time is available. The flow-based programming style can prove to be cumbersome depending on the task, specifically data preprocessing. This area may require an external preprocessing of the data before analysis. What type of functionality would you like to see in analytics tools/systems? Automation of the preprocessing of qualitative data would be a time-saving functionality. Especially the interpretation of markup languages in documents, extraction of related texts, and association of information based on document organization (replies). At the moment, I have been unsuccessful in finding extensions for the above tools that would satisfy these requirements.",
            "month-posted": "May",
            "date-posted": "28",
            "day-posted": "Saturday",
            "time-posted": "3:36 pm",
            "replies": {}
        },
        "post-Sheryl-Griffith": {
            "content": "Here are some of the tools and systems I have researched and liked best. Orange: I was able to download the software on my work computer, which was interesting in that the admin security wasn't required, and I was able to share it with my work colleague. I viewed a couple of YouTube videos and will be building interactive data visualizations and data analysis workflows using the software at work. The platform came with data sets that can be used for exercises. Tableau: Deemed as one of the leading analytics platforms used by businesses today for visualization. I could not download Tableau Public on my work computer because of admin security but was able to on my home computer. I got a few ideas from some of the vizzes I will use in my dashboards. Qlik: The Qlik suite comprises QlikView and Qliksense. I have been exposed to both, built a dashboard in QlikView, and was stepping it up with Qliksense. Qliksense has added features such as augmented intelligence and advanced data prep capabilities. However, the business has moved away from the Qlik suite to MS Power BI. Microsoft Power BI: One of the newer business analytics platforms. My organization has been utilizing the software for some time to build dashboards, visualize data, and do quick analytics. Still, we are expecting to use it for AI and predictive analytics such as inventory forecasting and customer insights. I am currently working on a project to migrate our asset solution from Oracle (data lake) and Business Objects (SAP Business Objects BI) systems to Azure and Power BI. Microsoft Azure:  I am just learning Azure, and I am excited to understand what this tool can provide in business analytics. There are some features, such as Azure IoT and Cloud-scale analytics,  which can be incorporated into this asset solution. I am mainly looking to explore advanced analytics such as predictive analytics, sentiment analysis,  artificial and augmented analytics, machine learning, and more data visualization in terms of analytics tools and systems. In my job, I aspire to drive analytics through our SAP system, however, it is currently the SAP S/4HANA® application, but this can be done through SAP Analytics Cloud. I will present a business case to our management to invest in the SAP Analytics Cloud version. I found this link helpful in understanding the cloud computing terms. Cloud Computing Terms | Microsoft Azure",
            "month-posted": "May",
            "date-posted": "21",
            "day-posted": "Satruday",
            "time-posted": "3:41 PM",
            "replies": {
                "reply-aidanpo1": {
                    "content": "I have actually been pretty interested in Azure recently because it's quickly rising in ranks for powerful tools over Google and even IBM alternatives.  I also think that the documentation is pretty great but I don't really have a personal reason to use it.  I do think that it does have great applications in data analysis though.",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "5:11 PM",
                    "replies": {}
                }
            }
        },
        "post-George-Adyrhaiev": {
            "content": "The practical usage of data analytics tools is quite new to me. I've had some limited experience with Tableau before, as well as built dashboards in Jira, however, I had never used tools like Gelphi, Netlytic or Orange before. In the past few days I've signed up and downloaded Gelphi, Orange, Tableau Public, as well as familiarized myself with the functionality of tools live Delve, Visualize Free, and enjoyed some eye-opening facts from Gapminder. Out of the above tools, the most confusing one that I found is Gelphi. This social network analytics tool is seemingly very powerful (especially if you watch their promotional video), however, I struggle to understand the relationships between the nodes, especially since I do not know the details of the sample data set I used. The quickstart guide [1], helped make sense of the user interface, but I'll be honest, there is still a lot to learn about the tool and its functionality and terminology used in order to be more efficient with it. I used the suggested Git repo with the public datasets for my testing.  On the other hand Orange, a tool that can be used for data mining and visualizations has surprised me with its robustness when it comes to the choice of available algorithms [2]. The concept of visualized workflows is a genius idea as it visualizes transformation steps to the original data set, as well as shows the entire pipeline that has been built to visualize or present the data. I am amazed that the tool has been developed as a free tool by the University of Ljubljana.  Tableau Public makes the creation of data visualizations and dashboards so much easier than most other tools. With some simple drag-and-drop, you can create beautiful visualizations that can help you tell the story of your data. Being one of the more popular data visualization tools, I can see how beneficial it can be for us to spend more time learning how to use it.  Additionally, I will also be looking more into Google Data Studio (some of my teams have been using it before with data coming from BigQuery), Amazon Quicksight and Microsoft PowerBI as commercial solutions that integrate well with the corresponding cloud infrastructure. I think these solutions meet most of the requirements I would have for an analytics system: easy integration with the data lake or data warehouse (data source), user management (permissions and data obfuscation/classification based on who can see what kind of data), relative ease of developing dashboards or models by data analysts, as well as an intuitive UI for end users. ",
            "month-posted": "May",
            "date-posted": "17",
            "day-posted": "Tuesday",
            "time-posted": "11:16 PM",
            "replies": {
                "reply-janineis1": {
                    "content": "I have a bit of experience with Amazon QuickSight - I found that it has great basic analytics but it was very limiting in terms of which fields you drag-and-drop into field wells, what data types you can drag/drop, and had very little customising ability.  The platform, when compared to Tableau and Power BI, is not as slick or modern in terms of how dashboards can look. While they had an option to customise columns using query editors, the documentation on what type of programming language was natively used or using correct syntax to avoid errors was difficult to find.  I think one advantage of that software was how you can easily integrate with AWS cloud storage which is very convenient but there were many disadvantages that I found as I used the software.",
                    "month-posted": "May",
                    "date-posted": "18",
                    "day-posted": "Wednesday",
                    "time-posted": "11:51 AM",
                    "replies": {
                        "reply-George-Adyrhaiev": {
                            "content": "Thank you for sharing your experience! Appreciate the insight into Amazon QuickSight as well as Tableau and PowerBI. I can definitely see how the main advantage of QuickSight is indeed its easy/native integration with AWS. For companies heavily invested in AWS infrastructure and that require more basic visualizations/dashboards it may be a good option, but for those with more complex needs sounds like Tableau is still the king among cloud-agnostic solutions.  Thank you ",
                            "month-posted": "May",
                            "date-posted": "18",
                            "day-posted": "Wednesday",
                            "time-posted": "4:49 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-abebayi": {
                    "content": "Thanks for sharing your thoughts on various analytic tools George! I appreciate that you actually downloaded and tried out some of the ones that you have not used before. Since I have limited experience with data analytics, I found watching tutorials on Youtube helped me get a better sense of how best to utilize these tools. It seems you also found Orange simple and robust.  Could you see any of these tools you explored being able to add value to your current job? Thank you,",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "12:40 PM",
                    "replies": {}
                },
                "reply-Robert-Kemp": {
                    "content": "George - Great summary and good insights! I have absolutely no experience using these analytical tools and your summary provided good information to understand these products - certainly as a starting point for me.   I had taken the Data Mining course and it was referenced by others but I have not used it but looks like a very good tool - for data mining at least Have you considered the Google Analytics certificate?  I was thinking this was a way to secure some 'hands on' experience to at least the Google tools... would this include the Google Data Studio?  I will spend some time looking at this. Thanks for this weeks posting",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "1:03 PM",
                    "replies": {}
                }
            }
        },
        "post-aidanpo1": {
            "content": "After working with the tools presented this week, I was very interested in Gapminder [2].  In terms of presentation and power, Gapminder had almost everything I would look for in a tool.  The presentation power alone is very impressive.  The multitude of visualisation formats and techniques that were present covered a lot of bases and were very easy to understand.  I think that Gapminder would only be powerful in an enterprise environment, only because it is built to analyze vast amounts of data from different areas and categories and compare them at a large scale.  The presented video on the site showed so many dimensions of data from decades of information [2] that I think your average person wouldn't be able to make much use of the software for smaller personal projects.  In comparison, other programs like Orange [1] seem perfect for personal and group level use if you have a good amount data that you would like to analyze in different visualization formats.  There are lots of formats from graphs, charts and different plots [1] that can really quickly display data to pick apart.  Although, I was really unimpressed with Gephi [3].  I do understand that it is a much older program, but it is very clunky and really only provides good support for node graphs [3].  I don't see myself or other people choosing this software over newer alternatives like Orange [2] due to ease of use and flexibility. I am personally a fan of more nitty gritty tools like TensorFlow [4] used in conjunction with Matplotlib [5].  TensorFlow is a machine learning and data mining library for Python that allows for all types of models to be created and used programmatically [4].  Matplotlib is also a Python library that allows for the visualization of data and outputs of libraries like TensorFlow with many incredibly customizable options [5] that I believe make it one of the best visualization and analysis tools out there.  I do recognize that there is a pretty big barrier to entry in the fact that using these two libraries requires both programming experience and a fairly thorough understanding of the mechanics behind machine learning algorithms, so this combination is not for everyone.  It is also not needed for all projects, so it really is a give and take for the output that one needs.",
            "month-posted": "May",
            "date-posted": "22",
            "day-posted": "Sunday",
            "time-posted": "5:09 PM",
            "replies": {}
        },
        "post-suzanneva5": {
            "content": "In a previous course, I worked with a group to harvest tweets to study word/term frequency analysis in relation to a political figure. Because we were in a python class, we had to use python as a the tool. If you're aware of python, but don't know the various python projects and packages, you should get to know them. After the harvesting and cleaning, we used a wordcloud package to generate a visualization of the most used terms. In addition to the wordcloud, we created a couple visualizations related to tweet activity based on a specific date. To do that, we used the Pandas Dataframe and Matplotlib. We also created a simple word frequency list.  It was a lot of work learning how to scrape data through an endpoint and process it in the various python tools. ",
            "month-posted": "May",
            "date-posted": "17",
            "day-posted": "Tuesday",
            "time-posted": "10:05 PM",
            "replies": {
                "reply-Sheryl-Griffith": {
                    "content": "Thanks Suzanne for sharing. I will take you up on your advice and do some research and some training on Python as I have zero experience with the application. Your project sounds very interesting. Scrapping is a new term for me and do you think this technique can be used for the Participation Portfolio and process through Python?",
                    "month-posted": "May",
                    "date-posted": "22",
                    "day-posted": "Sunday",
                    "time-posted": "3:15 PM",
                    "replies": {}
                }
            }
        },
        "post-Adaobi-Emoka": {
            "content": "Which tools/systems did you like? Asides from the list of tools/links provided, I've worked with two major Analytics tools or as I like to call them BI Tools.  I currently work with IBM Cognos Analytics and Microsoft's Power BI (recently introduced at my workplace in just under 16 months).  They each have their pros and cons. With IBM cognos, I find creating data sources (packages) isn't something most people in my organization are tasked with. We have two dedicated cognos Admins, whose job is to create packages, cubes etc which can be used for reporting, dashboards, exploration, and storytelling. Power BI , on the other hand allows a Data analyst or BIA more flexibility in terms of how they would like to transform and model the data.  You can merge data from multiple sources (flat files & DB tables) to create a Data source / Data source that can be used for reporting. I took a course in Data Mining where I worked with WEKA for the first time. I found the interface a bit archaic but eventually got the hang of it when I really got into using the machine learning algorithms and analyzing different datasets. What type of functionality would you like to see in analytics tools/systems? There is a deprecated tool in IBM Cognos called Metric Studio. I really like this tool as it allows users to track and analyze Metrices on customizable scorecards. In the recent version of IBM V11 a.k.a Cognos Analytics, this feature is no longer available. Microsoft introduced a similar tool in Power BI called Power BI Goals. It' very similar in concept to IBM's cognos Metric studio but lacks some of the great features Metric studio has. Like the number of metrices (sub-goals) one can create in Power BI Goals, the capability to rollup sub-goals into goals in a scorecard isn't yet available in power BI goals but exists in Cognos Metric Studio. I really do wish IBM would include/upgrade the Metric Studio in future releases.",
            "month-posted": "May",
            "date-posted": "22",
            "day-posted": "Sunday",
            "time-posted": "9:48 PM",
            "replies": {}
        }
    },
    "week-5": {
        "post-Moustafa-Mahmoud": {
            "content": "Linked Data (LD) interlinking data from different sources which providing data in a structured way. It provides a Web of data. It uses RDF: resource description framework in order to query information about URLs using Simple Protocol and RDF Query Language: SPARQL (O. Lassila and Ralph R. Swick, 1999). Open Data or Linked Open Data (LOD): interlink a globe repository from many datasets in the Linked Open Data (LOD) in a structured format using a semantic representation. (E. Rajabi,  2015). Semantic web: semantically enrich an educational resource, systems often extract structured information from unstructured data and link to external knowledge bases in the Linked Open Data (V. Uren,2006). Semantic web, Linked Data, open data are being helped the Learning analytics in the following three areas: 1-Data integrating of educational data and resources with other linked datasets: It can be an alternative for sharing and reusing educational data and resources, providing interoperability among repositories, enriching content, exploring large datasets relevant to education performance analysis (Silva,2017). They allow to share data such as courses offered by universities, statistical data, organizational data, educational resources such as videos, presentations, lectures, books and games(Silva,2017). Without these technologies LA is facing a challenge. Which is due to the variety of systems being used around the world, and the need for reuse of educational resources, and data. One of the examples of provided solutions: Penteado (2016) and Maturana et al. (2013) created applications that allow users to perform data analysis and to explore data about researchers, conferences, and publications available in a Learning Analytics and Knowledge (LAK) dataset. As explained by Zouaq et al., Linked Data has the potential to be a possible solution by allowing the mapping between various data models. Thus, semantically similar models that are represented differently can still be aligned using links to establish meaningful connections between concepts from different models. Open data enhances information retrieval and improve interoperability. Information retrieval is improved by the ability to perform searches which exploit the ontology to make inferences about data from heterogeneous resources; furthermore, annotations based on a common ontology can provide a common framework for the integration of information from heterogeneous sources (V. Uren,2006). 2-Enriching educational content: data reusing for enrichment, recommendation (and customization) of educational content available in the Web of Data, and expansion of search terms . To semantically enrich an educational resource, systems often extract structured information from unstructured data and link to external knowledge bases in the Linked Open Data (LOD), such as DBpedia. (V. Uren,2006). This application adds markups with semantic information surrounding atomic elements (entities). These entities are the ones found in the application dataset, and each one contains structured information extracted from Wikipedia. The DBpedia Lookup Service can be used to look up DBpedia URIs by related keywords. Semantic Web provides 2 types of browsers: First, Text-based browsers display the data using text structures such as tables and lists and may have more advanced features, allowing navigation through the data. For example, semantic web browsers of this type include Marbles (R. Navarrete,2015) and URIburner(G. Vega-Gorgojo,2015). Second, Browsers offering data visualization options use structures such as images, charts, graphs and timelines to present the data. Examples include DBpedia Mobile(B. Kitchenham,2007), IsaViz(J. Biolchini,2005), OpenLink Data Explorer (ODE)( H. Qing,2012) and Tabulator(D. Mouromtsev,2-16). 3-personalizing and recommending educational content and practices: individualizing and personalizing learning. As an example: they are using semantic context modelling  and creation of Linked Data from LMS application logs (R. Meymandpour,2007) which allows to provide customizations depend in the user historical choices.",
            "month-posted": "June",
            "date-posted": "3",
            "day-posted": "Friday",
            "time-posted": "10:13 AM",
            "replies": {}
        },
        "post-atulch": {
            "content": "The Semantic Web is a global network of information connected together in such a way that machines can readily process it. It's a globally linked database or an efficient technique of representing data on the World Wide Web. Tim Berners-Lee, the creator of the WWW, URIs, HTTP, and HTML, devised the Semantic Web. The World Wide Web Consortium (W3C) has a committed team of people trying to enhance, extend, and standardize the system, and numerous languages, publications, tools, and other things have already been established. However, Semantic Web technologies are still in their infancy, and while the project's future is bright in general, there appears to be no consensus on the early Semantic Web's anticipated trajectory and characteristics. What is the logic behind this system? In some cases, data that is stored in HTML files is valuable, but not in others. The majority of data on the Web in this form is difficult to use on a broad scale since there is no global method for posting data in such a way that it can be easily processed by anybody. Consider local sports event information, weather forecasts, flight schedules, MLB statistics, and TV listings. HTML is used to convey all of this information. The issue is that in some situations, using this data in the ways it was intended is challenging. When individuals are initially introduced to XML RDF, they frequently have two questions: why use RDF instead of XML? and should we utilize XML Schema with RDF? The answer to the question why use RDF instead of XML? is two-fold. To begin, drafting a language in RDF has the advantage of mapping information directly and explicitly to a model, a decentralized model for which numerous generic parsers are already available. This implies you can tell which bits of data are the application's semantics and which are merely syntactic fluff in an RDF application. Because RDF is so well known, not only do you know that, but everyone does. The second part of the two-part response is that we believe that RDF data will become a component of the Semantic Web, so the advantages of writing your data in RDF today are similar to the advantages of writing your information in HTML in the early days of the Web. [1] Linked data is a method of organizing and sharing information via the use of links. These links enhance the meaning and utility of data. Linked data's benefits: Because you can follow links, they can lead to additional information about a subject. Machines can traverse these graphs if you link to other linked data resources. Links eliminate ambiguity, making what is being said very plain. Decentralized architecture is possible with linked data. URLs can link datasets together since they point directly to the data source, even if the data is on a separate domain and server. [2] Open data is described as machine-readable structured data that may be freely shared, used, and built upon. The data must be available in its whole and at a fair replication cost, preferably by internet download. The information must also be in a usable and editable format. The data must be made available under terms that allow for re-use and redistribution, as well as mixing with other datasets. Everyone should be able to use, re-use, and disseminate information. There should be no distinction made between fields of endeavor or between individuals or groups. For example, 'non-commercial' restrictions that would ban 'commercial' use, as well as restrictions on use for certain purposes (for example, just in education), are not permitted. [3]",
            "month-posted": "May",
            "date-posted": "27",
            "day-posted": "Friday",
            "time-posted": "3:51 PM",
            "replies": {}
        },
        "post-leminhducng": {
            "content": "Learning Analytics is about the processing of data about learners and their environments for the purpose of understanding and optimising learning. The introduction of Learning Analytics gives rise to some significant challenges with respect to the management of the data sources [1]: - LA processes the data from a wide range of different contexts and domain which have their own approaches to data management and to the modelling of information - It requires the combination of many data sources in order to gain insights on the connnections between different aspects of the learning experience and the learner's context - Information sources and background knowledge at different stages is required by LA processs, and normally they are not always anticipated in advance. The specific issues with data management for LA requires the solution from state-of-the-art data management approaches that can flexibly interconnect all sorts of hetegeneos data sources into the analytical process. That can be said that Linked Data and Semantic Web technologies are the potential candidates for data management layer for LA, as the concept of Linked data is to use the architecture of the Web to share, distribute and interconnect data from various origins. In addition, Semantic Web, related to Ontology representation, provides web-enabled, reusable and connected common vocubularies that make distributed, linked data exploitable accross origins and domains.[2]",
            "month-posted": "May",
            "date-posted": "30",
            "day-posted": "Monday",
            "time-posted": "8:11 AM",
            "replies": {}
        },
        "post-Robert-Kemp": {
            "content": "Learning and knowledge analytics is built on the foundational concepts of Sematic Web, Open Data and Linked Data and without these domains being studied and investigated Learning and knowledge analytics would not be as evolved as it is today and in the same thought neither would data visualization [1] and dashboarding [2]. Artificial Intelligence, which has been evolving and essentially incubating for many years - where for example neural networks were discussed and paper's published 30 years ago by Geoff Hinton, the Godfather of AI [3] [4].  Neural Networks evolved into the concept of deep learning with practical and far reaching capabilities. Without the foundational research from 30+ years ago neural networks would not be as advanced today. Similarly, Semantic web was discussed in 1994 by Tim Berners-Lee and then in 2006 outlined a set of rules for publishing data on the way in a way that all published data becomes part of a single global data space.  How we looked at data too has changed with firms such as GapMinder and the learning from data that can be achieved as communicated by Hans Rosling, a GapMinder founder, in a TED talk from 2006 timeframe[5].  This data learning movement evolved into a 2018 best selling book - Factfullness [6].  The last decade has seen an explosion in the number and capability of the analytical tools. There are easily 40 different tools available based upon a quick search of the internet. One of which is Gephi which helps “data analysts and scientists explore and understand graphs and help make hypothesis, intuitively discover patterns and isolate faults” [7]. This is the foundation of Learning and knowledge Analytics as defined by the SoLar site - LEARNING ANALYTICS is the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs [8]. Hitzler (2021) in his review of the sematic web indicated that interest in the concept of 'Linked Data'  was declining [9] but its not difficult to appreciate that the combination of Sematic web concepts, Open Data and Linked Data laid the foundation to newer fields of study such as Learning and knowledge analytics, data visualization and dashboarding. We are just started to understand and realize the benefits today. Btw - the TED talk is by Hans Rosling [4] is recommended to watch",
            "month-posted": "May",
            "date-posted": "25",
            "day-posted": "Wednesday",
            "time-posted": "8:23 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "To some degree, the web will always be linked.  Interest may be declining in a formal linked data structure, but the very nature of the web means that data will always lead to other data. Do you think that this decline in formally linked data will lead to the potential for greater misunderstanding or misinformation? Does formally linking data have any effect on the correctness of it? Thanks!",
                    "month-posted": "May",
                    "date-posted": "26",
                    "day-posted": "Thursday",
                    "time-posted": "7:46 PM",
                    "replies": {
                        "reply-Robert-Kemp": {
                            "content": "Ken, thank you for your comments. I agree, the formal, linked data structure approach maybe declining or lower priority, but I believe, some very very smart people were listening very closely to Hans Rosling and others were espousing about the power of the data and took advantage of the situation, The likes of Cambridge Anaytica, Facebook and Google types - they knew exactly how to leverage or maximize this data...the created their own data linked data structures to be mined ...The correctness depends upon who is the judge ...just like truth ... ... More data will beget more data and I think it won't be used correctly or with best intentions whether its formally linked or not ... these are deep questions",
                            "month-posted": "May",
                            "date-posted": "26",
                            "day-posted": "Thursday",
                            "time-posted": "9:20 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-George-Adyrhaiev": {
                    "content": "I don't know if I fully agree with you on this. Semantic Web, Linked Data and Open data serve a certain purpose that learning analytics can benefit from, but it doesn't have to. And I would argue that SoLAR's definition of LA that you refer to LEARNING ANALYTICS is the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs can be efficiently achieved without Semantic Web (SW) or Linked Data (LD) principles, and especially Open data (as defined by what data can not be open as per GoC[1]). The fact that Semantic Web and Linked Data strive for having a certain type of data to be more connected through specific standardization of its elements doesn't mean that data visualizations and dashboards originated from SW and LD or any of their principles. In my mind, they are way more linked to specific business issues they were trying to solve as well as the development of the RDBMS and relational data in general and evolved from there.",
                    "month-posted": "May",
                    "date-posted": "27",
                    "day-posted": "Friday",
                    "time-posted": "5:35 PM",
                    "replies": {}
                }
            }
        },
        "post-kennethmc39": {
            "content": "As soon as I saw the line all of this information is presented by numerous sites, but all in HTML (Palmer, 2001), I knew the material about the Semantic Web was probably quite old. And indeed, at the bottom of the page is a copyright date of 2001 (and in the URL as well).  Ancient, in terms of computing.  HTML lives on today, but data is captured in many dozens of different formats on today's internet In many ways, it feels as though the idea of the Semantic Web runs counter to the spirit of the internet. That is, this attempt to enforce a rigid data structure feels authoritarian, and many of the early heroes of the internet were decidedly antiauthoritarian.   Yet, the idea and the rationale behind the Semantic Web is still applicable.  Information on the internet is not stored according to some standard or style. Instead, it's all over the place.   Wouldn't it be nice if there were some standard way of formatting pieces of data, to ensure that the relationship between that data could be understood? This is the way in which RDF is applicable today. I've been playing with or making a living from the internet for most of my life, and I'd never heard of RDF or the Semantic Web. The latest version of the project came out over eight years ago, leading me to wonder how widespread the use of this technology is. My understanding is that contemporary RDF acts as an interchange standard, which is useful in systems that need to be able to parse the relationship between data.  AI is one such example. (Onto Text, 2022) I am interested in the overlap between RDF and graph databases.  Graph databases are excellent at representing the relationship between entities, and it turns out that RDF Graph Databases exist for cataloguing and representing the relationship between metadata nodes and vertices. The idea that data may be linked, and that conclusions may be drawn from those links, is foundational to data mining and especially to analytical methods such as Social Network Analysis.  Without concepts such as the Semantic Web, data mining would be a far less effective analytical discipline.   The ideas behind the Semantic Web live on.",
            "month-posted": "May",
            "date-posted": "23",
            "day-posted": "Monday",
            "time-posted": "6:29 PM",
            "replies": {}
        },
        "post-janineis1": {
            "content": "Hi everyone, During my research on Linked Data, I stumbled upon this video that sums up the concept in a really digestible way. Hope it is useful!",
            "month-posted": "May",
            "date-posted": "31",
            "day-posted": "Tuesday",
            "time-posted": "5:01 PM",
            "replies": {}
        },
        "post-janineis1-2": {
            "content": "Put simply, the semantic web or Web 3.0 is the 3rd generation of the Internet in which data is distributed to make machines understand linkages and relationships between multiple topics. In order to understand the Semantic Web, looking into how we got to Web 3.0 was a must: 1) Web 1.0 was the primitive version of the Internet in which information lived in a repository of hyperlinks that related concepts in a manual approach. 2) Web 2.0 transformed this repository into applications such as LinkedIn or Facebook that are essentially data silos of information. 3) Web 3.0 is the idea of connecting this web of data together and removing silos in order to efficiently relate information without having to update in separate places. From my personal research, Web 3.0 seems to be a way to democratise data in which data is open and freely used, modified, or shared. Moreover, one of the key pillars of Web 3.0 is that data is linked. Linked data is such that information can traverse through different websites via hyperlinks, making the Internet a global repository of information that anyone can access and digest. Zouaq et al. (2017) suggest that with the emergence of open data, other learning environments such as MOOCs (massive online open courses) have provided insights on how users can interact with learning subjects but also created a link to social media in which students can further connect and exchanges ideas. Although this provides a enriched database of information, there are challenges in terms of collecting too much data and privacy concerns when linking learning data with social media platforms. However, as linked open data provides an excellent foundation for learning and knowledge analytics, this relationship is fundamental to improving student performance and overall interaction with learning platforms. Without these concepts, the learning analytics field would be rudimentary to where it is presently at. Learning platforms would also face challenges in terms of transferring data from other websites into its own database and thus, take a substantial amount of time creating curriculums.",
            "month-posted": "May",
            "date-posted": "31",
            "day-posted": "Tuesday",
            "time-posted": "1:43 PM",
            "replies": {}
        },
        "post-Che-Little-Leaf-Matusiak": {
            "content": "After doing the readings and watching videos, I found the concept of the Semantic web by way of establishing of an RDF structure to extend the web to be readable to human made electronic devices an intriguing concept. (Palmer, 2001-2009)  I spoke with a web developer with more than 25 years experience that I know personally.  Surprisingly this individual was not well versed in the subject as most of the time this individual uses the conventional web development process of using technologies such as HTML, CSS, Javascript libraries, and sources to create web pages for human audiences. To discuss the semantic web relation question, according to Halimi and Seridi-Bouchelaghem (2021), to improve the appraisal methodology of student/learner proficiencies, the use of inference and analysis instruments used in the semantic web is taken advantage of to construct models in combination with diverse analytics methods such as sentiment analysis.  There is strength in collaboration, and to not have these additional tools be a part of learning analytics research would styme the processes in this field.",
            "month-posted": "May",
            "date-posted": "30",
            "day-posted": "Monday",
            "time-posted": "11:59 PM",
            "replies": {}
        },
        "post-christianmu5": {
            "content": "I would like to start by defining the 3 terms: Semantic web, open data, and link data. Thereafter I will explain how they contribute to learning analytics. Semantic web: In short, the Semantic Web is a model that allows data to be shared and reused between multiple applications. All this is in order to allow all users to find, exchange, and associate, more simply and without an intermediary, data[1] Thanks to the Semantic web, learning materials are constantly improving and this provides learners access to knowledge in a variety of forms. Another advantage Is that it can help teachers track students' progress and be in a better position to guide them. Linked data is a protocol that helps interconnect data published on the web, making data a more valuable asset. When information is interconnected, it simplifies research and facilitates critical thinking, instead of spending too much time on a large volume of resources [2]. Open Data basically is an approach to democratize data, which allows individuals and communities to access and re-use the available information as long as it's not something sensitive. The main potential of open data in learning analytics is that it opens up new possibilities for participation and discussion. In theory, open data allows all interested parties to have their own opinion about what they are learning and use it to achieve their goals. Open data enables new creative approaches to problem-solving and also offers new insights to those responsible for public education[2] For me without these three concepts, learning analytics would be laid back and we would not have the many innovations humans have come up with in the last few decades.",
            "month-posted": "May",
            "date-posted": "31",
            "day-posted": "Tuesday",
            "time-posted": "12:56 AM",
            "replies": {}
        },
        "post-Matthew-Basaraba": {
            "content": "Discuss in the Week 5 discussion forum how the Semantic Web, open data and linked data are related to learning and knowledge analytics. Where would learning and knowledge analytics be without those concepts? Advances in knowledge modeling and representation, the semantic web, data mining, analytics, and open data form a foundation for new models of knowledgedevelopment and analysis. (Siemens, 2012). Learning and knowledge analytics is a field with many stakeholders and basing the roots of this field in available and transparent data is a necessity for creating informed discussion so that all parties can continue to be part of the dialogue in this emerging field. Linked data on the other hand appears to be an area where there are many opportunities for adoption in learning analytics fields by making data relevant to learning analytics more readily available by analytics practioners in the filed gaining more proficiency with linked data techonologies (d'Aquin, et. al, 2014). I highly recommend exploring the paper Using Linked Data in Learning Analytics as it provides some concrete examples of data visualizations with tools we've seen in Unit 3 such as Gephi.",
            "month-posted": "May",
            "date-posted": "29",
            "day-posted": "Sunday",
            "time-posted": "11:13 PM",
            "replies": {}
        },
        "post-abebayi": {
            "content": "No submission",
            "month-posted": "No submission",
            "date-posted": "No submission",
            "day-posted": "No submission",
            "time-posted": "No submission",
            "replies": {}
        },
        "post-eugenegr": {
            "content": "Williamson (2019) examines the Datafication of education and provides an example in the UK, where data is linked from LMS, reading lists, and LA platforms to government datasets. These connections offer analytic possibilities to optimize learning and the environments in which it occurs (Siemens, 2013). Datafication and the ability to collect and share data across multiple institutions provide opportunities to access course material and learner strengths at an institutional level allowing for the required feedback for learner and institutional-based improvement. The use of datafication is seen as a requirement to facilitate the shift to real-time adaptive assessments, tracking individual learners' performance, and optimizing the education system to ensure the global competitiveness of the learners.  Arafat, et al. (2019) discuss unintentional learning and the requirement for links to multiple sources of materials, technologies and individuals. The unintentional learning effectively creates a social machine that uses semantic web principles to facilitate e-learning. The intent is to explain the human learning experience and derive strategies to improve the experience. E-learning's information requirement demands the integration of heterogeneous data sources using technology described by the semantic web concepts. Without the expansion into external data sources and a knowledge-sharing concept, Learning and Knowledge Analytics would be restricted to localized information. This restriction may inhibit required advances in the improvement of learner experience.",
            "month-posted": "May",
            "date-posted": "29",
            "day-posted": "Sunday",
            "time-posted": "2:34 PM",
            "replies": {}
        },
        "post-Sheryl-Griffith": {
            "content": "This weeks' module took me out of my comfort zone, as an analyst, I am usually on the other side manipulating and analysing the data, so I enjoy these learnings. Honestly had to read the content a couple of times to wrap my head around the concept of the Semantic Web. My understanding is that the Semantic Web is Web 3.0 but had been seen by some as being indescribable or elusive when first introduced by Tim Berners-Lee around 1994. [1] The concept is a decentralized system, where a web of data is created and linked by relationships to be fully influential. [2] Consequently, the concept of linked data is realised. I also learnt here that with the Semantic web, when new data is generated, correlations and illations are formed automatically through software and no human intervention is required. Another cool feature of the Semantic web is this same software through analysis, makes the connection if there are variations in the linked data. This is powerful! With Open data, after researching, my concerns were privacy and the veracity of this shared data. My thoughts and findings are that the responsibility is on the one sharing to put steps in place to secure and vet the data.[3] If we associate these concepts to learning and knowledge analytics, we as students are benefitting in terms of our online studies. Using Google scholar, YouTube and any other platform on the web, where data is linked and is open.",
            "month-posted": "May",
            "date-posted": "27",
            "day-posted": "Friday",
            "time-posted": "3:18 PM",
            "replies": {
                "reply-George-Adyrhaiev": {
                    "content": "Thank you for sharing your thoughts. I feel like you and I are on the same page about the role of the Semantic Web, Linked and Open data and learning analytics. LA can benefit from these ideas and technologies but is not defined nor requires them to be successful in meeting its objectives.",
                    "month-posted": "May",
                    "date-posted": "27",
                    "day-posted": "Friday",
                    "time-posted": "5:21 PM",
                    "replies": {}
                },
                "reply-abebayi": {
                    "content": "Thank you, Sheryl for sharing your thoughts on this topic and moreover the great video. It explained it so well that I no longer feel lost. I am conflicted with what Web 3.0 offers - on the one hand that means we are able to access real-time data about any given topic which allows us to dive deeper in our studies and it encourages further advancements. However, as you mentioned the concern I have is loss of privacy that is ingrained with this transition. I feel like already with Web 2.0 we are losing our right to anonymity so is something we need to be really concerned with despite the many benefits that can come from it. Do you think there needs to be intervention to prevent this to maintain a sliver of our privacy?",
                    "month-posted": "May",
                    "date-posted": "29",
                    "day-posted": "Sunday",
                    "time-posted": "6:18 PM",
                    "replies": {
                        "reply-Sheryl-Griffith": {
                            "content": "Thank you for your feedback and thoughtful comments. In regard to privacy, in the last decade, we have become more aware of how our activities on the internet affect our privacy and how this can become an issue. As users, we have to educate ourselves and take the necessary precautions to protect our identity and data. It seems as if there is a cyber security breach daily, technology and analytics are evolving to predict threats. Cybersecurity jobs are also in demand.",
                            "month-posted": "May",
                            "date-posted": "29",
                            "day-posted": "Sunday",
                            "time-posted": "8:25 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-George-Adyrhaiev": {
            "content": "After having gone through this week's material I can't help myself but feel that the Semantic Web is more of a vision of connected data, rather than a practical and massively adopted implementation of it. W3C looks at it as a web of data [1], which makes perfect sense for those use-cases that rely (or can benefit greatly) from established connections between various data points. However, that is only one of the ways the web is being used today, and concepts of Semantic Web and Linked Data won't be applicable to a lot of privacy-focused applications of the web.  The concept of the Semantic Web relies on Linked Data - the standardized way of implementing certain types of connections (through interfaces, vocabularies, etc) that form relationships between various data points [2]. Linked data enables more accurate and faster querying that in turn results in the Semantic Web's vision of the web of data.  On the other hand, open data is one of the key concepts in formal information-sharing conducted by various levels of the government. Since the government's role is to serve the people through elected officials and staff needed to support various activities, the data that is being produced by its operation, as well as the data that is being used for making various forms of decisions should be shared with those who hire the government.  Without the concepts of Semantic Web, Linked Data, and Open Data, learning analytics would have developed in one shape or another. I am not convinced at all that any of these concepts have revolutionized or even enabled Learning Analytics, however, they have definitely helped it grow, given the principles of Linked Data that tie different data points together and Open Data - transparent sharing of public data both provide access to significant datasets that can be used to achieve some of Learning Analytics' goals.",
            "month-posted": "May",
            "date-posted": "27",
            "day-posted": "Friday",
            "time-posted": "5:15 PM",
            "replies": {}
        },
        "post-aidanpo1": {
            "content": "Semantic Web, Open Data and Linked Data are intrinsically linked to learning and knowledge analytics.  Teaching methods and learning styles are not unique among individuals or organizations, there are lots of similarities in how information is conveyed, this is why there are universal categories of learning [1].  Those being, visual, auditory, kinesthetic, and reading/writing learning styles [1].  Learning and knowledge analytics are quickly growing largely due to open data.  Having a multitude of research papers being published in the ever-growing discipline, many of which providing their own data sets upon which their study is based upon really helps the area as a whole.  Also, in terms of linked data, when it comes to examining internal data regarding students and learners in academic environments, linked data is helping advance the field as well.  d'Aquin et al. studied the power of using linked data within learning environments, specifically the causes of student enrolment [2].  It was concluded that the main advantage of this approach is that they were able to provide an analyst with the data, customizable views and dashboards to identify patterns in enrollment as well as course structures [2]. I personally believe that there is likely a lot of ground to be covered in terms of analyzing singular students that might be having trouble in courses and what patterns of assignments and work they have problems with to allow for quicker intervention to course correct a student.",
            "month-posted": "May",
            "date-posted": "23",
            "day-posted": "Monday",
            "time-posted": "1:04 AM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "Were you able to find any examples of the Semantic Web being a current technology? I know RDF is still in use, but the Semantic Web itself seems to be an obsolete concept.",
                    "month-posted": "May",
                    "date-posted": "23",
                    "day-posted": "Monday",
                    "time-posted": "6:38 PM",
                    "replies": {}
                }
            }
        },
        "post-suzanneva5": {
            "content": "With this week's readings about the Semantic Web, I had flashbacks to a core library science course. We discussed not only XML for markup but also MARC 21 for the Library of Congress, among others. In addition, after reading Berners-Lee's article about linked data, I can tell that social media has embraced the concept of RDF. For Facebook sharing, your content has to have certain metadata following RDFs. For Twitter, they reference Twitter cards. These are essential to creating a rich social experience.  At the heart of the Semantic Web, open data, and linked data is the concept of shareable and reusable data, using a common language or structure. By data, I don't just mean datasets. We already see this in action in online libraries at all levels where their systems connect and allow search.  Open data in education also empowers families to learn more about schools in their area. In British Columbia, the initiative is Discover Your School under the Canada Open Data Initiative. There are similar initiatives elsewhere in the world. As for where learning and knowledge analytics would be without these concepts and tools, they would likely be non-existent. ",
            "month-posted": "May",
            "date-posted": "23",
            "day-posted": "Monday",
            "time-posted": "9:50 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "I can well imagine that library science would include a lot of study of metadata and markup languages.  Do you think that the open nature of this data results in a greater collective understanding?  Or has free and open access to structured data provided some bad actors with a vector for manipulating the conversation? Thanks!",
                    "month-posted": "May",
                    "date-posted": "26",
                    "day-posted": "Thursday",
                    "time-posted": "7:43 PM",
                    "replies": {
                        "reply-suzanneva5": {
                            "content": "I'm not really sure I understand your questions. If you're referring to data within a library science environment as being open with regard to access, that's true. If you're applying 'open nature' to formatting of information/data in that context, you'd be mistaken. It's quite strict. And I haven't even brought up the archival aspect of information/data. That's a whole other beast. Who are these bad actors?",
                            "month-posted": "May",
                            "date-posted": "26",
                            "day-posted": "Thursday",
                            "time-posted": "9:13 PM",
                            "replies": {}
                        }
                    }
                }
            }
        }
    },
    "week-6": {
        "post-Moustafa-Mahmoud": {
            "content": "IoT connects devices other than PCs and smart phones to the internet such as the refrigerator and washing machine! (Insider Intelligence ,2022), The aim is to obtain a smart control over the connected devices. This transforming allows us to obtain numerous data about the users' activities using the devices and opened a door for analytics and machine learning. IoT helps us in many areas, the following is a brief of its usage in teaching:  IoT in classroom: According to Sciforce (2019), many devices in the classroom can transform to be smart. For example: Smart board, environmental sensors, Security cameras, Wireless door locks and others. How IoT can help in teaching? 1-IoT enhances the lesson plans Enhance access to information in the learning environment leads to smarter lesson plans. Connecting devices to internet and using cloud storage allows teachers and education professionals to collect information about the students (Learning analytics). These statistics help teachers improve student engagement, as well as adjust their lesson plans for future classes (Insider Intelligence ,2022). 2-IoT increases school safety: IoT allows schools to improve the safety of their campuses. For example, students would be able to keep track of connected buses and adjust their schedules accordingly, which would prevent them from spending unnecessary time in potentially dangerous areas. Another example is Automated attendance tracking systems (Sciforce ,2019). 3-IoT reduces operating costs: Keep track of key resources, increased energy efficiency and reduced operating costs: using a web-based system that controls all mechanical equipment inside the buildings (Insider Intelligence ,2022).. Also, as mentioned above, student attendance can be tracked using smart fingerprint devices or digital token devices connected to the cloud. Similar, Digital signature devices instead of paper based may use in the admission process(Sciforce ,2019)..",
            "month-posted": "June",
            "date-posted": "3",
            "day-posted": "Friday",
            "time-posted": "12:41 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "Regarding the IoT's ability to decrease operating costs, do you think that it also provides an opportunity for organizations to be dishonest?  For example, Volkswagen got caught using software to lie about their diesel emissions.  Will the Internet of Things provide more opportunities for companies to misreport on things like this? Thanks",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Saturday",
                    "time-posted": "8:40 AM",
                    "replies": {
                        "reply-Moustafa-Mahmoud": {
                            "content": "Thanks for the great example. Frankly using IoT to report dishonest  reports to the customers is a new topic for me. II think that the company's decision to issue inaccurate reports to customers would have been made either if IoT or any other technology was used, and that this depends only on the company's honesty, credibility and application of honesty and honor policies. ",
                            "month-posted": "June",
                            "date-posted": "6",
                            "day-posted": "Monday",
                            "time-posted": "10:13 AM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-atulch": {
            "content": "Internet of Things The Internet of Things (IoT) is a new paradigm that has shifted people's lifestyles from traditional to high-tech. IoT has brought about changes such as smart cities, smart homes, pollution management, energy conservation, smart transportation, and smart industries. In order to improve technology through IoT, numerous important research studies and investigations have been conducted. To realise the full potential of IoT, however, a number of obstacles and issues must be addressed. These difficulties and challenges must be evaluated from a variety of perspectives, including applications, challenges, enabling technologies, social and environmental implications etc. Effect of IoT in healthcare Using the Internet of Things in medical allows for better, healthier, and more convenient patient care. The Internet of Things will speed up healthcare delivery by allowing doctors to spend less time on transportation, identify ailments, and connect with patients, among other things. The healthcare business benefits from IoT asset monitoring in a variety of ways. Doctors, nurses, and orderlies frequently need to know the exact location of patient-assistance equipment such as wheelchairs. When a hospital's wheelchairs are equipped with IoT sensors, they can be tracked using an IoT asset monitoring tool, allowing anyone looking for a wheelchair to find one fast. Benefits of IoT in healthcare sector 1. Real time monitoring and reporting Real-time tracking provided by linked devices can save lives in the event of a medical emergency such as heart failure, diabetes, or asthma attacks. With real-time condition monitoring in place utilizing a smart medical device connected to a smartphone app, connected devices can collect medical and other health data as needed and send it to a physician via the smartphone's data connection. 2. Remote medical help Patients can use a smart smartphone app to contact a doctor who is several kilometers distant in the event of an emergency. Physicians may immediately check patients and diagnose on-the-go problems with mobile technologies in healthcare. 3. Conduct a study In the health-care industry, IoT can also be used for research. It's because the Internet of Things allows us to collect a vast quantity of data about a patient's disease in a fraction of the time it would take if we did it manually. As a result, the information gathered can be used in a statistical analysis to aid medical research. INTERNET AND EDUCATION The Internet of Things can start disrupting education as early as kindergarten and continue through 12th grade, but higher education has the most dramatic effects. Students, particularly those in college, are increasingly turning to tablets and laptops instead of printed books. Students can now learn at their own pace and have a nearly comparable educational experience at home and in the classroom since they have all of the required knowledge at their fingertips. And, while this tendency benefits students, it also benefits teachers by making the teaching process more efficient. Because of the rise of connected technology, instructors no longer need to grade examinations on paper or complete other common duties. Instead, professors can concentrate on what matters most to their students: personalized learning. Professors can collect data on their students using cloud-connected devices and then determine which ones require the most individualized attention. Teachers can use these statistics to improve student engagement and adjust lesson plans for future classes. Universities can use connected devices outside of the classroom to monitor their students, staff, resources, and equipment at a lower cost of operation, saving everyone money. In addition, these tracking capabilities should make campuses safer. For example, students would be able to keep track of connected buses and adjust their schedules accordingly, which would prevent them from spending unnecessary time in potentially dangerous areas.",
            "month-posted": "June",
            "date-posted": "5",
            "day-posted": "Sunday",
            "time-posted": "12:30 PM",
            "replies": {
                "reply-aidanpo1": {
                    "content": "Sunday",
                    "month-posted": "June",
                    "date-posted": "5",
                    "day-posted": "Sunday",
                    "time-posted": "2:44 PM",
                    "replies": {}
                }
            }
        },
        "post-leminhducng": {
            "content": "IoT is developing quickly and changing many sectors, including higher education institutions. The future of education is about how schools and universities will adapt to the changing needs of the future knowledge worker, the future of work, and the economy. One of the examples is the concept of digital campus. It is a platform for students to get all kinds of information. A digital campus has the technology that enabled teaching and learning, and empower collaborative research.  IoT applications play significant role in a digital campus. [1] According to Cisco -Digitizing Higher Education To enhance experiences and improve outcomes- IoT applications differ from conventional network applications as they support sensors and sensor data, rather than users and user data. IoT applications for the digital campus include five main categories: Building Control and Management; Security and Access Control; Video and Information Systems; Location and Attendance Systems; Energy Monitoring and Control How does learning analytics benefit from IoT? The efficient and effective use of IoT data is critical to addressing many issues today's education organizations and agencies face, including monitoring sensors and facilities, the health and safety of students, and improving the student experience. Education organizations need to connect and analyze vast amounts of real-time IoT data and student interaction data to generate insights for increased efficiency and operating cost savings. Through the power of advanced IoT analytics, education organizations can be better equipped to make informed, accurate and timely decisions. ",
            "month-posted": "June",
            "date-posted": "5",
            "day-posted": "Sunday",
            "time-posted": "10:19 PM",
            "replies": {}
        },
        "post-Robert-Kemp": {
            "content": "Given the new domains - IOT cross sectioned with Learning and Knowledge analytics will have profound effect as the IOT and LAK domains mature.  IoT is considered a disruptive emerging technology [1] and with new IoT devices  IoT adoption and implementation will build the knowledge base of both successes and failures - of what is acceptable versus what is not acceptable. With near ubiquitous mobile devices and/or yet to be created IoT devices coupled with the education system should be very impactful. Its not hard to conceive that the traditional in-person classroom experience could be improved or enhanced.  Using IoT interconnected devices could easily provide almost instant feedback to on a wide range of topics directly to teachers, to parents, to the administrators.  There is a wide range of possible outcomes such increasing the level of engagement which decreases drop out rate.  In the public school system, a system could be established to seek instant feedback using simple Sentiment analysis, even if the children can read or write yet. On a personal note, when my daughter was young she really liked sending emojis to me even before she was able to read or write via the various chat tools …and she could construct 'sentences of emojis' …this idea could be enhanced and formalized to increase learning rates for the younger children is just one example of how learning and leveraging analytics will impact teaching. Similarly, educational programs could be ranked or rated by students or teachers via mobile or IoT devices and if necessary the program offerings be tweaked and finetuned to provide the best learning outcomes for that particular group of students. Healthcare seems to be an obvious application where IoT can flourish.  Perhaps, a number of IoT sensors combined will be the basis of something similar to Star Trek's medical scanner - the tricorder used by Dr. McCoy [2], [3]. -;)  This is one area that a range of low costs sensors combined with analytics could have huge impact, ie savings lives and/or saving time - the scanners provide feedback into a system so the clinicians have the information available and the system benefits from increased effectiveness and efficiency.  Success will beget success. Incremental successes will facilitate more research and acceptance by all stakeholders too - patients, medical staff and administrators. Within a hospital setting, it seems to be more acceptable to monitor any patient and collect and aggregate personal information - as long as it is kept secure. One major IoT challenge is that of security and that sphere of influence and when it becomes intrusive.  For example, even a simple tracking tool placed in a vehicle could be viewed as a protection device, ie monitor the vehicle's location in case its ever stolen.  Yet the same device could be used in nefarious ways used by Carjackers to identify the location so it can be stolen at a convenient time as reported recently in the news [4].  IoT devices could easily be abused and everyone can be tracked and monitored without their knowledge as has been recently reported [5] .  This is where Learning and Analytics will play an important role providing us the ability to enhance the learning experience but at the same time make us aware of basic levels of privacy we expect.",
            "month-posted": "June",
            "date-posted": "1",
            "day-posted": "Wednesday",
            "time-posted": "7:46 AM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "You and I seem to have similar takes on the subject. There's potential for big change, but also potential for issues around privacy. Interesting that you view Knowledge and Learning Analytics as a possible way to mitigate those concerns.  Do you feel that knowledge alone is enough?  In other words, though we may become aware of the level of privacy that we expect, how do we affect that change in spite of powerful financial and cultural forces that would say otherwise? Thanks!",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Saturday",
                    "time-posted": "8:48 AM",
                    "replies": {
                        "reply-Robert-Kemp": {
                            "content": "Ken, thank you for taking the time to read and respond to my discussion post. Big change is inevitable and as a society, now dependent upon the internet, we need to catch up and every level, the lowest common denominator need to understand what is at stake. Privacy and security are directly related and go 'hand-in-hand' and having knowledge is the first step. IoT will have huge impact and I think it behooves us to demand better security and privacy. Billions, if not Trillions of dollars are stolen yearly by cybercriminals …they will not think twice about stealing data collected by IoT devices and use it to profit at our expense or use it against us …ie sell DNA information to insurance companies or perhaps publish data that is accessible to public Even 'legitimate' businesses might want to gain access to data, just think if life insurance companies could get access to our DNA and then proactively analyze data and then refuse to provide life insurance. Another example is automobile insurance …with 'semi-autonomous' (almost) self-driving cars …perhaps based upon access to IoT sensors, they deny the claim based upon the sensor results preceding the accident … There is a need to become knowledgeable and leverage that learning to 'raise the bar' of good practices and LAK analytics is a good starting point to understand good from bad. As always, I appreciate your insights. Thank you",
                            "month-posted": "June",
                            "date-posted": "4",
                            "day-posted": "Saturday",
                            "time-posted": "1:01 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-kennethmc39": {
            "content": "The increasing interconnectedness of everything is inevitable. The convenience afforded by the ability to link together everything is too powerful a draw to be ignored.   By connecting every appliance, every person, and every possible thing that can support a basic internet connection, monitoring and control become much easier. These are perhaps inflammatory words, but they're not inherently bad.  I would love to be able to monitor my kitchen stove to be confident that it's turned off when I'm away from home, for example.  What I don't want is the ability to control it, because that runs the risk of either malfunction or someone releasing malware that randomly turns my stove on. We have seen the effects of the move toward mass adoption of remote learning in the last few years.  Perhaps the internet of things holds some solution for the near future that does away with the heinous experience that is a proctored online exam.  I'm not sure what that would look like, but that would be my wish for the internet of things in the realm of learning. When devices are connected, they may be more easily set up to log data and gather metrics.  For that reason, the future of Knowledge Analytics and the future of the Internet of Things goes hand in hand.   Data will continue to grow and expand, I believe this is inarguable.  There may be an opportunity to gather specific datapoints with the Internet of Things, rather than the firehose that is currently available. Similarly, the Internet of Things will create more datapoints and more opportunities for monitoring in the healthcare sector.  This is a positive thing in my view, as more consistent and accurate monitoring can only lead to better healthcare services.  The only possible issue there is what if insurance companies use historical monitoring data from the Internet of Things to preemptively exclude people from certain services or coverage? It is entirely possible that the future is dystopian, and that the Internet of Things is the all-seeing ever-present monitoring tool with which we will be subjugated.   I don't think it'll be quite that bad, but there will come a day where everything we say and do will be part of an indelible record, even if we're not posting it to the internet ourselves.",
            "month-posted": "June",
            "date-posted": "4",
            "day-posted": "Saturday",
            "time-posted": "8:29 AM",
            "replies": {
                "reply-Robert-Kemp": {
                    "content": "Thank you for your discussion post and insights You make an interesting distinction - receiving information versus controlling the device - we want the benefits but not be overwhelmed with detail. (as an aside, I have turned off many notifications on my phone --they are just a constant set of distractions now) Similarly, There is also 'digitization' of a number of environments and many times the old 'analogue' methods I believe are much better. Examples are the new stove tops - many now come with a set of high tech buttons that require a user manual on how to operate a stove, We prefer the analogue knob to turn the stove on and off (even though its connected to sophisticated electronics out of sight).  As well, in vehicles, the heating and AC controls …many are digitally controlled with touch control buttons and have many options that essentially require a user manual …the old style analogue controls - knobs for Fan speed and Temperature make sense when it -20 C and you have gloves on waiting to get warm. Regarding the dystopian future …I think its already here …our local hydro commission had to suspend the smart meter programs because there was insufficient security and anybody could download the neighborhood electricity usage on a house by house basis…  Yes, indeed, IoT devices will cause disruption and I hope we can implement the necessary controls to ensure the benefits  far outweigh the concerns. Thanks",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Satruday",
                    "time-posted": "5:30 PM",
                    "replies": {}
                }
            }
        },
        "post-janineis1": {
            "content": "The Internet of Things (IoT) describes the network of physical objects -- from smart phones or watches to sophisticated tools in an industry setting, these devices use technologies that can collect and transmit information on an individual's activity or what a machine is tracking. IoT has revolutionised many industries including the medical field and education. In the medical field, a patient's health activity is able to be monitored by medical professionals from the comfort of their home or frankly, any location. This is beneficial as it increases patient autonomy and can monitor compliance with health plans. In terms of education, Kuyoro et al. (2015) suggests there are many benefits including: 1) increased safety with surveillance cameras tracking individuals entering and leaving the building 2) enhanced ways of learning which integrates technology and includes mixed media like screensharing, remote learning, tailored apps for each subject, online testing 3) increased efficiency with gathering data on how users interact with the learning platform - this data can be used to identify challenges that can ultimately enhance the user experience 4) increased accessibility for students who require extra assistance to achieve success - for example, students with low mobility can still be provided with top-notch education from any location through remote learning As we continue to add to the IoT, we also learn that IoT itself is pervasive in a sense that we are tending to over-rely on technology and privacy issues are less prioritised in the search for simplifying everyday tasks. Integrating Learning Analytics (LA) with IoT suggests a technologically advanced learning environment in which students can link devices that enhance learning such as trips to the library via wireless sensors connected to their phones and have access to learning resources at the tips of their fingers (Cheng & Liao, 2012). Despite the benefits and improved accessibility to learning resources, Ahad and Agarwal (2018) suggest challenges to a combined LA and IoT approach include an intense volume of data captured at a high velocity, possible network and power failures contributing to a mass shutdown on all connected devices, and privacy. However, the authors suggest that LA-based methods within an educational setting increases student cognition and retention as well as providing a tailored learner-centric rather than a teacher-centric model. Thus, while challenges continue to exist in combining LA methods to IoT, benefits in high education settings outweigh the disadvantages such that LA methods can promote independence, critical thinking, and collaborative mindsets as opposed to conventional teaching environments.",
            "month-posted": "June",
            "date-posted": "3",
            "day-posted": "Friday",
            "time-posted": "12:10 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "Do you have any concerns about the increasing over-reliance on software in terms of built-in bias that is present in that software?  This article does a good job of outlining on example of inherent bias in software. I guess my question is: will the Internet of Things increase the amount of software to which we are subjected in our daily lives, and is there a need for us to periodically collectively examine the results of that experience? Thanks!",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Saturday",
                    "time-posted": "8:43 AM",
                    "replies": {}
                }
            }
        },
        "post-Che-Little-Leaf-Matusiak": {
            "content": "I think the Internet of things can have many benefits to society from vulnerable groups receiving better patient care in disease progression, prevention, and prognosis, to helping educators with student success, to everything in between.  However, at the same time, this can quickly become a legal and privacy hot potato.  Especially if the devices that share data do not have the proper methodology shields to protect personal data from nefarious uses in the contexts of education and health care.  Being in governance, whenever we create/update our policies and laws regarding personal data sharing across devices in its use in our education and health care departments, we must always consult with our Legal team to make sure we are covering the grey areas that can exist in newer IT technologies and processes. I think the statement that was really eye opening for me in the readings this week was the article The Internet of Things: Review and theoretical framework. (Nord et al., 2019)  Nord et al. (2019) conveyed the study from the US Department of security regarding IoT vulnerabilities that too many electronic manufactures are not developed with the most vital security features (DHS, 2014).  Data sharing and privacy can be so easily compromised without proper safeguards as demonstrated in the news this week in Canada.  A popular coffee chain's app was improperly collecting geolocation data about users and did not comply with privacy laws with a proper procedure to address this issue (Mallees, 2022).",
            "month-posted": "June",
            "date-posted": "5",
            "day-posted": "Sunday",
            "time-posted": "11:51 PM",
            "replies": {}
        },
        "post-christianmu5": {
            "content": "Since this course is about learning and knowledge analytics, I would like to share my thoughts about IoT in education. The Internet of Things (IoT) is transforming the education sector and making the whole learning experience simpler and faster. IoT devices give students better access to everything from learning materials to communication channels and allow teachers to measure students' learning progress in real time. It also provides accommodations for students with disabilities and special needs. Whether providing IoT devices, such as tablets, to facilitate classroom learning or creating quiet environments for students with sensory needs. Interconnected devices such as smart pens, tablets, and smart boards can improve the delivery of learning materials from teachers to students and vice versa. Thus, IoT has the potential to save not only time and physical resources but also human resources while maintaining a higher level of education. However, the growth of IoT in education is leading to an explosion of threats to cybersecurity because the proliferation of sensors and connected equipment dramatically expands the attack surface on a network. Therefore, the IoT systems are becoming the weak link in security in the establishments of teaching. What do you guys think?",
            "month-posted": "June",
            "date-posted": "5",
            "day-posted": "Sunday",
            "time-posted": "8:27 AM",
            "replies": {
                "reply-aidanpo1": {
                    "content": "I do remember when smart pens were being pushed but understandably they fell off as their usage was a little to narrow.  I do think that real time learning is a big step forward with things like being able to connect a computer to all devices in a classroom to communicate information like projectors, microphones and other things.  They are all things we take for granted these days but they are a huge step forward from having to speak loudly and writing on a chalkboard.  I do agree with cyber security threats because no systems is perfectly safe, but I think that the pros certainly outweigh the cons in this scenario. ",
                    "month-posted": "June",
                    "date-posted": "5",
                    "day-posted": "Sunday",
                    "time-posted": "2:47 PM",
                    "replies": {}
                }
            }
        },
        "post-Matthew-Basaraba": {
            "content": "This figure above (Camacho et. al, 2020) demonstrates the continual process for learning analytics. It is my belief that by utilizing the IoT data available can grow not only in quantity available to researchers but also in variety and also relevance. Variety is an important element of Big Data so that new insights may be made from data not easily available from data collected in surveys or thought to be collected. Relevance to me is the idea that data can be collected on students currently involved in learning so that educators can more quickly adapt to the environment to provide support for learners to improve learning outcomes or the learning environment. Wearable IoT devices can be used to support learning in healthcare for surgery training (Castillo-Segura, et. al, 2021). Rather than having medical professionals learn from new techniques solely from other experts guidance, they can leverage learning analytics models that have been used to create monitoring systems that deliver feedback and results when learners train different medical techniques. Figures 3 and 12 (Castillo-Segura, et. al, 2021) demonstrate a wearable IoT information collection system and a sample of feedback respectively used within such an education system. With real evidence of wearable IoT learning programs it is easy to see that there can be a large benefit in leveraging such systems especially for environments where the cost of alternatives is already high, e.g. medical professionals using time to train others, and with enough adoption of such programs and experience designing systems these advantages could make their way to more cost-prohibitive environments such as public classrooms where funding is one of the largest barriers for entry and adoption. ",
            "month-posted": "June",
            "date-posted": "5",
            "day-posted": "Sunday",
            "time-posted": "11:25 PM",
            "replies": {}
        },
        "post-abebayi": {
            "content": "No submission",
            "month-posted": "No submission",
            "date-posted": "No submission",
            "day-posted": "No submission",
            "time-posted": "No submission",
            "replies": {}
        },
        "post-eugenegr": {
            "content": "Ling et al. (2022) studied the Internet of Things (IoT) in early childhood education.  The incorporation of IoT devices primarily focused (54.5%) on the play process, 27.3% of the IoT devices focused on play-based learning,  and 18.2% focused on learning support. The activities were usually conducted in the participant's home or school and utilized toys or games with an IoT component. The devices were connected to allow the learner to explore the physical world and receive feedback to produce a play-based learning experience. The analytics obtained from the IoT devices permitted the system to modify the learning experience, allowing an increase in the complexity of the play and enhancing the learner's acquired knowledge. In addition, the study mentions the use of (IoT) in class attendance, language learning, interpretation skills, and self-learning as areas of IoT usage.  Concerns about data security and possible adverse learning experiences should be considered when dealing with the Internet of Toys. Self-learning and knowledge retention are other areas of focus for IoT devices. Arif et al. (2021) demonstrated that IoT devices could be used to develop CPR skills in the home. Courses designed to develop IoT devices have led to remote laboratories accessible to students. The study demonstrated the use of IoT in education and allows information capture from student interaction for further enhancement.  (Fernandez, et al., 2015) Abbasy and Quesada (2017) discuss the Internet of Education Things, associating the term with big data and linking institutions for information sharing. The Internet of Education Things components are engagement, creativity, E-Learning, and Self-Learning.  In addition to Learning and Knowledge Analytics data requirements, the components mentioned above provide an opportunity for IoT to influence and impact the Internet of Education Things.",
            "month-posted": "June",
            "date-posted": "2",
            "day-posted": "Thursday",
            "time-posted": "11:02 AM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "There are quite a few articles out there on the issues of privacy around children's smart toys.  I would be very hesitant to let my children play with anything internet-connected that had a camera in it (if I had any children). How do you feel these issues might best be addressed or mitigated? Is it a matter of enacting stricter laws around IoT toys? Thanks!",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Satruday",
                    "time-posted": "8:58 AM",
                    "replies": {
                        "reply-eugenegr": {
                            "content": "Focusing on toys could be misleading. For example, within my family, all children younger than 14 years of age were taught to use tablets and phones containing cameras. The device interface was easily manipulated by very young children and provided something new to explore (feedback in the form of sounds and images). Later on, the devices were loaded with suitable interactive games. The primary concern for older children that weren't being supervised was control over their access to the internet and identification of their location. (The very young children, of course, had a parent in control of the device to ensure it wasn't tasted)  I agree with laws governing access and requiring full disclosure of access concerning information gathering; however, given the increased use of cameras in our devices, homes, and society, can we reasonably expect laws to prevent the illegal use of information. The only plausible solution would be a complete disconnect from the grid.",
                            "month-posted": "June",
                            "date-posted": "4",
                            "day-posted": "Saturday",
                            "time-posted": "11:37 AM",
                            "replies": {}
                        },
                        "reply-suzanneva5": {
                            "content": "Other than smart devices, I'm not sure there are connected toys for children that also have cameras. I haven't been toy shopping in years so I could be mistaken. That said, I would be very reluctant to use a camera monitor that's also connected to the internet. There have been incidents of hacking involved. Hacking or other incidents could occur whenever a child (or anyone) is playing with a connected device.",
                            "month-posted": "June",
                            "date-posted": "4",
                            "day-posted": "Saturday",
                            "time-posted": "10:01 PM",
                            "replies": {}
                        }
                    }
                },
                "reply-Robert-Kemp": {
                    "content": "What struck me about your posting this week was the good use of references and how it substantiated and added significance to discussion.  I am slowly learning how important that is. Given IoT will be pervasive, having the thinking that they are toys makes a lot of sense.  Just like children have learned and benefited from using construction sets such as Lego (which develops great spatial skills), Raspberry Pi devices and a using a host of lower cost digital cameras will lead to interesting set of next generation toys.  The reference from  Arif et al have identified a nice suite of expectations, such as CPR skills and many other ideas that are simple but really effective. Personally, I think of sensors for the home, ie detect water leakage and report back to a console in a simple and coordinated manner… We need to demand these manufacturers address the Security and Privacy  … What are your prognostications for the best use of IoT devices? Thanks for your posting.",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Saturday",
                    "time-posted": "4:29 PM",
                    "replies": {
                        "reply-eugenegr": {
                            "content": "The price of an ESP32 microprocessor with a camera for a single-unit purchase is $15.76. I have frequently used these to monitor temperature, humidity, cloud cover, and object detection and recognition. These microprocessors contain dual processors, have low power consumption, and support WiFI and Bluetooth.  With the addition of a memory card and batteries, these devices can sit off the grid and collect information for months. (no need for expensive cellular or satellite connections) These devices may not be considered IoT based on the lack of internet connection in the mentioned configuration. To make these IoT, the data could be extracted by a passing drone (vehicle, jogger, etc.). That mobile data collection device connects to the ESP32, receives the stored data, and subsequently transmits the data to internet storage. Completing the requirements for IoT. As they exist today, these devices can contain some form of computer intelligence. In addition, these devices have more than enough external connections to include microphone and external control capabilities. If an individual understands programming and the electrical requirements for power, ADC, DAC, and I/O ports, all contained on the device, the use of the device is only restricted to imagination and device specifications. Given this, what happens if one of these devices fails? The price ensures multiple backups can be used. In addition, the local peer-to-peer WiFi connections ensure that all collected data is available to all devices. This exists today. What is the best use of IoT now and in the future? The collection of required data and the sharing of the derived knowledge. Limited by imagination, device specifications and laws. Maybe? What process is in place to ensure data and knowledge gained from all the IoT devices using and generating linked data are preserved as open data for future use? ",
                            "month-posted": "June",
                            "date-posted": "22",
                            "day-posted": "Sunday",
                            "time-posted": "10:24 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-Sheryl-Griffith": {
            "content": " Internet of Things It is important to note that the Internet of Things or IoT is among the up-and-coming areas in modern advances in technology and is exceedingly promising, with great potential to significantly improve society. Its impact on healthcare and education can be profound in the future on a revolutionary scale. Human health can become trackable to perfect preventative medicine before a disease develops. In education, IoT can automate research, schedule planning, and note-taking, making both teaching and learning better. Learning and knowledge analytics will enhance future competencies in higher educational settings. In the case of learning and knowledge analytics, a wide range of competencies can be bettered among both the students and educators (Kleimola & Leppisaari, 2022). In addition, the Smart Health Sensing system or SHSS can be “used to monitor the critical health conditions in the hospitals and trauma centers” and track fitness levels and calorie intake (Kumar et al., 2019, p. 2). In other words, healthcare will be cheaper because many diseases and illnesses will be diagnosed before they become a costly problem. I am particularly interested in the future of IoT in special education and training, for ADHD, and autism.  IoT technologies can equip people with special needs with the devices and means to augment their lives. In conclusion, the future of IoT will enhance health monitoring in healthcare, and more efficiency for teaching and learning, whereas an increase in competencies, will be found in higher education due to learning and knowledge analytics. Thus, IoT has a massive potential to revolutionize many critical social well-being areas. More research and development are needed to widen the range of applicability of IoT.",
            "month-posted": "June",
            "date-posted": "4",
            "day-posted": "Saturday",
            "time-posted": "5:19 pm",
            "replies": {
                "reply-Robert-Kemp": {
                    "content": "Sheryl, thank you for your discussion post and the solid set of references I agree that IoT is a fast growing domain that will have major impact in our society. Special needs is a very interesting domain that could yield amazing benefits. Is there a limit to how far or how many sensors we can use and implement and how much data that can be generated? In your opinion Who would make that type of decision? Thank you",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Saturday",
                    "time-posted": "5:43 PM",
                    "replies": {
                        "reply-Sheryl-Griffith": {
                            "content": "Thanks for your feedback, as always you bring good points. I think we have only seen the beginning with this technology, the sky has no limits here. Data privacy and storage will be concerns as described by Ranger 2020. In terms of the Who, I believe users and businesses will have to jump on board or be left behind. I am interested in this technology and where it will go from here. I may just have this as my project proposal. :-) This is such a stimulating article!",
                            "month-posted": "June",
                            "date-posted": "5",
                            "day-posted": "Sunday",
                            "time-posted": "12:43 PM",
                            "replies": {}
                        }
                    }
                }
            }
        },
        "post-George-Adyrhaiev": {
            "content": "Think about and discuss how the Internet of Things will impact particular areas, such as teaching and learning, health care, etc., in the future. What role will learning and knowledge analytics play in such future scenarios?  This is actually a tricky question as it can be a stretch to connect learning and knowledge analytics to IoT devices. IoT devices are impartial generators of data that can then be used in whatever way an analyst or business user or consumer wants. The role of learning and knowledge analytics from this perspective is irrelevant to the rise in prevalence of the IoT devices. This is not to say about the role of analytics in general, as more robust and efficient ways of data mining and visualizations would need to be developed to accommodate and link the plethora of data from all the IoT devices out there.  In most cases, I believe that IoT devices will help each of the aforementioned industries (teaching and learning, healthcare, etc) gather more granular data more quickly (and at a right time) and precisely. Sensors of any type in these industries can help paint a fuller picture of certain behavioural aspects of the patients, students, and customers, which may be then analyzed and linked to some correlational outcomes. Some of the data points that can be gathered by these devices help answer (or at least shed more light onto) questions like do students who spend more time on campus achieve better results? In a class of 200 how many students typically show up for lectures? Does the university need larger auditoriums to accommodate them? Additionally, educational institutions can implement devices that track student's head position to ensure no (or less) cheating during tests and exams. Or devices that measure student's physical activity in a day or sensors that help universities save on utilities or provide better security where needed. Gathering all of this data may help improve the overall learning experience for students as well as teaching for teachers.  The role of learning and knowledge analytics in these scenarios would be to keep adapting the tools and techniques developed for other analytics use-cases (e.g. BI, Security, healthcare, etc) to use-cases that define the essence of learning analytics, contributing the overall goal of the domain. ",
            "month-posted": "June",
            "date-posted": "1",
            "day-posted": "Wednesday",
            "time-posted": "6:22 PM",
            "replies": {
                "reply-Robert-Kemp": {
                    "content": "George, thank you for your thoughts in this weeks discussion post. I agree that that the LAK analytics is independent of the increase in the prevalence of IoT devices. Yes, the increase in the number of sensors creating the data that can be analyzed.  Do you think the traditional University offering - ie classrooms  and larger auditoriums is an outdated concept?   Perhaps, distance learning with the right amount and type of discourse is a better route? What about 'high school' - most teenagers are not 'morning people', their brain activity is best much later in the day and this has been well documented [1], yet we continue to have morning classes for teenagers. More than likely because teachers need to work  a regular 'work day' so they can return home at a reasonable time- this of course is at the expense of the very students they are trying teach. That's a good point, IoT devices for physical activity …this could apply to everyone … something like doing the 10,000 steps via the fitbit … Thanks for your posting.",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Saturday",
                    "time-posted": "1:26 PM",
                    "replies": {
                        "reply-George-Adyrhaiev": {
                            "content": "Thank you for your response.  I don't think that the traditional University offering is outdated. If I had a chance to take an MScIS degree part-time with an on-campus component where it is convenient for me - I would for sake of face-to-face interaction with the professors and my peers as well as a more defined sense of belonging to the cohort and the university community itself, which is very important. However, there aren't any other universities in Canada offering this degree, and I definitely do enjoy the flexibility of learning whenever and wherever it suits me.  Teenagers and schooling is whole different beast - and you're right what's best for the is being overlooked by the societal norms right now... We shall see how that changes over time. ",
                            "month-posted": "June",
                            "date-posted": "Saturday",
                            "day-posted": "4",
                            "time-posted": "5:18 PM",
                            "replies": {
                                "reply-suzanneva5": {
                                    "content": "I agree with George that traditional in-person learning at the university level isn't outdated; however, universities are facing stiff competition from online options, even non-degree granting programs. I recently did a situation analysis and subsequently a marketing plan for my UBC program. It is losing potential students to online or hybrid offerings because students want the flexibility that type of delivery method gives them. The pandemic was a reckoning in a lot of ways that we can learn and work outside of specific walls.",
                                    "month-posted": "June",
                                    "date-posted": "4",
                                    "day-posted": "Saturday",
                                    "time-posted": "10:28 PM",
                                    "replies": {}
                                }
                            }
                        }
                    }
                }
            }
        },
        "post-aidanpo1": {
            "content": "Think about and discuss in the Week 6 discussion forum how the Internet of Things will impact particular areas, such as teaching and learning, health care, etc., in the future. What role will learning and knowledge analytics play in such future scenarios? The internet of things is both impressive and a little scary.  Although I feel that way about a lot of technology at this scale.  However, I think the benefits far outweigh the negatives.  A lot of fields directly benefit from this technology, the biggest one being the medical field.  The ability for objects to communicate information has made for huge advancements for surgery as assistive devices can now communicate information directly to surgeons about the status of a patient instantly.  This majorly increases surgery efficiency and success rate.  Another huge advancement is the effect it has had on people's everyday lives.  Many people own smart devices like Fitbits or smart watches that track their daily fitness levels and communicate information directly with cellphones.  Many devices like TVs can connect to the internet allowing for connecting to streaming services like Netflix and even YouTube.  Phones can now wirelessly communicate with speakers and even sometimes crazy things like coffee makers.  I have a friend who has a wireless coffee maker that could brew a coffee through a phone app.  It was pretty wild.  This next level of convenience has made a lot of people's lives easier which I personally am very grateful for. In terms of Learning and knowledge analytics, the internet of things applications are directly related to student benefits.  Originally, the idea of the SmartBoard, a large touchpad connected to a computer and a projector that allowed for physical interaction with an object that communicated the information directly to the computer.  This was short lived but pretty cool in theory.  A more applicable advancement was just having a computer directly communicated with a camera, microphone and then a projector to have a professor at the front of a lecture hall be able to speak to all students and display what they are writing on or just the visual output of a computer.  Learning has come a long way from chalkboards and it directly benefits students.",
            "month-posted": "June",
            "date-posted": "5",
            "day-posted": "Sunday",
            "time-posted": "2:41 PM",
            "replies": {
                "reply-Sheryl-Griffith": {
                    "content": "Aidan seems like we are both on the same page here as I agree that IoT is both scary and intriguing at the same time. Are you a Trekkie by chance? Beam me up, Scottie! It feels like we may be able to say that sometime in the future with IoT. Good read!",
                    "month-posted": "Sunday",
                    "date-posted": "5",
                    "day-posted": "Sunday",
                    "time-posted": "6:42 PM",
                    "replies": {}
                }
            }
        },
        "post-suzanneva5": {
            "content": "According to Ranger, “the term IoT is mainly used for devices that wouldn't usually be generally expected to have an internet connection, and that can communicate with the network independently of human action”. With that in mind and based off familial circumstances, I'll focus my discussion post on healthcare for the elderly.             Dementia care is one aspect where I can see the use of IoT, not just for the patients themselves but also for the caregivers. As of September 2021, there were 55+ million people around the world with some form of dementia (Dementia, 2021). Wandering is a common sign of dementia and can put patients at risk. They can even get lost in known environments. Wearable devices that can keep track of their whereabouts through cellular and GPS technologies are of great benefit to them and offer relief to the caregivers (Ray et al., 2019). Under Telus' health umbrella, they have a wearable device for medical emergencies that also has fall detection. If a fall is detected, the operator receives an alert. The wearer then gets a call through the device. It also has GPS technology in case emergency must be dispatched. How could learning and knowledge analytics play in this scenario? If the device is also learning about the wearer's behavior and recording their movements, then that information can used to help them make a decision, find them if lost, as well as teach the wearer how to get back to safety.",
            "month-posted": "June",
            "date-posted": "1",
            "day-posted": "Wednesday",
            "time-posted": "4:56 PM",
            "replies": {
                "reply-kennethmc39": {
                    "content": "My grandmother has dementia. She's not prone to wandering, but she has absolutely no short-term memory left.  I wonder if there might be a need for a system that tracks the location not only of the patient, but also notes when support staff are near.  What I'm thinking is some kind of system that, for example, only allows the stove to turn on when a home-care worker is in the apartment?  That way patients can't set out to cook lunch and then forget about food on the stove or in the oven. Thanks!",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Saturday",
                    "time-posted": "8:51 AM",
                    "replies": {
                        "reply-suzanneva5": {
                            "content": "I think that's more than doable. It can be part of a smart home. The outlet for the appliance would need to be configured so it's part of the system. The worker would need credentials for the system or just a bar code from their phone to let the outlet have power. It could stay powered on until they either log out OR a certain time limit has passed (example, 1 hour). The system could prompt them with a warning Stove outlet is shutting off in 5 minutes in case they haven't logged out but need more time.",
                            "month-posted": "June",
                            "date-posted": "4",
                            "day-posted": "Saturday",
                            "time-posted": "9:36 AM",
                            "replies": {}
                        }
                    }
                },
                "reply-Robert-Kemp": {
                    "content": "I agree, The area of IoT and the benefits for health care are only limited by our imagination. IoT has many possibilities. I have elderly parents - taking and dispensing medication sounds easy, except when you mix in 4 or 5 prescriptions, some are three times a day, some are twice a day with some having to take half a tablet twice a day too. A low cost effective solution is required other than a paper based checklist. I have trouble trying to remember if I have taken my Vitamins ;-)  Wearable devices is another good solution, even tracking devices for those suffering from dementia and if they wander off. Do you know if devices can legally or ethically deployed today regarding personal freedoms vs safety vs privacy concerns? Thanks for your post",
                    "month-posted": "June",
                    "date-posted": "4",
                    "day-posted": "Saturday",
                    "time-posted": "5:04 PM",
                    "replies": {
                        "reply-suzanneva5": {
                            "content": "I hadn't even considered the daunting task of prescriptions. It was a big thing to finally get daily blister paks. Imagine if we could do something else to alleviate the burden of having to remember which med and when! I'm not sure what you're asking about personal freedoms vs safety vs privacy concerns. No one's personal freedoms are being infringed because they can go where they want. It's just a matter of knowing where they are IF they wander. If they're with you, no need to track them. It's not like a fleet management tool where you create boundaries for each driver. I'd only be concerned if the data was being sold without knowledge and personal data wasn't scrubbed. I'm of the mind that if technology can help bring someone back home safely, then it's worth the gamble. One of my first positions was in the GPS tracking industry.",
                            "month-posted": "Saturday",
                            "date-posted": "4",
                            "day-posted": "Saturday",
                            "time-posted": "9:52 PM",
                            "replies": {}
                        }
                    }
                }
            }
        }
    }
}